# Fitting and Interpreting Mixed-Effects Models {#intro-lmer-fitting}

```{r echo=FALSE, message=FALSE}
library(knitr)
library(kableExtra)

opts_knit$set(
  width = 85,
  tibble.print_max = Inf
  )

opts_chunk$set(
  prompt = FALSE,
  comment = NA,
  message = FALSE,
  warning = FALSE,
  tidy = FALSE,
  fig.align = 'center',
  out.width = '50%'
  )
```



In this set of notes, you will learn how to fit and interpret the fixed- and random-effects from a linear mixed-effects model.

---

### Preparation {-}

Before class you will need to read the [Relational data](https://r4ds.had.co.nz/relational-data.html) chapter from following:

- Grolemund, G., &amp; Wickham, H. (2017). [R for Data Science: Visualize, model, transform, tidy, and import data](https://r4ds.had.co.nz/). **. Sebastopol, CA: Oâ€™Reilly.

Focus on the information on mutating joins.

<br />

---



## Dataset and Research Question

In this set of notes, we will use data from two files, the *netherlands-students.csv* file and the *netherlands-schools.csv* file (see the [data codebook](#netherlands) here). These data include student- and school-level attributes, respectively, for $n_i=2287$ 8th-grade students in the Netherlands.

```{r message=FALSE, paged.print=FALSE}
# Load libraries
library(AICcmodavg)
library(broom)
library(dplyr)
library(ggplot2)
library(lme4) #for fitting mixed-effects models
library(readr)
library(sm)

# Read in student-level data
student_data = read_csv(file = "~/Documents/github/epsy-8252/data/netherlands-students.csv")

# Read in school-level data
school_data = read_csv(file = "~/Documents/github/epsy-8252/data/netherlands-schools.csv")

# Join the two datasets together
joined_data = left_join(student_data, school_data, by = "school_id")
head(joined_data)
```

We will use these data to explore the question of whether verbal IQ scores predict variation in post-test language scores.


## Fitting the Mixed-Effects Regression Model in Practice

In practice, we use the `lmer()` function from the **lme4** library to fit mixed-effect regression models. This function will essentially do what we did in the previous section, but rather than independently fitting the team-specific models, it will fit all these models simultaneously and make use of the information in all the clusters (schools) to do this. This will result in better estimates for both the fixed- and random-effects.

The syntax looks similar to the syntax we use in `lm()` except now we split it into two parts. The first part of the syntax gives a model formula to specify the outcome and fixed-effects included in the model. This is identical to the syntax we used in the `lm()` function. In our example: `language_post ~ 1 + verbal_iq` indicating that we want to fit a model that includes fixed-effects for both the intercept and the effect of verbal IQ score.

We also have to declare that we want to fit a model for each school. To do this, we will include a random-effect for intercept. (We could also include a random-effect of verbal IQ, but to keep it simpler right now, we only include the RE of intercept.) The second part of the syntax declares this: `(1 | school_id)`. This says fit school-specific models that vary in their intercepts. This is literally added to the fixed-effects formula using `+`. The complete syntax is:

```{r message=FALSE}
# Fit mixed-effects regression model
lmer.1 = lmer(language_post ~ 1 + verbal_iq + (1 | school_id), data = joined_data)
```

To view the fixed-effects, we use the `fixef()` function.

```{r}
fixef(lmer.1)
```

This gives the coefficients for the fixed-effects part of the model (i.e., the global model),

$$
\hat{\mathrm{Language~Score}_{ij}} = 40.61 + 2.49(\mathrm{Verbal~IQ}_{ij})
$$

Note that the notation now includes two subscripts. The *i* subscript still indicates the *i*th student, and the new *j* subscript indicates that the student was from the *j*th school. Since we accounted for school in the model (schools are allowed to have different intercepts) we need to now identify that in the equation. We interpret these coefficients from the fixed-effects equation exactly like `lm()` coefficients. Here,

- The predicted average post-test language score for students with a mean verbal IQ score (=0) is 40.61.
- Each one-point difference in verbal IQ score is associated with a 2.49-point difference in language scores, on average.

To view the school-specific random-effects, we use the `ranef()` function (only the first 5 rows are shown).

```{r eval=FALSE}
ranef(lmer.1)
```

```
$school_id
    (Intercept)
1   -0.37573940
2   -6.04469893
10  -3.66481710
12  -2.91463441
15  -5.74351132
```

The random-effects indicate how the school-specific intercept differs from the overall average intercept. For example, the intercept for School 1 is approximately 0.38-points lower than the average intercept of 40.61 (the fixed-effect). This implies that, on average, students from School 1 with a mean verbal IQ score (=0) have an average post-test language score that is 0.38-points lower than their peers who also have a mean verbal IQ score. 

From the estimated fixed- and random-effects, we can re-construct each school-specific fitted equation if we are so inclined. For example, to construct the school-specific fitted equation for School 1, we combine the estimated coefficients for the fixed-effects and the estimated random-effect for School 1:

$$
\begin{split}
\hat{\mathrm{Language~Score}_{i}} &= \bigg[ 40.61 -0.376 \bigg]+ 2.49(\mathrm{Verbal~IQ}_{i}) \\[1ex]
&= 40.2 + 2.49(\mathrm{Verbal~IQ}_{i}) 
\end{split}
$$

In this notation, the *j* part of the subscript is dropped since *j* is now fixed to a specific school; $j=1$.


## Example 2: Life Satisfaction of NBA Players

As a second example, we will explore the question of whether NBA players' success is related to life satisfaction. To do this, we will use two datasets, the *nba-player-data.csv* and *nba-team-data.csv* file (see the [data codebook](#nba) here). These data include player- and team-level attributes for $n=300$ players and $n=30$ teams, respectively. To begin, we will import both the player-level and team-level datasets and then join them together.

```{r}
# Read in player-level data
nba_players = read_csv(file = "~/Documents/github/epsy-8252/data/nba-player-data.csv")

# Read in team-level data
nba_teams = read_csv(file = "~/Documents/github/epsy-8252/data/nba-team-data.csv")

# Join the datasets together
nba = nba_players %>%
  left_join(nba_teams, by = "team")

head(nba)
```

We want to fit a model that regresses the `life_satisfaction` scores on the `success` values. In these data, however, we might expect that the life satisfaction of players is correlated within team. To account for that fact, we can include a random-effect of intercept in our regression model.

### Fit the Mixed-Effects Model

Below we fit a mixed-effects regression model to predict variation in life satisfaction scores that includes success as a predictor. We also include a random-effect of intercept to account for the within-team correlation of life satisfaction scores. The statistical model is:

$$
\mathrm{Life~Satisfaction}_{ij} = \bigg[\beta_0 + b_{0j} \bigg] + \beta_1(\mathrm{Success}_{ij}) + \epsilon_{ij}
$$

We fit the model using `lmer()` as:

```{r}
# Fit model
lmer.1 = lmer(life_satisfaction ~ 1 + success + (1 | team), data = nba)
```

We can then extract the fixed-effects estimates using the `fixef()` function. 

```{r}
# Get fixed-effects
fixef(lmer.1)
```

We write the fitted fixed-effects model as:

$$
\hat{\mathrm{Life~Satisfaction}}_{ij} = 11.57 + 1.67(\mathrm{Success}_{ij})
$$

Again, we can interpret these fixed-effects estimates the same way we do any other regression coefficient.

- The predicted average life satisfaction for NBA players with a success score of 0 (free-throw percentage in the lowest 20\%) is 11.57.
- Each one-unit difference in success (one-quantile difference) is associated with a 1.67-point difference in life satisfaction score, on average.

If we are interested in the fitted model for a SPECIFIC team, we can extract the random-effect of intercept for that team and add it to the fixed-effect intercept estimate. For example, the equation for the Minnesota Timberwolves is:

```{r}
# Obtain random-effects
ranef(lmer.1)
```

$$
\begin{split}
\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 + 3.66 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
&= 15.23 + 1.67(\mathrm{Success}_{i})
\end{split}
$$

For Timberwolves players,

- The predicted average life satisfaction for Timberwolves players with a success score of 0 (free-throw percentage in the lowest 20\%) is 15.23. This is a score that is 3.66 points above average for all NBA players with a success score of 0.
- Each one-unit difference in success is associated with a 1.67-point difference in life satisfaction score for Timberwolves players, on average. This is the same rate-of-change as for NBA players in general.

As a comparison, we can also consider the equation for the Phoenix Suns:

$$
\begin{split}
\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 - 6.13 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
&= 5.44 + 1.67(\mathrm{Success}_{i})
\end{split}
$$

- The predicted average life satisfaction for Suns players with a success score of 0 (free-throw percentage in the lowest 20\%) is 5.44. This is a score that is 6.13 points below average for all NBA players with a success score of 0.
- Each one-unit difference in success is associated with a 1.67-point difference in life satisfaction score for Suns players, on average. This is the same rate-of-change as for NBA players in general.

Comparing these two team's equations gives us some insight into why the effects are referred to as *fixed-effects* or *random-effects*. 

$$
\begin{split}
\mathbf{Timberwolves:~}\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 + 3.66 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
\mathbf{Suns:~}\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 - 6.13 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
\end{split}
$$

In the two equations, the fixed-effects of intercept (11.57) and success (1.67) are represented in both equations; they are fixed. On the other hand, the random-effect is different for each team. Since we included a random-effect of intercept in out model, this means that the intercept value for each team equation will be different.


<!-- ## Other Resources {-} -->

<!-- In addition to the notes and what we cover in class, there many other resources for learning information criteria for model selection. Here are some resources that may be helpful in that endeavor: -->

<!-- - Anderson, David R. (2008). [Model based inference in the life sciences: A primer on evidence](https://link-springer-com.ezp3.lib.umn.edu/book/10.1007%2F978-0-387-74075-1). New York: Springer. -->
<!-- - Burnham, Kenneth P., &amp; Anderson, David R. (2002). [Model selection and multimodel inference: A practical information-theoretic approach](http://ecologia.ib.usp.br/bie5782/lib/exe/fetch.php?media=bie5782:pdfs:burnham_anderson2002.pdf). New York: Springer. -->

<!-- <br /> -->

<!-- For **table formatting** using R Markdown, check out: -->

<!-- - [kableExtra Documentation](https://haozhu233.github.io/kableExtra/) -->
<!-- - [gt Documentation](https://gt.rstudio.com/) -->





