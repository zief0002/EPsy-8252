---
title: "Introduction to Mixed-Effects Models"
author: "Andrew Zieffler"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    highlight: zenburn
    css: ['style/style.css', 'style/table-styles.css', 'style/syntax.css', 'style/notes.css']
bibliography: ['epsy8251.bib', 'epsy8252.bib']
csl: 'style/apa-single-spaced.csl'
---


<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/es5/tex-mml-chtml.js">
</script>


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE, fig.width=6, fig.height=6,
               fig.pos='H', fig.align='center', out.width='40%')
opts_knit$set(width=85)
options(scipen=5)
```

# Preparation


In this set of notes, you will learn the conceptual ideas behind *linear mixed-effects models*, also called *multilevel models* or *hierarchical linear models*. To do so, we will use data from two files, the *netherlands-students.csv* file and the *netherlands-schools.csv* files (see the [data codebook](https://zief0002.github.io/book-8252/data-codebook.html#netherlands) here). These data include student- and school-level attributes, respectively, for 2,287 8th-grade students in the Netherlands.

```{r message=FALSE}
# Load libraries
library(broom)
library(educate)
library(lme4) #for fitting mixed-effects models
library(patchwork)
library(tidyverse)

# Read in student-level data
student_data = read_csv(file = "~/Documents/github/epsy-8252/data/netherlands-students.csv")
head(student_data)

# Read in school-level data
school_data = read_csv(file = "~/Documents/github/epsy-8252/data/netherlands-schools.csv")
head(school_data)
```

We will use these data to explore the question of whether students' verbal IQ scores predict variation in their post-test language scores.

<br />

## Join the Student- and Classroom-Level Data

Before analyzing the data, we need to join, or merge, the two datasets together. To do this, we will use the `left_join()` function from the `{tidyverse}` package. 

```{r}
# Join the data
joined_data = student_data %>%
  left_join(school_data, by = "school_id")

# View joined data
head(joined_data)
```

In this join, we are adding records from the school data (right data) to the student data (left data).^[They are referred to as 'left' and 'right' because of where they appear in the syntax. If we wrote the syntax without the pipe it would be `left_join(student_data, school_data, by = "school_id")`. In the syntax, `student_data` is to the left of `school_data`.] When we perform this left join, the final joined data will include all records from the student dataset (left) but only those records from the school dataset whose key (`school_id` value) is contained in the student data. For example, if the school data included information about a school that was not included in the student data, it would not appear in the joined data.

The `{tidyverse}` package includes six different join functions. You can read about four common join functions and see an animation illustrating them [here](https://thomasadventure.blog/posts/r-merging-datasets/).

<br />


# Fixed-Effects Regression Model

To examine the research question of whether verbal IQ scores predict variation in post-test language scores, we might regress language scores on the verbal IQ scores using the `lm()` function. The `lm()` function fits a *fixed-effects regression model*.

```{r}
# Fit fixed-effects model
lm.1 = lm(language_post ~ 1 + verbal_iq, data = joined_data)

# Model-level output
glance(lm.1)

# Coefficient-level output
tidy(lm.1)
```

The model-level summary information suggests that differences in verbal IQ scores explains 37.2\% of the variation in post-test language scores. The estimated intercept suggests that the average predicted post-test language scores for students with a mean verbal IQ score (= 0) is 40.93. The estimated slope indicates that each one-point difference in verbal IQ score is associated with a difference in post-test language scores of 2.65, on average. To have faith in the analytic results from this model, we need to evaluate whether the assumptions are satisfied.

<br />

### Residual Analysis

```{r fig.width=8, fig.height=4, out.width='90%', fig.cap='Residual plots for the fixed-effects regression model.'}
# Obtain the augmented data frame
out = augment(lm.1)
head(out)

# Scatterplot of standardized residuals vs. fitted values
p1 = ggplot(data = out, aes(x = .std.resid)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_bw() +
  xlab("Standardized residuals") +
  ylab("Probability density")

# Density plot of standardized residuals
p2 = ggplot(data = out, aes(x = .fitted, y = .std.resid)) +
  geom_smooth(se = TRUE) +
  geom_hline(yintercept = 0) +
  geom_point() +
  theme_bw() +
  xlab("Fitted values") +
  ylab("Standardized residuals")

# Display plots side-by-side
p1 | p2
```

The assumption that the mean residual is 0 seems reasonably satisfied, however those of normality and homoscedasticity seem less feasible, especially given the large sample size. More importantly, the assumption of independence (which we don't evaluate from the common residual plots) is probably not tenable. Students' post-test language scores (and thus the residuals) are probably correlated within schools---this is a violation of independence which assumes that the correlation between student's residuals is 0. If we have a variable that identifies classroom, we can actually examine this by plotting the residuals separately for each classroom.

In this case, we actually can evaluate the independence assumption to see if students' residuals are correlated within schools. To do so we need to include the `school_id` variable in the augmented data (using `mutate()`) so we can show the residuals versus the fitted values by school. The `school_id` variable includes 131 different schools, which means that we would be looking at 131 different residual plots. `r emo::ji("grimace")` When we use `facet_wrap()` with that many levels each plot will be too small to see, so we will instead select a random sample of, say, 25 of the schools to plot.

```{r fig.height=10, fig.width=12, out.width='90%', fig.cap='Scatterplots of the standardized residuals versus the fitted values for 25 schools.'}
# Make random sample reproducible
set.seed(100)

# Draw random sample of 25 schools without replacement
my_sample = sample(school_data$school_id, size = 25, replace = FALSE)

# Mutate on school ID and draw random sample
out = out %>%
  mutate(school_id = joined_data$school_id) %>%
  filter(school_id %in% my_sample)

### Show residuals by school
ggplot(data = out, aes(x = .fitted, y = .std.resid)) +
	geom_point() +
	geom_hline(yintercept = 0) +
	theme_bw() +
  xlab("Fitted values") +
  ylab("Studentized residuals") +
	facet_wrap(~school_id, nrow = 5)
```

The residuals for several of the schools show a systematic trends of being primarily positive or negative within schools. For example, the residuals for several schools (e.g., 47, 256 258) are primarily negative. This is a sign of non-independence of the residuals. If we hadn't had the school ID variable we could have still made a logical argument about this non-independence via substantive knowledge. For example, students who attend a "high performing" will likely tend to have positive residuals (scores above average relative to the population), even after accounting for their verbal IQ scores.

To account for this within-school correlation we need to use a statistical model that accounts for the correlation among the residuals within schools. This is what *mixed-effects models* bring to the table. By correctly modeling the non-independence, we get more accurate standard errors and *p*-values.

Another benefit of using mixed-effects models is that we also get estimates of the variation accounted for at both the school- and student-levels. This disaggregation of the variation allows us to determine which level is explaining more variation and to study predictors appropriate to explaining that variation. For example, suppose that you disaggregated the variation in language scores and found that:

- 96% of the variation in these scores was at the student-level, and
- 3% of the variation in these scores was at the classroom-level, and
- 1% of the variation in these scores was at the school-level.

By including school-level or classroom-level predictors in the model, you would only be "chipping away" at that 1% or 3%, respectively. You should focus your attention and resources on student-level predictors!

<br />


# Conceptual Idea of Mixed-Effects Models

In this section we will outline the conceptual ideas behind mixed-effects models by linking the ideas behind these models to the conventional, fixed-effects regression model. *It is important to realize that this is just conceptual in nature. Its purpose is only to help you understand the output you get from a mixed-effects model analysis.*

To begin, we remind you of the fitted equation we obtained earlier from the fixed-effects regression:

$$
\hat{\mathrm{Language~Score}_i} = 34.19 + 2.04(\mathrm{Verbal~IQ}_i)
$$

This is the fitted equation from the *global model*, so called because it was fitted with the data from all students across all schools.^[Some authors refer to this as a completely pooled model.] As such, the intercept and slope estimates from this equation are averaged across students and schools.  

Mixed-effects regression actually fits a global model (like the one above) AND a school-specific model for each school. Conceptually, the school-specific model is akin to fitting a regression model for each school separately. Below I show the results (for five of the schools) of fitting a different regression model to each school, but keep in mind that this is not actually what happens; it is only to help you understand.

```{r}
# Fit school models
school_models = joined_data %>%
  group_by(school_id) %>%
  summarize(
    tidy(lm(language_post  ~ 1 + verbal_iq))
    ) %>%
  ungroup()

# View coefficients from fitted models
school_models
```

As an example, let's focus on the fitted model for School 1.

$$
\mathbf{School~1:}~~\hat{\mathrm{Language~Score}_i} = 39.79 + 2.24(\mathrm{Verbal~IQ}_i)
$$

Comparing this school-specific model to the global model, we find that School 1's intercept is higher than the intercept from the global model (by 5.65) and it's slope is also higher than the slope from the global model (by 0.20). We can actually re-write the school-specific model using these ideas:

$$
\mathbf{School~1:}~~\hat{\mathrm{Language~Score}_i} = \bigg[34.19 + 5.65\bigg] + \bigg[2.04 + 0.20\bigg](\mathrm{Verbal~IQ}_i)
$$

In the language of mixed-effects modeling:

- The global intercept and slope are referred to as *fixed-effects*. (These are also sometimes referred to as *between-groups* effects.)
  + The fixed-effect of intercept is 34.19; and
  + The fixed effect of the slope is 2.04.
- The school-specific deviations from the fixed-effect values are referred to as *random-effects*. (These are also sometimes referred to as *within-groups* effects.)
  + The random-effect of the intercept for School 1 is 5.65; and
  + The random-effect of the slope for School 1 is  0.20.


Note, each school could potentially have a different random-effect for intercept and slope. For example, writing the school-specific fitted equation for School 2 in this manner,

$$
\begin{split}
\mathbf{School~2:}~~\hat{\mathrm{Language~Score}_i} &= 27.35 + 1.28(\mathrm{Verbal~IQ}_i)\\
 &= \bigg[34.19 - 6.84\bigg] + \bigg[2.04 - 0.76\bigg](\mathrm{Verbal~IQ}_i).\\
\end{split}
$$

In this model:

- The fixed-effects (global effects) are the same as they were for School 1.
  + The fixed-effect of intercept is 34.19; and
  + The fixed effect of the slope is 2.04.
- The random-effect of intercept for School 2 is $-6.84$.
- The random-effect of slope for School 2 is $-0.76$.

<br />


# Fitting the Mixed-Effects Regression Model in Practice

In practice, we use the `lmer()` function from the `{lme4}` library to fit mixed-effect regression models. This function will essentially do what we did in the previous section, but rather than independently fitting the school-specific models, it will fit all these models simultaneously and make use of the information in all the clusters (schools) to do this. This will result in better estimates for both the fixed- and random-effects.

The syntax looks similar to the syntax we use in `lm()` except now we split it into two parts. The first part of the syntax gives a model formula to specify the outcome and fixed-effects included in the model. This is identical to the syntax we used in the `lm()` function. In our example: `language_post ~ 1 + verbal_iq` indicating that we want to fit a model that includes fixed-effects for both the intercept and the effect of verbal IQ score.

We also have to declare that we want to fit a model for each school. To do this, we will include a random-effect for intercept. (We could also include a random-effect of verbal IQ, but to keep it simpler right now, we only include the RE of intercept.) The second part of the syntax declares this: `(1 | school_id)`. This says to fit school-specific models that vary in their intercepts. This is literally added to the fixed-effects formula using `+`. The complete syntax is:

```{r message=FALSE}
# Fit mixed-effects regression model
lmer.1 = lmer(language_post ~ 1 + verbal_iq + (1 | school_id), data = joined_data)
```

To view the fixed-effects, we use the `fixef()` function.

```{r}
fixef(lmer.1)
```

This gives the coefficients for the fixed-effects part of the model (i.e., the global model),

$$
\mathbf{Fixed\mbox{-}Effects~Equation:}~~\hat{\mathrm{Language~Score}_{ij}} = 40.61 + 2.49(\mathrm{Verbal~IQ}_{ij})
$$

Note that the notation now includes two subscripts. The *i* subscript still indicates the *i*th student, and the new *j* subscript indicates that the student was from the *j*th school. Since we accounted for school in the model (schools are allowed to have different intercepts) we need to now identify that in the equation. We interpret these coefficients from the fixed-effects equation exactly like `lm()` coefficients. Here,

- The predicted average post-test language score for students with a mean verbal IQ score (=0) is 40.61.
- Each one-point difference in verbal IQ score is associated with a 2.49-point difference in language scores, on average.

To view the school-specific random-effects, we use the `ranef()` function (only the first 5 rows are shown).

```{r eval=FALSE}
ranef(lmer.1)
```

```
$school_id
    (Intercept)
1   -0.37573940
2   -6.04469893
10  -3.66481710
12  -2.91463441
15  -5.74351132
```

The random-effects indicate how the school-specific intercept differs from the intercept in the fixed-effects equation (which is the average intercept across schools). For example, the random-effect for School 1 is $-0.38$, ehich indicates that the intercept for School 1 is approximately 0.38-points lower than the average intercept of 40.61 (the fixed-effect). This implies that, on average, students from School 1 with a mean verbal IQ score (=0) have an average post-test language score that is 0.38-points lower than their peers who also have a mean verbal IQ score.

From the estimated fixed- and random-effects, we can re-construct each school-specific fitted equation if we are so inclined. For example, to construct the school-specific fitted equation for School 1, we combine the estimated coefficients for the fixed-effects and the estimated random-effect for School 1:

$$
\begin{split}
\mathbf{School~1:}~~\hat{\mathrm{Language~Score}_{i}} &= \bigg[ 40.61 -0.376 \bigg]+ 2.49(\mathrm{Verbal~IQ}_{i}) \\[1ex]
&= 40.2 + 2.49(\mathrm{Verbal~IQ}_{i})
\end{split}
$$

In this notation, the *j* part of the subscript is dropped since *j* is now fixed to a specific school; $j=1$.

<br />


# Example 2: Life Satisfaction of NBA Players

As a second example, we will explore the question of whether NBA players' success is related to life satisfaction. To do this, we will use two datasets, the *nba-player-data.csv* and *nba-team-data.csv* file (see the [data codebook](https://zief0002.github.io/book-8252/data-codebook.html#nba) here). These data include player- and team-level attributes for $n=300$ players and $n=30$ teams, respectively. To begin, we will import both the player-level and team-level datasets and then join them together.

```{r}
# Read in player-level data
nba_players = read_csv(file = "~/Documents/github/epsy-8252/data/nba-player-data.csv")

# Read in team-level data
nba_teams = read_csv(file = "~/Documents/github/epsy-8252/data/nba-team-data.csv")

# Join the datasets together
nba = nba_players %>%
  left_join(nba_teams, by = "team")

# View joined data
head(nba)
```

We want to fit a model that regresses the `life_satisfaction` scores on the `success` values. In these data, however, we might expect that the life satisfaction of players is correlated within team. To account for that fact, we can include a random-effect of intercept in our regression model.

<br />

## Fit the Mixed-Effects Model

Below we fit a mixed-effects regression model to predict variation in life satisfaction scores that includes success as a predictor. We also include a random-effect of intercept to account for the within-team correlation of life satisfaction scores. The statistical model is:

$$
\mathrm{Life~Satisfaction}_{ij} = \bigg[\beta_0 + b_{0j} \bigg] + \beta_1(\mathrm{Success}_{ij}) + \epsilon_{ij}
$$

We fit the model using `lmer()` as:

```{r}
# Fit model
lmer.1 = lmer(life_satisfaction ~ 1 + success + (1 | team), data = nba)
```

We can then extract the fixed-effects estimates using the `fixef()` function.

```{r}
# Get fixed-effects
fixef(lmer.1)
```

We write the fitted fixed-effects equation as:

$$
\hat{\mathrm{Life~Satisfaction}}_{ij} = 11.57 + 1.67(\mathrm{Success}_{ij})
$$

Again, we can interpret these fixed-effects estimates the same way we do any other regression coefficient.

- The predicted average life satisfaction for NBA players with a success score of 0 (free-throw percentage in the lowest 20%) is 11.57.
- Each one-unit difference in success (one-quantile difference) is associated with a 1.67-point difference in life satisfaction score, on average.

If we are interested in the fitted model for a SPECIFIC team, we can extract the random-effect of intercept for that team and add it to the fixed-effect intercept estimate. For example, the equation for the Minnesota Timberwolves is:

```{r eval=FALSE}
# Obtain random-effects
ranef(lmer.1)
```

```
$team
                       (Intercept)
Atlanta Hawks                2.747
Boston Celtics               5.359
Brooklyn Nets                4.101
      ...                     ...
Minnesota Timberwolves       3.662
      ...                     ...
Phoenix Suns                -6.134
      ...                     ...
Washington Wizards           2.083
```


$$
\begin{split}
\mathbf{Minnesota~Timberwolves:}~~\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 + 3.66 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
&= 15.23 + 1.67(\mathrm{Success}_{i})
\end{split}
$$

For Timberwolves players,

- The predicted average life satisfaction for Timberwolves players with a success score of 0 (free-throw percentage in the lowest 20%) is 15.23. This is a score that is 3.66 points above average for all NBA players with a success score of 0.
- Each one-unit difference in success is associated with a 1.67-point difference in life satisfaction score for Timberwolves players, on average. This is the same rate-of-change as for NBA players in general.

As a comparison, we can also consider the equation for the Phoenix Suns:

$$
\begin{split}
\mathbf{Phoenix~Suns:}~~\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 - 6.13 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
&= 5.44 + 1.67(\mathrm{Success}_{i})
\end{split}
$$

- The predicted average life satisfaction for Suns players with a success score of 0 (free-throw percentage in the lowest 20%) is 5.44. This is a score that is 6.13 points below average for all NBA players with a success score of 0.
- Each one-unit difference in success is associated with a 1.67-point difference in life satisfaction score for Suns players, on average. This is the same rate-of-change as for NBA players in general.


Comparing these two team's equations gives us some insight into why the effects are referred to as *fixed-effects* or *random-effects*.

$$
\begin{split}
\mathbf{Minnesota~Timberwolves:~}\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 + 3.66 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
\mathbf{Phoenix~Suns:~}\hat{\mathrm{Life~Satisfaction}}_{i} &= \bigg[11.57 - 6.13 \bigg] + 1.67(\mathrm{Success}_{i}) \\[1ex]
\end{split}
$$

In the two equations, the fixed-effects of intercept (11.57) and success (1.67) are represented in both equations; they are fixed across the clusters (teams). On the other hand, the random-effect is different for each team. Since we included a random-effect of intercept in out model, this means that the team-specific intercept value for each team equation will be different.





