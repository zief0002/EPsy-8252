---
title: "Linear Mixed-Effects Models: Longitudinal Analysis"
author: "Andrew Zieffler"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    highlight: zenburn
    css: ['style/style.css', 'style/table-styles.css', 'style/syntax.css', 'style/notes.css']
bibliography: ['epsy8251.bib', 'epsy8252.bib']
csl: 'style/apa-single-spaced.csl'
---


<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/es5/tex-mml-chtml.js">
</script>


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE, fig.width=6, fig.height=6,
               fig.pos='H', fig.align='center', out.width='40%')
opts_knit$set(width=85)
options(scipen=5)
```



In this set of notes, you will learn how to use the linear mixed-effects model to analyze longitudinal data.


# Dataset and Research Question

In this set of notes, we will use data from the file *vocabulary.csv* (see the [data codebook](http://zief0002.github.io/epsy-8252/codebooks/vocabulary.html) here). These data include repeated measurements of scaled vocabulary scores for $n=64$ students.

```{r message=FALSE}
# Load libraries
library(AICcmodavg)
library(broom.mixed)
library(educate)
library(lme4) #for fitting mixed-effects models
library(patchwork)
library(tidyverse)

# Read in data
vocabulary = read_csv(file = "~/Documents/github/epsy-8252/data/vocabulary.csv")
head(vocabulary)
```

We will use these data to explore the change in vocabulary over time (longitudinal variation in the vocabulary scores). We will focus on two primary research questions: (1) What is the growth pattern in the average vocabulary score over time? and (2) Is this growth pattern different for females and non-females?

<br />


# Data Structure: Tidy/Long Data vs. Wide Data

Before doing any analysis of the data, it is worth understanding the structure of the data. There are two common structures for repeated measures data: *tidy/long structured data* and *wide structured data*.

- In **tidy/long** structured data, there is a single column per variable. For example, the outcome variable (vocabulary scores) would be organized into a single column. Similarly, the predictor that designates time (grade-level in our example) would also be organized into a single column.
- In **wide** structured data, the outcome variable (or predictor variables) is typically spread out over multiple columns. Often there are not columns that include data on the time predictor; instead this information is typically embedded in the column name.

The vocabulary data is currently structured as wide data; the vocabulary scores are organized into four separate columns and the information about grade-level (the time predictor) is embedded in the variable names (e.g., `vocab_08` indicates 8th-grade). The same data are presented below in the tidy/long structure.

```{r echo=FALSE}
vocabulary %>%
  pivot_longer(cols = vocab_08:vocab_11, names_to = "grade", values_to = "vocab_score") %>%
  arrange(id, grade) %>%
  head(12) %>%
  kable(
    caption = "Vocabulary Data Presented in the Tidy/Long Format",
    format = "latex",
    booktabs = TRUE,
    align = "c"
    ) %>%
  kable_styling(full_width = FALSE, latex_options = "HOLD_position")
```

Notice that in the tidy/long structured data that the vocabulary scores (outcome) are now organized into a single column. Grade-level (the time predictor) is also now explicitly included in the data and is also organized as a single column in the data. Note that in the long structure, each row now represents a particular student at a particular grade-level, and that each student's data now encompasses several rows.

There are advantages to each of the structures. For example the wide structure has the advantage of being a better structure for data entry. Since each row corresponds to a different student, there are fewer rows and therefore less redundancy in the data entry process. Compare this to the tidy/long data where each student's data encompasses four rows. If you were doing data entry in this structure you would need to record the student's sex four times rather than once in the wide structure.

The tidy/long structure is the structure that is needed for modeling. Thus, if one of the analytic goals is to fit a linear mixed-effects model to explain variation or examine predictor effects, the tidy/long data structure is key. Note that the wide structured data is also used in some analyses (e.g., computing correlations).

<br />


## Switching between the Two Data Structures

The library `{tidyr}` (loaded as part of the `{tidyverse}` metapackage) has two functions, `pivot_longer()` (wide $\rightarrow$ tidy/long) and `pivot_wider()` (tidy/long $\rightarrow$ wide), that convert data between these two structures. Below, I show the code for going from the wide structured data (`vocabulary`) to the tidy/long structure.

```{r}
# Convert from wide to long structured data
vocabulary_long = vocabulary %>%
  pivot_longer(cols = vocab_08:vocab_11, names_to = "grade", values_to = "vocab_score") %>%
  arrange(id, grade)

# View data
vocabulary_long
```

For more information about using these functions, Google "tidyr pivot" and read through any number of great tutorials or vignettes; for example [here](https://tidyr.tidyverse.org/articles/pivot.html). You can also read Hadley Wickham's [-@Wickham:2014] original paper on tidy data.

<br />


# Exploration: Plot of the Mean and Individual Profiles

There are two plots that are particularly useful in exploring longitudinal data. The first is a plot of the mean value of the outcome at each time point (*mean profile plot*). This shows the average growth profile and is useful for determining the functional form of the fixed-effects part of the model; is the mean change over time linear? Quadratic? Log-linear? 

Another plot that is often examined is a plot of the individual patterns or profiles, referred to as a *spaghetti plot*. A spaghetti plot is useful for determining whether there is variation from the average profile. This helps us to consider the set of random-effects to include in the model. Below we examine both the mean profile and individual profiles simultaneously.

```{r fig.width=6, fig.height=6, fig.cap='Plot showing the change in vocabulary score over time for 64 students. The average growth profile is displayed as a thicker line.'}
ggplot(data = vocabulary_long, aes(x = grade, y = vocab_score)) +
  geom_line(aes(group = id), alpha = 0.3) +                      #Add individual profiles
  stat_summary(fun = mean, geom = "line", size = 2, group = 1) + #Add mean profile line
  stat_summary(fun = mean, geom = "point", size = 3) +           #Add mean profile points
  theme_light() +
  scale_x_discrete(
    name = "Grade-level",
    labels = c("8th-grade", "9th-grade", "10th-grade", "11th-grade")
    ) +
  ylab("Vocabulary score")
```

Based on this plot:

- The average profile displays change over time that is positive (growth) and linear (or perhaps log-linear).
- The individual profiles show variation from the average profile; they have different vocabulary scores in 8th-grade and the profiles themselves vary in terms of their change (some show more change; others show decline)

<br />


# Modeling: Unconditional Random Intercepts Model

As in a cross-sectional analysis we begin a longitudinal analysis by fitting the unconditional random intercepts model. The statistical model in this example can be expressed as:

$$
\mathrm{Vocabulary~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \epsilon_{ij}
$$

where,

- $\mathrm{Vocabulary~Score}_{ij}$ is the vocabulary score at time point $i$ for student $j$;
- $\beta_0$ is the fixed-effect of intercept;
- $b_{0j}$ is the random-effect of intercept for student $j$; and
- $\epsilon_{ij}$ is the error at time point $i$ for student $j$.


```{r}
# Fit unconditional random intercepts model
lmer.0 = lmer(vocab_score ~ 1 + (1|id), data = vocabulary_long, REML = FALSE)

# Coefficient-level output
tidy(lmer.0)

# Compute variance components
1.35 ^ 2 #Residual
1.72 ^ 2 #Intercept
```

Fitting the unconditional random intercepts model gives us our baseline comparison model. The variance components suggest that there is unexplained within-student variation ($\hat\sigma^2_{\epsilon}=1.83$) and unexplained between-student variation ($\hat\sigma^2_{\mathrm{ID}}=2.95$). Most of the unexplained variation seems to be between-student variation (61.8%).

<br />


# Modeling: Unconditional Growth Model

We can now add the fixed-effect of time (the time predictor) to the model. In this data set, the time predictor is `grade`, which is a categorical predictor. We could create dummy variables, or simply add `grade` into the model and let R choose the reference group alphabetically (`vocab_08` in this example). The statistical model in this example can be expressed as:

$$
\mathrm{Vocabulary~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Vocab\_09}_{ij}) + \beta_2(\mathrm{Vocab\_10}_{ij}) + \beta_3(\mathrm{Vocab\_11}_{ij}) + \epsilon_{ij}
$$

where,

- $\mathrm{Vocabulary~Score}_{ij}$ is the vocabulary score at time point $i$ for student $j$;
- $\beta_0$ is the fixed-effect of intercept;
- $b_{0j}$ is the random-effect of intercept for student $j$;
- $\mathrm{Vocab\_09}_{ij}$, $\mathrm{Vocab\_10}_{ij}$, and $\mathrm{Vocab\_11}_{ij}$ are dummy coded variable indicating grade-level,
- $\beta_1$ is the effect of 9th-grade (i.e., mean vocabulary score difference between 8th- and 9th-grade),
- $\beta_2$ is the effect of 10th-grade (i.e., mean vocabulary score difference between 8th- and 10th-grade),
- $\beta_3$ is the effect of 11th-grade (i.e., mean vocabulary score difference between 8th- and 11th-grade), and
- $\epsilon_{ij}$ is the error at time point $i$ for student $j$.


Fitting the model:

```{r}
# Fit unconditional growth model
lmer.1 = lmer(vocab_score ~ 1 + grade + (1|id), data = vocabulary_long, REML = FALSE)

# Coefficient-level output
tidy(lmer.1)

# Compute variance components
0.899 ^ 2 #Residual
1.791 ^ 2 #Intercept
```

The fitted equation is:

$$
\hat{\mathrm{Vocabulary~Score}_{ij}} = 1.13 + 1.41(\mathrm{Vocab\_09}_{ij}) + 1.86(\mathrm{Vocab\_10}_{ij}) + 2.34(\mathrm{Vocab\_11}_{ij})
$$

Interpreting the coefficients,

- The predicted average vocabulary score for 8th-grade students (intercept) is 1.13.
- On average, 9th-grade students have a vocabulary score that is 1.41-points higher than 8th-grade students.
- On average, 10th-grade students have a vocabulary score that is 1.86-points higher than 8th-grade students.
- On average, 11th-grade students have a vocabulary score that is 2.34-points higher than 8th-grade students.

Looking at the variance components and comparing them to those from the unconditional random intercepts model:

- The model has explained 55.8% of the within-student variation. This is because grade is a within-student predictor (it has values that vary within each student).
- The model has *increased* the variation between-students ($-8.7$%). This is a mathematical artifact of the estimation process.

<br />


## Repeated-Measures ANOVA (RM-ANOVA)

One historic method of analyzing longitudinal data is Repeated Measures Analysis of Variance (RM-ANOVA).^[Unfortunately RM-ANOVA is still used, despite many known methodological limitations of the methodology, and the availability of better analytic options (e.g., LMER).]  The linear mixed-effects model that includes time as one or more categorical predictors and a random-effect of intercept produces the same results as the RM-ANOVA.

:::note
You should not use RM-ANOVA to analyze longitudinal data. It requires a condition called *sphericity* that makes some stringent assumptions about the variances and correlations between repeated measures that are never met in practice. Instead, use LMER to carry out these analyses.
:::

<br />


# Quantitative Time Predictor: A More Flexible Model for Repeated Measures Data

One advantage to using the linear mixed-effects model to analyze repeated measures data over traditional methods (e.g., RM-ANOVA or MANOVA) is that the regression model allows for both categorical and quantitative variables. For example, rather than code our grade-levels categorically (as `vocab_08`, `vocab_09`, `vocab_10` and `vocab_11`), which was a necessity in days of yore, we could have simply coded them as 8, 9, 10, and 11. Then we could have fitted the LME model using this quantitative predictor. The statistical model when time is quantitative would be:

$$
\mathrm{Vocabulary~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Grade}_{ij}) + \epsilon_{ij}
$$


where,

- $\mathrm{Vocabulary~Score}_{ij}$ is the vocabulary score at time point $i$ for student $j$;
- $\beta_0$ is the fixed-effect of intercept;
- $b_{0j}$ is the random-effect of intercept for student $j$;
- $\mathrm{Grade}_{ij}$ is a quantitative variable indicating grade-level,
- $\beta_1$ is the effect of a one-unit change in grade, and
- $\epsilon_{ij}$ is the error at time point $i$ for student $j$.

This is still referred to as the *unconditional growth model* since the only predictor is a fixed-effect of time.

We will also look at a model that treats grade as a quantitative predictor, but also centers this predictor on the initial time point. This model is:

$$
\mathrm{Vocabulary~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Grade}_{ij}-8) + \epsilon_{ij}
$$

<br />


## Lookup Table: Mapping Categories to Quantities

One method to convert `grade` to a quantitative variable is to create a **lookup table**. A lookup table maps the levels of the categorical time predictor to the values we want to use in our new quantitative predictor. Below I create a lookup table to map the categorical time predictor to the relevant grade-level (`grade_quant`) and also to a centered grade level (`grade_quant_center`) which centers the `grade_quant` predictor by subtracting 8 from each value.


```{r message=FALSE, warning=FALSE}
# Create lookup table
lookup_table = data.frame(
  grade = c("vocab_08", "vocab_09", "vocab_10", "vocab_11"),
  grade_quant = c(8, 9, 10, 11),
  grade_quant_center = c(0, 1, 2, 3)
)

# View lookup table
lookup_table
```

Then, we join the tidy/long data with the lookup table.


```{r message=FALSE, warning=FALSE}
# Join the data with the lookup table
vocabulary_long = vocabulary_long %>%
  left_join(lookup_table, by = "grade")

# View data
vocabulary_long
```

This adds the quantitative variables (with the correct mapping) to our tidy data. There are, of course, other ways to accomplish the same thing. For example a `mutate()` using the `case_when()` function could also be used to create this mapping.


<br />


## Fitting the Unconditional Growth Model with a Quantitative Time Predictor

Below we fit the linear mixed-effects model using the `grade_quant` predictor.

```{r}
# Fit unconditional growth model
lmer.1_quant = lmer(vocab_score ~ 1 + grade_quant + (1|id), data = vocabulary_long, REML = FALSE)

# Coefficient-level output
tidy(lmer.1_quant)

# Compute variance components
0.947 ^ 2 #Residual
1.784 ^ 2 #Intercept
```

The fitted equation is:

$$
\hat{\mathrm{Vocabulary~Score}_{ij}} = -4.56 + 0.75(\mathrm{Grade\mbox{-}level}_{ij})
$$

The model using the quantitative predictor of grade-level is simpler than the model using the categorical version of grade-level since it has two fewer fixed-effects to estimate (fewer model degrees-of-freedom).

Interpreting the coefficients,

- The predicted average vocabulary score for 0th-grade students (intercept) is -4.55 (extrapolation).
- Each one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average.

Looking at the variance components and comparing them to the unconditional random intercepts model:

- The unconditional growth model has explained 50.8% of the within-student variation.
- The unconditional growth model has *increased* the variation between-students ($-7.8$%). This is a mathematical artifact of the estimation process.

This is similar to the variance components obtained from the model using the categorical predictors of grade level.

<br />


## Centering the Time Predictor: Better Interpretations of the Intercept

Now, let's fit the model using the quantitative predictor that was centered on the initial time point.

```{r}
# Fit unconditional growth model with centered grade
lmer.1_quant_cent = lmer(vocab_score ~ 1 + grade_quant_center + (1|id), data = vocabulary_long, REML = FALSE)

# Coefficient-level output
tidy(lmer.1_quant_cent)

# Compute variance components
0.947 ^ 2 #Residual
1.784 ^ 2 #Intercept
```

The fitted equation is:

$$
\hat{\mathrm{Vocabulary~Score}_{ij}} = 1.41 + 0.75(\mathrm{Centered~grade\mbox{-}level}_{ij})
$$

or,

$$
\hat{\mathrm{Vocabulary~Score}_{ij}} = 1.41 + 0.75(\mathrm{Grade\mbox{-}level}_{ij}-8)
$$


Interpreting the coefficients,

- The predicted average vocabulary score for 8th-grade students is 1.41. Centering removes the problem of extrapolation in the interpretation because we have now made 0 a legitimate value in the predictor.
- Each one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average. This is identical to the previous model since we have not changed what a one-unit difference in the predictor represents.


We can see why the intercepts are different but the slopes are the same by comparing the plots of the individual growth profiles and the fitted fixed-effects models for the two predictors.

```{r fig.width=8, fig.height=4, fig.cap='Plot showing the change in vocabulary score over time for 64 students. The average growth profile is also displayed. This is shown for the non-centered (left) and 8th-grade centered (right) grade-level. A large blue point is shown at the intercept value in both plots.', out.width='90%', echo=FALSE}
p1 = ggplot(data = vocabulary_long, aes(x = grade_quant, y = vocab_score)) +
  geom_line(aes(group = id), alpha = 0.3) +
  geom_abline(intercept = -4.56, slope = 0.75, color = "blue") +
  geom_point(x = 0, y = -4.56, size = 1.5, color = "blue") +
  theme_light() +
  scale_x_continuous(
    name = "Grade-level", 
    limits = c(0, 11), 
    breaks = c(0, 2, 4, 6, 8, 10)
    ) +
  scale_y_continuous(
    name = "Vocabulary score", 
    limits = c(-5, 10)
    )

p2 = ggplot(data = vocabulary_long, aes(x = grade_quant_center, y = vocab_score)) +
  geom_line(aes(group = id), alpha = 0.3) +
  geom_abline(intercept = 1.41, slope = 0.75, color = "blue") +
  geom_point(x = 0, y = 1.41, size = 1.5, color = "blue") +
  theme_light() +
  scale_x_continuous(
    name = "Centered grade-level (0 = 8th grade)", 
    limits = c(0, 11), 
    breaks = c(0, 2, 4, 6, 8, 10)
    ) +
  scale_y_continuous(
    name = "Vocabulary score", 
    limits = c(-5, 10)
    )

# Display plots
p1 | p2
```


Looking at the variance components and comparing them to the unconditional random intercepts model:

- The model has explained 50.8% of the within-student variation.
- The model has *increased* the variation between-students ($-7.8$%). This is a mathematical artifact of the estimation process.

These values are identical to the variance components obtained from the previous model.

<br />


## Comparing the Unconditional Growth Models

The unconditional growth model was fitted using three different methods.

- The first model treated time (grade) as categorical.
- The second two models treated time as a continuous variable.


Treating time continuously rather than as a categorical predictor has many advantages:

- In the real-world time is continuous.
- The model is simpler. In the model where time was treated categorically, we had to estimate four regression coefficients and two variance components. In the continuous models, we only had to estimate two regression coefficients and two variance components. If there were additional time points, we would still only need to estimate four parameters for the continuous model, but the number of estimates would increase for the categorical model.
- It allows us to include participants being measured at different times.
- It allows us to model nonlinear relationships more easily.


:::note
In general, **you should always treat time continuously when you have a longitudinal study!** This guidance also implies that you should use linear mixed-effects models rather than RM-ANOVA for the analysis of longitudinal data.

Moreover, because of the interpretive value of the intercept when we center the grade-level predictor at the first time point, it is often a good idea to center your time predictor.
:::

<br />


## Examining the Functional Form of the Growth Model

As in any regression analysis. we need to specify the functional form of the growth model. Since the spaghetti plot suggested that the relationship between grade-level and vocabulary score may be nonlinear, below we consider three potential functional forms for our growth model:

1. A linear relationship (fitted already as `lmer.3`);
2. A quadratic relationship; and
3. A log-linear relationship (based on log-transforming grade-level).

*Note:* Since the centered grade predictor includes values of zero, we need to add one to each value prior to log-transforming this predictor


```{r}
# Quadratic model
lmer.quad = lmer(vocab_score ~ 1 + grade_quant_center + I(grade_quant_center^2) + (1|id),
              data = vocabulary_long, REML = FALSE)

# Log-linear model
lmer.log = lmer(vocab_score ~ 1 + log(grade_quant_center + 1) + (1|id),
              data = vocabulary_long, REML = FALSE)
```



## Evaluating the Three Functional Forms

As with any model, we want to examine the residuals to see which of the potential candidate models meets the assumptions. In this examination, we will examine both the level-1 and level-2 residuals.

```{r}
# Obtain level-1 resduals and fitted values
mod1_lev1 = augment(lmer.1_quant_cent)
quad_lev1 = augment(lmer.quad)
log_lev1 = augment(lmer.log)

# Obtain a data frame of the random-effects (level-2 residuals)
mod1_lev2 = ranef(lmer.1_quant_cent)$id
quad_lev2 = ranef(lmer.quad)$id
log_lev2 = ranef(lmer.log)$id
```

We will then create the residual plots for the three models

```{r fig.width=9, fig.height=9, out.width='100%', fig.cap='Plots of the level-1 residuals and random-effects for the model with a linear Effect of Grade-Level (Row 1), linear and quadratic effects of grade-level (Row 2), and log-transformed effect of grade-level (Row 3).'}
##################### LINEAR MODEL #####################

# Density plot of the level-1 residuals
p1 = ggplot(data = mod1_lev1, aes(x = .resid)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-1 residuals") +
  ggtitle("Linear Model")

# Scatterplot of the Level-1 residuals versus the fitted values
p2 = ggplot(data = mod1_lev1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Level-1 residuals")

# Density plot of the level-2 residuals (RE of intercept)
p3 = ggplot(data = mod1_lev2, aes(x = `(Intercept)`)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-2 residuals")


##################### NONLINEAR (QUADRATIC) MODEL #####################

# Density plot of the level-1 residuals
p4 = ggplot(data = quad_lev1, aes(x = .resid)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-1 residuals") +
  ggtitle("Nonlinear Model (Quadratic)")

# Scatterplot of the Level-1 residuals versus the fitted values
p5 = ggplot(data = quad_lev1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Level-1 residuals")

# Density plot of the level-2 residuals (RE of intercept)
p6 = ggplot(data = quad_lev2, aes(x = `(Intercept)`)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-2 residuals")

##################### NONLINEAR (LOG) MODEL #####################

# Density plot of the level-1 residuals
p7 = ggplot(data = log_lev1, aes(x = .resid)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-1 residuals") +
  ggtitle("Nonlinear Model (Log)")

# Scatterplot of the Level-1 residuals versus the fitted values
p8 = ggplot(data = log_lev1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Level-1 residuals")

# Density plot of the level-2 residuals (RE of intercept)
p9 = ggplot(data = log_lev2, aes(x = `(Intercept)`)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-2 residuals")

# Layout plots
(p1 | p2 | p3) / (p4 | p5 | p6) / (p7  | p8 | p9)
```

The residual plots look similar indicating that all the models seem to meet the assumptions equally well. Since we are using the same outcome and data in all three models, we can also evaluate the models using information criteria and their related metrics. 

```{r}
# Model-evidence
aictab(
  cand.set = list(lmer.1_quant_cent, lmer.quad, lmer.log),
  modnames = c("Linear", "Quadratic", "Log-linear")
)
```

Given the data and candidate models, the evidence primarily supports the log-linear model. There is also some evidence for the quadratic model and almost no evidence for the linear model. This is consistent with the nonlinearity we observed in the mean profile earlier. Given this, the higher evidence for the log-linear model, and the simplicity of the log-linear model relative to the quadratic model, we will adopt the log-linear functional form for our unconditional growth model.

<br />

## Examining the Output for the Adopted Log-Linear Fitted Model


```{r}
tidy(lmer.log)

# Compute variance components
0.907 ^ 2 #Residual
1.790 ^ 2 #Intercept
```


The fitted equation is:

$$
\hat{\mathrm{Vocabulary~Score}_{ij}} = 1.21 + 1.67\bigg[\ln(\mathrm{Centered~grade\mbox{-}level}_{ij}+1)\bigg]
$$


Interpreting the coefficients,

- The predicted average vocabulary score for 8th-grade students is 1.21. Remember that the centered value for 8th-grade is 0, which results in $1.21 + 1.67\bigg[\ln(1)\bigg] = 1.21 + 1.67(0) = 1.21$.
- Since we used the natural log, we can interpret the change in *X* as a percent change and the changein *Y* as $\hat{\beta_1}/100$; Each one-percent difference in grade-level is associated with a 0.0166-point difference in vocabulary score, on average.

Looking at the variance components:

- The model has explained 54.9% of the within-student variation.
- The model has *increased* the variation between-students ($-8.4$%). This is a mathematical artifact of the estimation process.

These values are quite similar to the variance components obtained from the other unconditional growth models.

<br />


## Plot of the Unconditional Growth Model

To better understand the relationship between grade-level and vocabulary score represented in the adopted unconditional growth model, we can plot the predicted mean profile based on the model's fixed-effects.

```{r fig.width=6, fig.height=6, fig.cap='Predicted change in vocabulary score as a function of grade-level.'}
# Create plot
ggplot(data = vocabulary_long, aes(x = grade_quant_center, y = vocab_score)) +
  geom_point(alpha = 0) +
  geom_function(
    fun = function(x) {1.21 + 1.67 * log(x + 1)},
    color = "blue"
    ) +
  theme_light() +
  scale_x_continuous(
    name = "Grade-level",
    breaks = c(0, 1, 2, 3),
    labels = c("0\n(8th)", "1\n(9th)", "2\n(10th)", "3\n(11th)")
    ) +
  ylab("Vocabulary score")
```

Based on this plot and the coefficient-level output, we can answer the first research question.

:::interpret
The growth pattern in vocabulary is log-linear over time. While the change in vocabulary score, on average, is positive, the growth rate somewhat diminishes over time.
:::


<br />


# Examining the Second Research Question

To answer the second research question about whether the growth pattern is different for females and non-females, we again start by plotting the individual and mean growth profiles, this time conditioning on sex.

```{r fig.width=8, fig.height=4, fig.cap='Plot showing the change in vocabulary score over time for 64 students conditioned on sex (female; non-female). The average growth profile for females and non-females are also displayed as a thicker lines.', out.width='80%'}
# Turn female into factor for better plotting
vocabulary_long %>%
  mutate(
    # Create factor for better plotting
    Sex = factor(female, levels = c(0, 1), labels = c("Non-female", "Female"))
  ) %>%
ggplot(aes(x = grade_quant, y = vocab_score, color = Sex)) +
  geom_line(aes(group = id), alpha = 0.3) +
  stat_summary(fun = mean, geom = "line", size = 2, group = 1) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  theme_light() +
  xlab("Grade-level") +
  ylab("Vocabulary score") +
  facet_wrap(~Sex) +
  ggsci::scale_color_d3() +
  guides(color = FALSE)
```

Examining the average profiles for females and non-females in this plot suggests that the females tend to have higher average vocabulary scores than non-females at each grade level. The sample average growth profiles also show slightly different patterns of growth between females and non-females. To further examine these potential differences, we can fit a set of candidate models that include fixed-effects of sex in addition to the grade-level predictor, namely:

1. A model that includes main effects of sex and grade-level; and
2. A model that includes main effects of sex and grade-level and the interaction-effect between sex and grade-level.

```{r}
# Main-effect of sex
lmer.main = lmer(vocab_score ~ 1 + log(grade_quant_center + 1) + female +
                (1|id), data = vocabulary_long, REML = FALSE)

# Interaction-effect between sex and grade-level
lmer.int = lmer(vocab_score ~ 1 + log(grade_quant_center + 1) + female +
                  log(grade_quant_center + 1):female + (1|id),
                data = vocabulary_long, REML = FALSE)
```

These two models allow us to test different hypotheses about the patterns of growth between males and females:

- If there is a main-effect of sex, it will allow us to conclude that the growth pattern is the same, but that the average females vocabulary score is systematically different than that for non-females at each time point (e.g., always lower or higher by the same amount).
- If there is an interaction-effect between sex and grade-level, it will allow us to conclude that the pattern of change over time is different between females and non-females.

To facilitate model selection we will examine a table of model evidence using the following candidate models: (1) the adopted unconditional growth model, (2) the model that includes main-effects of grade-level and sex, and (3) the model that includes an interaction-effect between grade-level and sex.

```{r}
# Model-evidence
aictab(
  cand.set = list(lmer.log, lmer.main, lmer.int),
  modnames = c("Unconditional Growth", "Sex Main Effect", "Sex Interaction Effect")
)
```

Here the empirical evidence (given the candidate models and data) slightly favors the model that includes the main-effect of sex. There is also a fair bit of evidence to support the interaction model. Again, we should probably examine the residuals from both of these models and adopt the model that better meets the assumptions.


```{r}
# Obtain the level-1 residuals and fitted values
main_lev1 = augment(lmer.main)
int_lev1 = augment(lmer.int)

# Obtain a data frame of the random-effects
main_lev2 = ranef(lmer.main)$id
int_lev2 = ranef(lmer.int)$id
```

```{r fig.width=9, fig.height=6, out.width='100%', fig.cap='Plots of the level-1 residuals and random-effects for the main effects model (Row 1) and the interaction model (Row 2).'}
##################### MAIN EFFECTS MODEL #####################

# Density plot of the level-1 residuals
p1 = ggplot(data = main_lev1, aes(x = .resid)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-1 residuals") +
  ggtitle("Main Effects Model")

# Scatterplot of the Level-1 residuals versus the fitted values
p2 = ggplot(data = main_lev1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Level-1 residuals")

# Density plot of the level-2 residuals (RE of intercept)
p3 = ggplot(data = main_lev2, aes(x = `(Intercept)`)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-2 residuals")


##################### INTERACTION MODEL #####################

# Density plot of the level-1 residuals
p4 = ggplot(data = int_lev1, aes(x = .resid)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-1 residuals") +
  ggtitle("Interaction Model")

# Scatterplot of the Level-1 residuals versus the fitted values
p5 = ggplot(data = int_lev1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_light() +
  xlab("Fitted values") +
  ylab("Level-1 residuals")

# Density plot of the level-2 residuals (RE of intercept)
p6 = ggplot(data = int_lev2, aes(x = `(Intercept)`)) +
  stat_density_confidence(model = "normal") +
  stat_density(geom = "line") +
  theme_light() +
  xlab("Level-2 residuals")

# Layout plots
(p1 | p2 | p3) / (p4 | p5 | p6)
```


The residuals for both models look fairly reasonable. Neither model shows improved fit based on these plots.

Using the results from evaluating the table of model evidence and the residual plots, there is evidence to support both the models that include the main-effects of grade-level and sex, as well as the model that includes the interaction-effect between grade-level and sex. We will interpret the coefficients and variance components from each of these models, and also plot their fixed-effects.

:::protip
If you opted to choose the main effects model because of parsimony, it would be prudent to point out that the evidence also showed some support for the interaction model. This would cue researchers to continue to probe whether this effect is supported in future research.
:::

<br />


## Main Effects Model

```{r}
tidy(lmer.main)

# Compute variance components
0.907 ^ 2 #Residual
1.232 ^ 2 #Intercept
```

The fitted equation is:

$$
\hat{\mathrm{Vocabulary~Score}_{ij}} = -0.01 + 1.67\bigg[\ln(\mathrm{Grade\mbox{-}level}_{ij}-8+1)\bigg] + 2.60(\mathrm{Female}_{\boldsymbol{\cdot}j})
$$



Interpreting the coefficients,

- The predicted average vocabulary score for non-female 8th-grade students is $-0.01$.
- Each one-percent difference in grade-level is associated with a 0.0167-point difference in vocabulary score, on average, controlling for differences in sex.
- Females have an average vocabulary score that is 2.60-points higher than non-females, controlling for differences in grade-level.

Looking at the variance components and comparing to the unconditional random intercepts model:

- The model has explained 54.9% of the within-student variation.
- The model has explained 16.8% of the between-student variation.

We expect the model to explain variation between-students as the `female` predictor we included was a between-students predictor.

```{r fig.width=6, fig.height=6, fig.cap='Predicted change in vocabulary score as a function of grade-level for females (orange, solid line) and non-females (Blue, dashed line).'}
ggplot(data = vocabulary_long, aes(x = grade_quant_center, y = vocab_score)) +
  geom_point(alpha = 0) +
  geom_function(
    fun = function(x) {-0.01 + 1.67 * log(x + 1)},
    color = "blue",
    linetype = "dashed"
    ) +
  geom_function(
    fun = function(x) {2.59 + 1.67 * log(x + 1)},
    color = "orange"
    ) +
  theme_light() +
  scale_x_continuous(
    name = "Grade-level",
    breaks = c(0, 1, 2, 3),
    labels = c("0\n(8th)", "1\n(9th)", "2\n(10th)", "3\n(11th)")
    ) +
  ylab("Vocabulary score")
```

Plotting this model we find that the growth pattern in vocabulary is log-linear over time for both females and males. While the change in vocabulary score, on average, is positive for both sexes, the growth rate somewhat diminishes over time. Moreover, while females tend to have a higher vocabulary score at each grade level, the change patterns seem to have the same rate of growth for both sexes.

<br />


## Interaction Model

```{r}
tidy(lmer.int)

# Compute variance components
0.903 ^ 2 #Residual
1.233 ^ 2 #Intercept
```

The fitted equation is:

$$
\begin{split}
\hat{\mathrm{Vocabulary~Score}_{ij}} = &-0.11 + 1.79\bigg[\ln(\mathrm{Grade\mbox{-}level}_{ij}-8+1)\bigg] + 2.81(\mathrm{Female}_{\boldsymbol{\cdot}j}) \\
& - 0.27\bigg[\ln(\mathrm{Grade\mbox{-}level}_{ij}-8+1)\times(\mathrm{Female}_{\boldsymbol{\cdot}j})\bigg]
\end{split}
$$



Interpreting the coefficients ,

- The predicted average vocabulary score for male 8th-grade students is -0.11.
- For males, each one-percent difference in grade-level is associated with a 0.0179-point difference in vocabulary score, on average.
- Eighth-grade females have an average vocabulary score that is 2.81-points higher than 8th-grade males.
- For females, each one-percent difference in grade-level is associated with a 0.0152-point difference in vocabulary score, on average. This is less than the effect for males by 0.0027.

More generally, we might say:

- The effect of grade-level differs by sex.
- The effect of sex differs by grade-level

Looking at the variance components:

- The model has explained 55.3% of the within-student variation.
- The model has explained 16.7% of the between-student variation.

```{r fig.width=6, fig.height=6, fig.cap='Predicted change in vocabulary score as a function of grade-level for females (orange, solid line) and non-females (blue, dashed line).'}
ggplot(data = vocabulary_long, aes(x = grade_quant_center, y = vocab_score)) +
  geom_point(alpha = 0) +
  geom_function(
    fun = function(x) {-0.11 + 1.79 * log(x + 1)},
    color = "blue",
    linetype = "dashed"
    ) +
  geom_function(
    fun = function(x) {2.7 + 1.52 * log(x + 1)},
    color = "orange"
    ) +
  theme_light() +
  scale_x_continuous(
    name = "Grade-level",
    breaks = c(0, 1, 2, 3),
    labels = c("0\n(8th)", "1\n(9th)", "2\n(10th)", "3\n(11th)")
    ) +
  ylab("Vocabulary score")
```


Plotting this model we find that:

- The growth pattern in vocabulary is log-linear over time for both females and non-females.
- While the change in vocabulary score, on average, is positive for females and non-females, the growth rate somewhat diminishes over time.
- Moreover, while females tend to have a higher vocabulary score at each grade level, the growth rate for females is slightly smaller than that for non-females.

<br />


# References



