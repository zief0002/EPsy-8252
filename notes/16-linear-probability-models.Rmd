---
title: "Linear Probability Models"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
    highlight: zenburn
    fig_width: 6
    fig_height: 6
header-includes: 
  - \usepackage{graphicx}
  - \usepackage{rotating}
  - \usepackage{xcolor}
  - \definecolor{umn}{HTML}{FF2D21}
  - \usepackage{caption}
  - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \usepackage{floatrow}
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
  - \usepackage{xfrac}
mainfont: "Sabon"
sansfont: "Futura"
monofont: Inconsolata    
urlcolor: "umn"
bibliography: [epsy8251.bib, epsy8252.bib]
csl: apa-single-spaced.csl
---

<!-- Set the document spacing -->
\frenchspacing


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE, fig.width=6, fig.height=6,
               fig.pos='H', fig.align='left')
opts_knit$set(width=85)
options(scipen=5)
```


# Preparation

In this set of notes, you will learn how about linear probability models, and why they are not typically used to model dichotomous categorical outcome variables (e.g., dummy coded outcome). We will use data from the file *graduation.csv* (see the [data codebook](http://zief0002.github.io/epsy-8252/codebooks/graduation.html)) to explore predictors of college graduation. 



```{r message=FALSE}
# Load libraries
library(broom)
library(corrr)
library(educate)
library(patchwork)
library(tidyverse)

# Read in data
grad = read_csv(file = "~/Documents/github/epsy-8252/data/graduation.csv")

# View data
head(grad)
```

Note that in these analyses the outcome variable (`degree`) is a dichotomous dummy-coded categorical variable indicating whether or not a student graduated.

\newpage

# Data Exploration

To begin the analysis, we will explore the outcome variable `degree`. Since this is a categorical variable, we can look at counts and proportions. The analysis suggests that most students in the sample tend to graduate (73\%). 

```{r}
grad %>% 
  group_by(degree) %>% 
  summarize(
    Count = n(), 
    Prop = n() / nrow(grad)
    )
```

We will also explore the `act` variable, which we will use as a predictor in the analysis. 

```{r out.width='3in', fig.cap='Density plot of the ACT scores.', fig.pos='H'}
# Density plot
ggplot(data = grad, aes(x = act)) +
  geom_density() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Probability density")
```

\newpage

```{r}
# Summary measures
grad %>% 
  summarize( 
    M = mean(act), 
    SD = sd(act) 
    )
```

The distribution is unimodal and symmetric. It indicates that the sample of students have a mean ACT score near 25. While there is a great deal of variation in ACT scores (scores range from 10 to 36), most students have a score between 21 and 29. 


## Relationship between ACT Score and Graduation

When the outcome variable was continous, we examined relationships graphically via a scatterplot. If we try that when the outcome is dichotomous, we run into problems. Since there are only two values for the outcome, many of the observations are over-plotted, and it is impossible to tell anything about the relationship between the variables. 

```{r out.width='45%', fig.cap='Scatterplot of whether a student graduated versus ACT score. In the right-hand plot, the points have been jittered to alleviate over-plotting, and the regression smoother has also been added to the plot.', fig.pos='H', fig.show='hold'}
# Scatterplot
ggplot(data = grad, aes(x = act, y = degree)) +
  geom_point() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Graduated")

# Jittered scatterplot
ggplot(data = grad, aes(x = act, y = jitter(degree))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Graduated")
```

One solution to this problem is to add (or subtract) a tiny amount to each student's `degree` value. This is called "jittering" the observations". To do this, use the `jitter()` function. (Note that the amount of jittering can be adjusted by including an additional argument to the `jitter()` function.) This spreads out the observations vertically, so we no longer have the problem of overplotting. But, the relationship is still difficult to see in this scatterplot. Adding the regression smoother helps us see that there is a positive relationship between ACT scores and graduation. 

What does this mean? To understand this, we have to think about what the linear regression is modeling. Remember that the regression model is predicting the average $Y$ at each $X$. Since our $Y$ is dichotomous, the average represents the proportion of students with a 1. In other words, the regression predicts the proportion of students who graduate for a particular ACT score. The positive relationship between ACT score and graduation suggests that higher ACT scores are associated with higher proportions of students who graduate. We can also see this relationship by examining the correlation matrix between the two variables.

```{r}
grad %>% 
  select(degree, act) %>% 
  correlate()
```

\newpage

# Fitting the Linear Probability (Proportion) Model

We can fit the linear model shown in the plot using the `lm()` function as we always have.

```{r}
# Fit the model
lm.1 = lm(degree ~ 1 + act, data = grad)

# Model-level- output
glance(lm.1)
```

Differences in ACT score account for 3.8\% of the variation in graduation. This is more than we would expect by chance if the null hypothesis was true, $F(1,~2342)=92.8$, $p<.001$. 

```{r}
# Coefficient-level- output
tidy(lm.1)
```



The fitted model is

$$
\hat{\pi}_i = 0.22 + 0.02(\mathrm{ACT~Score}_i)
$$

where $\hat{\pi}_i$ is the predicted proportion of students who graduate. Interpreting the coefficients,

- On average, 0.22 of students having an ACT score of 0 are predicted to graduate. (Extrapolation)
- Each one-point difference in ACT score is associated with an additional 0.02 predicted improvement in the proportion of students graduating, on average.

Let's examine the model assumptions.

```{r}
# Augment the model
out = augment(lm.1)
```

\newpage

```{r out.width='45%', fig.cap='LEFT: Density plot of the standardized residuals from the linear probability model. RIGHT: Scatterplot of the standardized residuals versus the fitted values from the linear probability model.', fig.pos='H', fig.show='hold'}
# View augmented data
head(out)

# Examine normality assumption
ggplot(data = out, aes(x = .std.resid)) +
  stat_density_confidence() +
  geom_density() +
  theme_bw() +
  xlab("Standardized residuals") +
  ylab("Probability density")

# Examine linearity and homoskedasticity
ggplot(data = out, aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_bw() +
  xlab("Fitted values") +
  ylab("Standardized residuals")
```


\newpage

It is clear that the assumptions associated with linear regression are violated. First off, the residuals are not normally distributed. They are in fact, bimodal. The scatterplot of the residuals versus the fitted values also indicates violation of the linearity assumption, as the average residual at each fitted value is not zero.

## Understanding the Residuals from the Linear Probability Model

Look closely at the scatterplot of the residuals versus the fitted values. At each fitted value, there are only two residual values. Why is this? Recall that residuals are computed as $\epsilon_i=Y_i - \hat{Y}_i$. Now, remember that $Y_i$ can only be one of two values, 0 or 1. Also remember that in the linear probability model $\hat{Y}_i=\hat{\pi}_i$. Thus, for $Y=0$,

$$
\begin{split}
\epsilon_i &= 0 - \hat{Y}_i \\
&= - \hat{\pi}_i
\end{split}
$$

And, if $Y=1$,

$$
\begin{split}
\epsilon_i &= 1 - \hat{Y}_i \\
&= 1 - \hat{\pi}_i
\end{split}
$$


This means that the residual computed using a particular fitted value (or from a particular ACT value given that the fitted values is a function of $X$) can only take on one of two values: $- \hat{\pi}_i$ or $1 - \hat{\pi}_i$. (Plotting the residuals for all fitted values, the marginal distribution of the residuals, results in a bimodal distribution for this same reason.)


