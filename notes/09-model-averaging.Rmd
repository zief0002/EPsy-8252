---
title: "Model Averaging"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
    highlight: zenburn
    fig_width: 6
    fig_height: 6
header-includes: 
  - \usepackage{graphicx}
  - \usepackage{rotating}
  - \usepackage{xcolor}
  - \definecolor{umn}{HTML}{FF2D21}
  - \usepackage{caption}
  - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \usepackage{floatrow}
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
  - \usepackage{xfrac}
mainfont: "Sabon"
sansfont: "Futura"
monofont: Inconsolata    
urlcolor: "umn"
bibliography: [epsy8251.bib, epsy8252.bib]
csl: apa-single-spaced.csl
---

<!-- Set the document spacing -->
\frenchspacing


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE, fig.width=6, fig.height=6,
               fig.pos='H', fig.align='left')
opts_knit$set(width=85)
options(scipen=5)
```

# Preparation

In this set of notes, we will learn about multimodel inference, an alternative to selecting a "best" model from a set of candidate models. To do so, we will use the *nels.csv* dataset (see the [data codebook](http://zief0002.github.io/epsy-8252/codebooks/nels.html)) to examine various prediction models to explain variation in students' school achievement.


```{r preparation, warning=FALSE, message=FALSE}
# Load libraries
library(broom)
library(educate) #Need version 0.1.0.1
library(MuMIn)
library(patchwork)
library(tidyverse)

# Import data
nels = read_csv("~/Documents/github/epsy-8252/data/nels.csv")

# View data
head(nels)
```

# Working Hypotheses and Candidate Models

Here, imagine a researcher who has five **scientific working hypotheses** about what underlies students' achievement:

- **H1:** Student achievement is a function of SES, prior achievment, and parent education level.
- **H2:** Student achievement is a function of SES, prior achievment, and parent education level, but also of the student's level of self-esteem.
- **H3:** Student achievement is a function of SES, prior achievment, and parent education level, but also of the student's degree of locus-of-control.
- **H4:** Student achievement is a function of SES, prior achievment, and parent education level, but also of the student's level of self-esteem and degree of locus-of-control.
- **H5:** Student achievement is a function of SES, prior achievment, and parent education level, but also of the interaction between student's level of self-esteem and degree of locus-of-control.

You should be able to tell by the scientific hypotheses, that this scenario represents a very exploratory approach to examining the underlying structure of the models (i.e., which predictors seem important). This is fine, we just need to acknowledge this so that our results re not seen as "final". Results from such an anlaysis are useful in helping plan a confirmatory analysis that can be carried out with new data.


The five working hypotheses are translated into the following five statistical models:


$$
\begin{split}
\mathbf{Model~1:~}\mathrm{Achievement}_i = &\beta_0 + \beta_1(\mathrm{SES}_i) + \beta_2(\mathrm{Prior~GPA}_i) + \beta_3(\mathrm{Parent~Education}_i) + \epsilon_i \\[2ex]
\mathbf{Model~2:~}\mathrm{Achievement}_i = &\beta_0 + \beta_1(\mathrm{SES}_i) + \beta_2(\mathrm{Prior~GPA}_i) + \beta_3(\mathrm{Parent~Education}_i) + \\ &\beta_4(\mathrm{Self\mbox{-}Esteem}_i) + \epsilon_i \\[2ex]
\mathbf{Model~3:~}\mathrm{Achievement}_i = &\beta_0 + \beta_1(\mathrm{SES}_i) + \beta_2(\mathrm{Prior~GPA}_i) + \beta_3(\mathrm{Parent~Education}_i) + \\ &\beta_4(\mathrm{Locus\mbox{-}of\mbox{-}Control}_i) + \epsilon_i \\[2ex]
\mathbf{Model~4:~}\mathrm{Achievement}_i = &\beta_0 + \beta_1(\mathrm{SES}_i) + \beta_2(\mathrm{Prior~GPA}_i) + \beta_3(\mathrm{Parent~Education}_i) + \\ &\beta_4(\mathrm{Self\mbox{-}Esteem}_i) + \beta_5(\mathrm{Locus\mbox{-}of\mbox{-}Control}_i) + \epsilon_i \\[2ex]
\mathbf{Model~5:~}\mathrm{Achievement}_i = &\beta_0 + \beta_1(\mathrm{SES}_i) + \beta_2(\mathrm{Prior~GPA}_i) + \beta_3(\mathrm{Parent~Education}_i) + \\ &\beta_4(\mathrm{Self\mbox{-}Esteem}_i) + \beta_5(\mathrm{Locus\mbox{-}of\mbox{-}Control}_i) + \beta_6(\mathrm{Self\mbox{-}Esteem}_i)(\mathrm{Locus\mbox{-}of\mbox{-}Control}_i) + \epsilon_i \\
\end{split}
$$

These models are then fitted to the data.

```{r}
# Fit Model 1
lm.1 = lm(social10 ~ 1 + ses + gpa + parent_ed, data = nels)

# Fit Model 2
lm.2 = lm(social10 ~ 1 + ses + gpa + parent_ed + self_esteem, data = nels)

# Fit Model 3
lm.3 = lm(social10 ~ 1 + ses + gpa + parent_ed + locus, data = nels)

# Fit Model 4
lm.4 = lm(social10 ~ 1 + ses + gpa + parent_ed + self_esteem + locus, data = nels)

# Fit Model 5
lm.5 = lm(social10 ~ 1 + ses + gpa + parent_ed + self_esteem + locus + 
            self_esteem:locus, data = nels)
```

\newpage

We can compute information-theoretic summaries (e.g., AICc, model probabilities) for these candidate models. To do this, we will use the `model.sel()` function from the **MuMIn** package. 

```{r}
#Create table
model.sel(list(lm.1, lm.2, lm.3, lm.4, lm.5))
```

The model evidence is less clear about which model is supported. The model having the most empirical evidence (given the data and candidate set of models) is Model 3 (the model that includes the locus-of-control main effect along with the three covariates). This model has a probability (AICc weight) of 0.51. This level of model evidence is far from overwhelming. Models 4 (Main effects of self-esteem and locus-of-control), 5 (the interaction model), and 2 also have a fair amount of empirical support. 

Anytime we  adopt a model from a set of candidate models (whether through information-theoretic approaches or through statistical inference), we have introduced some amount of uncertainty via the model selection process; referred to as **model selection uncertainty**. 

Think about having multiple replicate sets of data and fitting the same candidate models to each set of replication data. We would likely find that the "best" model varies across replicate sets of data. This is model selection uncertainty. 

The amount of the model selection uncertainty depends on the level of empirical evidence that supports one model over all others. In our example here, we would introduce quite a lot of model selection uncertainty since the empirical evidence for Model 3 is not overwhelming; the evidence also supports Models 4, 5, and 2.

This is important for a couple reasons. First, adopting one model when there is uncertainty that it is the "best" model may be misleading for the broader scientific enterprise. For example, consider trying to understand the effect of self-esteem on achievement. If we adopt Model 3 there is no effect of self-esteem. However, if Model 4 is actually closest to reality there is a positive effect of self-esteem on achievement. And, if Model 5 is closest to reality, then the effect of self-esteem on achievement depends on locus-of-control. Which are we to believe?

Another reason to think about model selection uncertainty is that any inferential results we report for a model are based around sampling variation being the only source of uncertainty. When there is model selection uncertainty, this is not the case. Increasing uncertainty directly affects the size of the standard errors which in turn affect the magnitude of the *p*-values and confidence intervals.

# Multimodel Inference

One solution to dealing with the problem of model selection uncertainty is to use *multimodel inference*. In this approach, information from all (or many) the models included in the candidate set is used to make statistical inferences. This approach to inference is rather new to the statistical sciences and the most commonly used method in multimodel inference is **model averaging**. 

## Model Averaging

In model averaging, we compute the coefficients and standard errors for the various predictors by incorporating informatin from these estimates from all the models in the candidate set. Rather than simple averaging, the estimates are weighted by the model probabilities. A simple averaging would give us a coefficient of

$$
\hat{\beta}_{\mathrm{Self\mbox{-}Esteem}} = \frac{0 + 0.3487 + 0.3767 + 0.9476 + 0}{5} = 0.3346
$$

However, this assumes that we believe each of the models equally. This is not true. The model probabilities we computed in Table 1 suggest that we should give the most weight to the coefficients in Model 3, then Model 4, etc. We can incorporate this by computing a weighted average where the weights we use are the model probabilities from the table of model evidence.

$$
\hat{\beta}_{\mathrm{Self\mbox{-}Esteem}} = \frac{0(0.505) + 0.3487(0.223) + 0.3767(0.156) + 0.9476(0.087) + 0(0.029)}{1} = 0.218
$$


Note that the denominator in the weighted average is simply the sum of the weights, which is 1. In general, we can express the estimate for the $k$th coefficient, $\hat\beta_k$, as 

$$
\hat\beta_k = w_j({\beta_k}_j)
$$

where $w_j$ is the AICc weight (or model probability) for candidate model $j$, and ${\beta_k}_j$ is the estimated coefficient for predictor $k$ in candidate model $j$.

Conceptually, the weighting across each of the models *shrinks* the value of the coefficent toward 0. This happens because (1) the predictor is not included in some models and, (2) because the weights are less than 1. This shrinkage is how model averaging accounts for model selection uncertainty.

If we carry out the model averaging for each of the coefficients, we find:

$$
\begin{split}
\hat{\beta}_0 &= 34.42(0.505) + 34.46(0.223) + 34.68(0.156) + 34.15(0.087) + 33.78(0.029) = 34.424 \\
\hat{\beta}_{\mathrm{SES}} &= 3.241(0.505) + 3.237(0.223) + 3.223(0.156) + 3.321(0.087) + 3.393(0.029) = 3.248 \\
\hat{\beta}_{\mathrm{GPA}} &= 5.064(0.505) + 5.050(0.223) + 5.028(0.156) + 5.169(0.087) + 5.304(0.029) = 5.071 \\
\hat{\beta}_{\mathrm{Parent~Education}} &= 0.4158(0.505) + 0.4143(0.223) + 0.4171(0.156) + 0.4114(0.087) + 0.4153(0.029) = 0.415 \\
\hat{\beta}_{\mathrm{Locus\mbox{-}of\mbox{-}Control}} &= 1.369(0.505) + 1.165(0.223) + 1.263(0.156) + 0(0.087) + 0(0.029) = 1.147 \\
\hat{\beta}_{\mathrm{Self\mbox{-}Esteem}} &= 0(0.505) + 0.3487(0.223) + 0.3767(0.156) + 0.9476(0.087) + 0(0.029) = 0.219 \\
\hat{\beta}_{\mathrm{Locus\mbox{-}of\mbox{-}Control~x~Self\mbox{-}Esteem}} &= 0(0.505) + 0(0.223) - 0.7216(0.156) + 0(0.087) + 0(0.029) = -0.112
\end{split}
$$

\newpage

We can also obtain these values from the `model.avg()` function from the **MuMIn** package. This utputs two sets of model averaged coefficients. The output in the `full` row, corresponds to the values we just computed. (The weighted average is computed based on the full set of candidate models.)


```{r}
# Model averaged estimates
model.avg(list(lm.1, lm.2, lm.3, lm.4, lm.5))
```

An alternative method of model averaging appears in the `subset` row. This *conditional averaging* only averages over the models where the parameter appears; it computes a weighted average over a subset of the candidate models. For example, the self-esteem predictor only appears in Models 4, 5, and 2, so the conditional weighted average would be computed as

$$
\hat{\beta}_{\mathrm{Self\mbox{-}Esteem}} = \frac{0.3487(0.223) + 0.3767(0.156) + 0.9476(0.087)}{0.223 + 0.156 + 0.087} = 0.4699
$$

Using the full set of candidate models, the model averaging produces the following fitted model:


$$
\begin{split}
\hat{\mathrm{Achievement}_i} = &34.42 + 3.25(\mathrm{SES}_i) + 5.07(\mathrm{Prior~GPA}_i) + 0.42(\mathrm{Parent~Education}_i) + \\ &0.22(\mathrm{Self\mbox{-}Esteem}_i) + 1.15(\mathrm{Locus\mbox{-}of\mbox{-}Control}_i) - 0.11(\mathrm{Self\mbox{-}Esteem}_i)(\mathrm{Locus\mbox{-}of\mbox{-}Control}_i)
\end{split}
$$

# Estimates of the Unconditional Variance

Ideally, we would also like to obtain variance estimates for each of our coefficients. These typically give us an indication of the amount of sampling uncertainty in the coefficient. The variance estimates (or standard errors) we get from statistical output (e.g., `tidy()`) are *conditional on both the sample size and the model selected*. In other words, the numerical values for these estimates are computed using ML or OLS after the model and sample size are specified.

In a perfect world, we would use one set of data for the model selection process, and then use a second set of data (of identical sample size) to then obtain the coefficient and variance estimates by fitting the "best" model we previously identified. In practice, we tend to perform both model selection and estimation on the same data set. This, means that our variance estimates (which only assume sampling uncertainty), are too small, as they should be augmented to include model selection uncertainty in addition to the sampling uncertainty. Conceptually,

$$
\mathrm{Var}(\hat\beta_k) = \mathrm{Sampling~variance~given~a~model} + \mathrm{Variance~due~to~model~selection~uncertainty}
$$

Specifically, we can compute this updated variance estimate using,

$$
\mathrm{Var}(\hat\beta_k) = \sum w_i\bigg(\mathrm{Var}(\hat{\beta_k}_i \vert \mathrm{Model}~i) + \bigg[\hat{\beta_k}_i - \hat{\bar{\beta_k}}\bigg]^2\bigg)
$$

where 

- $w_i$ is the model probability (AICc weight) for Model $i$, 
- $\mathrm{Var}(\hat{\beta_k}_i \vert \mathrm{Model}~i)$ is the sampling variance for $\hat{\beta_k}$ in Model $i$, 
- $\hat{\beta_k}$ is the estimated coefficient in Model $i$, and 
- $\hat{\bar{\beta_k}}$ is the model averaged value for $\hat{\beta_k}$ across all the candidate models.

The variance estimate is a weighted sum of two terms. The first term captures the sampling uncertainty for a particualar term ($\hat\beta_k$) given that it is in Model $i$. The second term captures the model selection uncertainty by measuring how much that coefficient changes from one model to another. Note that when the coefficient doesn't change much from one model to another, this second term is small and the overall variance for $\hat\beta_k$ is essentially the conditional sampling variance.

If, however, the coefficient changes a great deal from model-to-model, omitting this last term (as we usually do when we only report standard errors from `tidy()`), underestimates the amount of uncertainty for the coefficient. This error propogates through the inference by underestimating the size of the $p$-values  and confidence intervals (both should be larger).

## Computing the Unconditional Variance for the Self-Esteem Coefficient

We can obtain the estimated coefficients and the sampling variances from the `tidy()` output for each model. Note that this output gives the conditional standard errors for the coefficients which can be squared to obtain the sampling variances. 

```{r echo=FALSE}
data.frame(
  model = c(1, 2, 3, 4, 5),
  w = c(0.029, 0.087, 0.505, 0.223, 0.156),
  beta = c(0, 0.9476, 0, 0.3487, 0.3767),
  var_beta = c(0, 0.4634, 0, 0.5531, 0.5535)^2
) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    escape = FALSE,
    col.names = c("Model", "$w_i$", "beta", "var") #"$\hat{\beta}_k$", "$\mathrm{Var}(\hat{\beta_k}_i \vert \mathrm{Model}~i)$")
  )
```


We can get the AICc weights from the 


```{r echo=FALSE}
data.frame(
  model = c(1, 2, 3, 4, 5),
  w = c(0.029, 0.087, 0.505, 0.223, 0.156),
  # beta = c(33.7778, 34.1511, 34.4159, 34.4582, 34.6763),
  # var_beta = c(1.5480, 1.5555, 1.5581, 1.5602, 1.5714)^2
  beta = c(0, 0.9476, 0, 0.3487, 0.3767),
  var_beta = c(0, 0.4634, 0, 0.5531, 0.5535)^2
) %>%
  mutate(
    mod_sel_unc = (beta - sum(w*beta))^2,
    w_var = w * (var_beta + mod_sel_unc)
  ) %>%
  summarize(
    se = sqrt(sum(w_var))
  )
```




```{r}
# Coefficient table using full set of models
coefTable(
  model.avg(list(lm.1, lm.2, lm.3, lm.4, lm.5)), 
  full = TRUE
  )
```



# References

\noindent
\leftskip 0.2in
\parindent -0.2in
