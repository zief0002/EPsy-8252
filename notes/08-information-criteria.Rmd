---
title: "Information Criteria"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
    highlight: zenburn
    fig_width: 6
    fig_height: 6
header-includes: 
  - \usepackage{graphicx}
  - \usepackage{rotating}
  - \usepackage{xcolor}
  - \definecolor{umn}{HTML}{FF2D21}
  - \usepackage{caption}
  - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \usepackage{floatrow}
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
  - \usepackage{xfrac}
mainfont: "Sabon"
sansfont: "Futura"
monofont: Inconsolata    
urlcolor: "umn"
bibliography: [epsy8251.bib, epsy8252.bib]
csl: apa-single-spaced.csl
---

<!-- Set the document spacing -->
\frenchspacing


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE, fig.width=6, fig.height=6,
               fig.pos='H', fig.align='left')
opts_knit$set(width=85)
options(scipen=5)
```

# Preparation

In this set of notes, you will build a set of candidate models that we wwill ultimately use (in the next set of notes) in model selection. To do so, we will use the *usnews.csv* dataset (see the [data codebook](http://zief0002.github.io/epsy-8252/codebooks/usnews.html)) to examine the factors that underlie the ratings our academic peers give to graduate programs of education.


```{r preparation, warning=FALSE, message=FALSE}
# Load libraries
library(AICcmodavg)
library(broom)
library(educate) #Need version 0.1.0.1
library(patchwork)
library(tidyverse)

# Import data
usnews = read_csv("~/Documents/github/epsy-8252/data/usnews.csv")

# View data
head(usnews)
```

# Working Hypotheses and Candidate Models

Recall previously that we have three **scientific working hypotheses** about how academics perceive and, ultimatley rate, graduate programs:

- **H1:** Student-related factors drive the perceived academic quality of graduate programs in education.
- **H2:** Faculty-related factors drive the perceived academic quality of graduate programs in education.
- **H3:** Institution-related factors drive the perceived academic quality of graduate programs in education.

These working hypotheses were translated into the following three statistical models:

**Model 1**

$$
\mathrm{Peer~Rating}_i = \beta_0 + \beta_1(\mathrm{GREQ}_i) + \beta_2(\mathrm{GREQ}^2_i) + \beta_3(\mathrm{GREQ}^3_i) + \epsilon_i
$$

**Model 2** 

$$
\mathrm{Peer~Rating}_i = \beta_0 + \beta_1(\mathrm{Funded~research}_i) + \beta_2(\mathrm{Funded~research}^2_i) + \beta_3(\mathrm{PhDs~granted}_i) + \beta_4(\mathrm{PhDs~granted}^2_i) + \epsilon_i
$$

**Model 3** 

$$
\mathrm{Peer~Rating}_i = \beta_0 + \beta_1(\mathrm{PhD~acceptance~rate}_i) + \beta_2(\mathrm{PhD~student\mbox{-}to\mbox{-}faculty~ratio}_i) + \beta_3(\mathrm{Enrollment}_i) + \epsilon_i
$$





In these models, peer rating, funded research, Ph.D.s granted, Ph.D. acceptance rate, enrollment, and Ph.D. student-to-faculty ratio were all log-transformed.

```{r}
# Drop rows with missing data
# Create log-transformed variables
educ = usnews %>%
  drop_na() %>%
  mutate(
    Lpeer = log(peer),
    Lfunded_research_per_faculty = log(funded_research_per_faculty),
    Lphd_granted_per_faculty = log(phd_granted_per_faculty + 1),
    Ldoc_accept = log(doc_accept),
    Lphd_student_faculty_ratio = log(phd_student_faculty_ratio + 1),
    Lenroll = log(enroll)
    )

# Fit Model 1
lm.1 = lm(Lpeer ~ 1 + gre_quant + I(gre_quant^2) + I(gre_quant^3), data = educ)

# Fit Model 2
lm.2 = lm(Lpeer ~ 1 + Lfunded_research_per_faculty + I(Lfunded_research_per_faculty^2) + Lphd_granted_per_faculty  + I(Lphd_granted_per_faculty^2), data = educ)

# Fit Model 3
lm.3 = lm(Lpeer ~ 1 + Ldoc_accept + Lenroll + Lphd_student_faculty_ratio, data = educ)
```


# Log-Likelihood

Recall that the likelihood gives us the probability of a particular model given a set of data and assumptions about the model, and that the log-likelihood is just a mathematically convenient transformation of the likelihood. Log-likelihood values from different models can be compared, so long as:

- The exact same data is used to fit the models, 
- The exact same outcome is used to fit the models, and
- The assumptions underlying the likelihood (independence, distributional assumptions) are met.


In all four models we are using the same data set and outcome, and the assumptions seem reasonably tenable for each of the four fitted candidate models. This suggests that the likelihood (or log-likelihood) can provide some evidence as to which of the four candidate models is most probable. Below we compute the log-likelihood values for each of the four candidate models.

```{r}
logLik(lm.1)
logLik(lm.2)
logLik(lm.3)
```

Note that the log-likelihood values are also available from the `glance()` function's output (in the `logLik` column).

```{r}
glance(lm.1)
```

These values suggest that the model with the highest probability given the data and set of assumptions is Model 3; it has the highest log-likelihood value.


# Deviance: An Alternative Fit Value

It is common to multiply the log-likelihood values by $-2$. This is called the *deviance*. Deviance is a measure of model-data error, so when evaluating deviance values, lower is better. (The square brackets in the syntax grab the log-likelihood value from the `logLik()` output.) 

```{r}
-2 * logLik(lm.1)[1] #Model 1
-2 * logLik(lm.2)[1] #Model 2
-2 * logLik(lm.3)[1] #Model 3
```


Here, the model that produces the lowest amount of model-data error is Model 3; it has the lowest deviance value. Since the deviance just multiplies the log-likelihood values by a constant, it produces the same rank ordering of the candidate models. Thus, whether you evaluate using the likelihood, the log-likelihood, or the deviance, you will end up with the same ordering of candidate models. Using deviance, however, has the advantages of having a direct relationship to model error, so it is more interpretable. It is also more closely aligned with other model measures associated with error that we commonly use (e.g., SSE, $R^2$).


# Akiake's Information Criteria (AIC)

Remember that lower values of deviance indicate the model (as defined via the set of parameters) is more likely (lower model-data error) given the data and set of assumptions. However, in practice we cannot directly compare the deviances since the models include a different number of parameters. It was not coincidence that our most probable candidate model also had the highest number of predictors.

To account for this, we will add a penalty term to the deviance based on the number of parameters estimated in the model. This penalty-adjusted value is called Akiake's Information Criteria (AIC). 

$$
AIC = \mathrm{Deviance} + 2(k)
$$

where $k$ is the number of parameters being estimated in the model (including the intercept and RMSE). The AIC adjusts the deviance based on the complexity of the model. Note that the value for $k$ is given as *df* in the `logLik()` output. For our four models, the *df* values are:

- **M1:** 5 *df* ($\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$, $\hat\beta_3$, RMSE)
- **M2:** 6 *df* ($\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$, $\hat\beta_3$, $\hat\beta_4$, RMSE)
- **M3:** 5 *df* ($\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$, $\hat\beta_3$, RMSE)

Just as with the deviance, smaller AIC values indicate a more likely model.

```{r}
-2 * logLik(lm.1)[1] + 2*5 #Model 1
-2 * logLik(lm.2)[1] + 2*6 #Model 2
-2 * logLik(lm.3)[1] + 2*5 #Model 3
```


Arranging these, we find that again Model # (AIC = $-192.8$) is the most likely candidate model given the data and candidate set of models. We can also compute the AIC via the `AIC()` function.

```{r}
# Compute AIC value for Model 1
AIC(lm.1)
```

Lastly, we note that the AIC value is produced as a column in the model-level output. (Note that the `df` column from `glance()` does NOT give the number of model parameters.)

```{r}
# Model-level output for Model 1
glance(lm.1)
```


# Empirical Support for Hypotheses

Because the models are proxies for the scientific working hypotheses, the AIC ends up being a measure of empirical support for any particular hypothesis---after all, it takes into account the data (empirical evidence) and model complexity. In practice, we can use the AIC to rank order the models, which results in a rank ordering of the scientific working hypotheses based on the empirical support for each. Ranked in order of empirical support, the three scientific working hypotheses are:

- Peer ratings are attributable to institution-related factors. This hypothesis has the most empirical support of the three working hypotheses, given the data and other candidate models.
- Peer ratings are attributable to faculty-related factors.
- Peer ratings are attributable to student-related factors. This hypothesis has the least amount of empirical support of the three working hypotheses, given the data and other candidate models.

It is important to remember that the phrase "given the data and other candidate models" is highly important. Using AIC to rank order the models results in a *relative ranking of the models*. It is not able to rank any hypotheses that you didn't consider as part of the candidate set of scientific working hypotheses. Moreover, the AIC is a direct function of the likelihood which is based on the actual model fitted as a proxy for the scientific working hypothesis. If the predictors used in any of the models had been different, it would lead to different likelihood and AIC values, and potentially a different rank ordering of the hypotheses.



# Corrected AIC (AICc): Adjusting for Model Complexity and Sample Size

Based on the AIC values for the three candidate models we ranked the hypotheses based on the amount of empirical support:

```{r echo=FALSE}
data.frame(
  Hypothesis = c(
    "Institution-related factors", 
    "Faculty-related factors", 
    "Student-related factors"
    ),
  AIC = c(AIC(lm.3), AIC(lm.2), AIC(lm.1))
) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    escape = FALSE,
    caption = "Working Hypotheses Rank Ordered by the Amount of Empirical Support as Measured by the AIC", 
    digits = 1
    ) %>%
  kable_styling(latex_options = "HOLD_position")
```

Although AIC has a penalty correction that should account for model complexity, it turns out that when the number of parameters is large relative to the sample size, AIC is still biased in favor of models that have more parameters. This led @Hurvich:1989 to propose a second-order bias corrected AIC measure (AICc) computed as

$$
\mathrm{AIC_c} = \mathrm{Deviance} + 2(k)\left( \frac{n}{n - k - 1} \right)
$$

where $k$ is, again, the number of estimated parameters, and $n$ is the sample size used to fit the model. Note that when $n$ is very large (especially relative to $k$) that the last term is essentially 1 and the AICc value would basically reduce to the AIC value. When $n$ is small relative to $k$ this will add more of a penalty to the deviance. **The recommendation is to pretty much always use AICc rather than AIC when selecting models.**

Below, we will compute the AICc for the first candidate model. (Note that we use $n=122$ cases for the computation for all the models in this data.)

```{r}
n = 122
k = 5

# Compute AICc for Model 1
-2 * logLik(lm.1)[[1]] + 2 * k * n / (n - k - 1) #Model 1
```

In practice, we will use the `AICc()` function from the **AICcmodavg** package to compute the AICc value directly.

```{r message=FALSE}
AICc(lm.1)
AICc(lm.2)
AICc(lm.3)
```


Based on the $\mathrm{AIC_c}$ values, the model with the most empirical support given the data and four candidate models is Model 3. Again, because the models are proxies for the scientific hypotheses, we can rank order the scientific hypotheses based on the empirical support for each.




```{r echo=FALSE}
data.frame(
  H = c(  "Institution-related factors", 
          "Faculty-related factors", 
          "Student-related factors"
          ),
  AICc = c(AICc(lm.3), AICc(lm.2), AICc(lm.1))
  #a = c(-193.3285, -192.3311, -183.7639, -181.2357)
) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    escape = FALSE,
    caption = "Working Hypotheses Rank Ordered by the Amount of Empirical Support", 
    digits = 1,
    col.names = c("Hypothesis", "AICc")
    ) %>%
  kable_styling(latex_options = "HOLD_position")
```

# Model-Selection Uncertainty

When we adopt one model over another, we are introducing some degree of selection uncertainty into the scientific process. It would be nice if we can quantify and report this uncertainty, and this is the real advantage of using information criteria for model selection; it allows us to quantify the uncertainty we have when we select any particular candidate model.

The amount of model selection uncertainty we have depends on the amount of empirical support each of the candidate models has. For example, if one particular candidate model has a lot of empirical support and the rest have very little empirical support we would have less model uncertainty than if all of the candidate models had about the same amount of empirical support.

Since we measure the empirical support each hypothesis has by computing the AICc for the associated candidate model, we can look at how much more empirical support the most supported hypothesis has relative to each of the other working hypotheses by computing the difference in AICc values between the best fitting model and each of the other candidate models. This measure is referred to as # $\Delta$AICc.

In our example, the hypothesis with the most empirical support was the institution-related factors model as measured in Model 3.

```{r}
# Compute delta values
AICc(lm.1) - AICc(lm.3) #Student-related factors
AICc(lm.2) - AICc(lm.3) #Faculty-related factors
AICc(lm.3) - AICc(lm.3) #Institution-related factors
```

```{r echo=FALSE}
data.frame(
  H = c(  "Institution-related factors", 
          "Faculty-related factors", 
          "Student-related factors"
          ),
  AICc = c(AICc(lm.3), AICc(lm.2), AICc(lm.1))
  ) %>%
  mutate(
    delta_a = AICc - AICc(lm.3)
  ) %>% 
  kable(
    format = "latex",
    booktabs = TRUE,
    escape = FALSE, 
    caption = "Working Hypotheses Rank Ordered by the Amount of Empirical Support", 
    digits = 1,
    col.names = c("Hypothesis", "AICc", "$\\Delta$AICc")
  ) %>%
  kable_styling(latex_options = "HOLD_position")
```

@Burnham:2011 [p. 25] give rough guidelines for interpreting $\Delta$AICc values. They suggest that hypotheses with $\Delta$AICc values less than 2 are plausible, those in the range of 4--7 have some empirical support, those in the range of 9--11 have relatively little support, and those greater than 13 have essentially no empirical support. Using these criteria:

- The institution-related factors hypothesis (Model 3) has the most empirical support.
- The faculty-related factor hypothesis (Model 2) is plausible, although it has relatively little empirical support compared to the institution-related factors hypothesis.
- The student-related factor hypothesis (Model 1) has essentially no empirical support relative to the institution-related factor hypothesis.


# Relative Likelihood and Evidence Ratios

Onw way we mathematically formalize the strength of evidence for each model is to compute the relative likelihood. The relative likelihood provides the likelihood of each of the candidate models, given the set of candidate models and the data. To compute the relative likelihood,

$$
\mathrm{Relative~Likelihood} = e ^ {âˆ’\frac{1}{2} (\Delta AICc)}
$$




```{r}
# Institution-related factors
exp(-1/2 *  0.00)

# Faculty-related factors
exp(-1/2 *  8.6)

# Student-related factors
exp(-1/2 * 11.1)
```


```{r echo=FALSE}
data.frame(
  H = c(  "Institution-related factors", 
          "Faculty-related factors", 
          "Student-related factors"
          ),
  AICc = c(AICc(lm.3), AICc(lm.2), AICc(lm.1))
  ) %>%
  mutate(
    delta_a = AICc - AICc(lm.3),
    rel_lik = exp(-1/2 * delta_a)
    ) %>% 
  kable(
    format = "latex",
    booktabs = TRUE,
    escape = FALSE, 
    caption = "Working Hypotheses Rank Ordered by the Amount of Empirical Support", 
    digits = c(NA, 1, 1, 3),
    col.names = c("Hypothesis", "AICc", "$\\Delta$AICc", "Rel. Lik.")
    ) %>%
  footnote(
    general = "Rel. Lik. = Relative Likelihood",
    general_title = "Note.",
    footnote_as_chunk = TRUE
    ) %>%
  kable_styling(latex_options = "HOLD_position")

```

These quantities allow us to compute *evidence ratios*, which are evidentiary statements for comparing any two scientific hypotheses. Evidence ratios quantify how much more empirical support one hypothesis has versus another. To obtain an evidence ratio, we divide the relative likelihood for any two hypotheses. As another example,

- The empirical support for the institution-related factors hypothesis is 71.4 times that of the empirical support for the faculty-related factors hypothesis. (To obtain this we computed $1/.014=71.4$.)
- The empirical support for the institution-related factors hypothesis is 250 times that of the empirical support for the student-related factors hypothesis. (To obtain this we computed $1/.004=250$.)


# Model Probabilities

Also referred to as an Akaike Weight ($w_i$), a model probability provides a numerical measure of the probability of each model given the data and the candidate set of models. It can be computed as:

$$
w_i = \frac{\mathrm{Relative~Likelihood~for~Model~J}}{\sum_j \mathrm{Relative~Likelihood}}
$$



```{r}
# Compute sum of relative likelihoods
sum_rel = 1.000000000 + 0.01356856 + 0.003887457

# Institution-related factors
1.000000000 / sum_rel 

# Faculty-related factors
0.01356856 / sum_rel 

# Student-related factors
0.003887457 / sum_rel 
```

Since the models are proxies for the working hypotheses, the model probabilities can be used to provide probabilities of each working hypothesis as a function of the empirical support. Given the data and the candidate set of working hypotheses:

- The probability of the institution-related factors hypothesis is 0.983.
- The probability of the faculty-related factors hypothesis is 0.013.
- The probability of the student-related factors hypothesis is 0.004.

This suggests that it is highly probably that the institution-related factors hypothesis is the "best" (closest to truth). There is a tiny probability that the faculty-related factors hypothesis is "best"and almost no probability the student-related factors hypothesis is "best".



```{r echo=FALSE}
data.frame(
  H = c(  "Institution-related factors", 
          "Faculty-related factors", 
          "Student-related factors"
          ),
  AICc = c(AICc(lm.3), AICc(lm.2), AICc(lm.1))
  ) %>%
  mutate(
    delta_a = AICc - AICc(lm.3),
    rel_lik = exp(-1/2 * delta_a),
    model_prob = rel_lik / sum(rel_lik)
    ) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    escape = FALSE, 
    caption = "Working Hypotheses Rank Ordered by the Amount of Empirical Support", 
    digits = c(1, 1, 1, 3, 3),
    col.names = c("Hypothesis", "AICc", "$\\Delta$AICc", "Rel. Lik.", "AICc Weight")
    ) %>%
  footnote(
    general = "Rel. Lik. = Relative Likelihood",
    general_title = "Note.",
    footnote_as_chunk = TRUE
    ) %>%
  kable_styling(latex_options = "HOLD_position")
```


# Tables of Model Evidence

We will use the `aictab()` function from the **AICcmodavg** package to compute and create a table of model evidence values directly from the `lm()` fitted models. This function takes a list of models in the candidate set (it actually has to be an R list). The optional argument `modnames=` is a vector of model names associated with the models in the candidate set.

```{r}
#Create table
myAIC = aictab(
  cand.set = list(lm.1, lm.2, lm.3),
  modnames = c(
    "Student-related factors", 
    "Faculty-related factors", 
    "Institution-related factors"
    )
  )

# View output
myAIC
```

Note the output includes the number of parameters (`K`) and AICc value (`AICc`) for each candidate model, and prints them in order from the most empirical evidence to the least amount of empirical evidence based on the AICc. It also includes the $\Delta$AICc values, the model probabilities (`AICcWt`), and log-likelihood (`LL`) values. The `Cum.Wt` column gives the cumulative model probabilities. (For example the probability of the first two hypotheses is $0.99 + 0.01 = 1.00$.)

You can also directly compute the evidence ratios, but you have to do that separately. We do this using the `evidence()` function from the **AICcmodavg** package. This function takes the output from the `aictab()` function as well as the names from that table (given in the `modnames=` argument) for the two models you want to compute the evidence ratio for.

```{r}
# Evidence Ratio 1
evidence(
  myAIC,
  model.high = "Institution-related factors",
  model.low = "Faculty-related factors"
  )

# Evidence Ratio 2
evidence(
  myAIC,
  model.high = "Institution-related factors ",
  model.low = "Student-related factors"
  )
```

## Pretty Printing Tables of Model Evidence

We can use the `data.frame()` function to coerce the output from the `aictab()` function into a data frame. Then we can use **dplyr** functions to select the columns in the order we want them, add the column of evidence ratios, and re-name any column we want. Lastly, we can use the `kable()` function from the **knitr** package and other functions from the **kableExtra** package (namely `footnote()` and `kable_styling()`) to format the table for pretty-printing in RMarkdown.

```{r}
# Create data frame to format into table
tab_01 = data.frame(myAIC) %>%
  select(
    Modnames, LL, K, AICc, Delta_AICc, AICcWt
  ) %>%
  mutate(
    ER = max(AICcWt) / AICcWt
  ) %>%
  rename(
    Hypothesis = Modnames,
    # We can include LaTeX math notation in column names
    # Because \ is a special character we need two \\
    '$LL$' = LL,
    '$K$' = K,
    '$\\Delta$AICc' = Delta_AICc,
    'AIC Wt.' = AICcWt
  )

# Load libraries for formatting
library(knitr)
library(kableExtra)

# Format the table output
kable(tab_01,
      format = "latex",
      booktabs = TRUE,
      escape = FALSE,
      caption = "Table of Model Evidence for Three Working Hypotheses",
      digits = 2,
      align = c("l", "c", "c", "c", "c", "c", "c")
      ) %>%
  footnote(
    general = "LL = Log-Likelihood; K = Model df; AIC Wt. = Model Probability; ER = Evidence Ratio",
    general_title = "Note.",
    footnote_as_chunk = TRUE
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  row_spec(row = 0, align = "c")
```


# Some Final Thoughts

Based on the model evidence given the data for this candidate set of models:

- The institution-related factors hypothesis has the most empirical support.
- There is very little empirical support for either the faculty-related factor and student-related factors hypotheses relative to the institution-related factors hypothesis.

We can get a summary of the model rankings along with qualitative descriptors of the empirical support (weight) using the `confset()` function. The `method="ordinal"` argument rank orders the models for us.

```{r}
confset(
  cand.set = list(lm.1, lm.2, lm.3),
  modnames = c(
    "Student-related factors", 
    "Faculty-related factors", 
    "Institution-related factors"
    ),
  method = "ordinal"
  )
```

It is important to note that it is ultimately the set of scientific working hypotheses that we are evaluating, using the fit from the associated statistical models to a set of empirical data. If we had a different set of data, we may have a whole new ranking of models or interpretation of empirical support. The empirical support is linked to the data.

The amount of empirical evidence is also very much relative to the candidate set of models; a different candidate set of models may result in a different rank ordering or interpretation of empirical support. For example, consider if we had not done any exploration of the model's functional form, but instead had just included the linear main-effects for each model.

```{r}
# Fit models
lm.1_1 = lm(peer ~ 1 + gre_quant + gre_verbal, data = educ)
lm.2_1 = lm(peer ~ 1 + funded_research_per_faculty + phd_granted_per_faculty, data = educ)
lm.3_1 = lm(peer ~ 1 + doc_accept + enroll + phd_student_faculty_ratio, data = educ)

# Compute model evidence
aictab(
  cand.set = list(lm.1_1, lm.2_1, lm.3_1),
  modnames = c(
    "Student-related factors", 
    "Faculty-related factors", 
    "Institution-related factors"
    )
  )
```

In this example, the rank-ordering of hypotheses ended up being the same, but the evaluation of the empirical support is much different.

- There is about the same amount of empirical support for the institution-related factors hypothesis and the faculty-related factors hypothesis.
- There is still virtually no support for the student-related factors hypothesis.


It is important to note that although information criteria can tell you about the empirical support among a candidate set of models, it cannot say whether that is actually a "good" model. For that you need to look at the assumptions and other measures (e.g., $R^2$). You still need to do all of the work associated with model-building (e.g., selecting predictors from the substantive literature, exploring functional forms to meet the assumptions).


# Statistical Inference and Information Criteria

Finally, it is important to mention that philosophically, information-criteria and statistical inference  are two very different ways of measuring statistical evidence. When we use statistical inference for variable selection, the evidence, the $p$-values, is a measure of how rare an observed statistic (e.g., $\hat\beta_k$, $t$-value) is under the null hypothesis. The AIC, on the other hand, is a measure of the model-data compatibility accounting for the complexity of the model.

In general, the use of $p$-values is **not compatible** with the use of information criteria-based model selection methods; see @Anderson:2008 for more detail. Because of this, it is typical to not even report $p$-values when using information criteria for model selection. When using information criteria, however, the standard errors are reported, especially for any "best" model(s). This gives information about the statistical uncertainty that arises because of sampling error.

It is important that you decide how you will be evaluating evidence and making decisions about variable and model selection prior to actually examining the data. Mixing and matching is not cool!


# References

\noindent
\leftskip 0.2in
\parindent -0.2in
