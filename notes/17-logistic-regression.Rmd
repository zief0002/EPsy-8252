---
title: "Logistic Regression"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
    highlight: zenburn
    fig_width: 6
    fig_height: 6
header-includes: 
  - \usepackage{graphicx}
  - \usepackage{rotating}
  - \usepackage{xcolor}
  - \definecolor{umn}{HTML}{FF2D21}
  - \usepackage{caption}
  - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
  - \usepackage{floatrow}
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
  - \usepackage{xfrac}
mainfont: "Sabon"
sansfont: "Futura"
monofont: Inconsolata    
urlcolor: "umn"
bibliography: [epsy8251.bib, epsy8252.bib]
csl: apa-single-spaced.csl
---

<!-- Set the document spacing -->
\frenchspacing


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE, fig.width=6, fig.height=6,
               fig.pos='H', fig.align='left')
opts_knit$set(width=85)
options(scipen=5)
```


# Preparation

In this set of notes, you will learn how to use logistic regression models to model dichotomous categorical outcome variables (e.g., dummy coded outcome). We will use data from the file *graduation.csv* (see the [data codebook](http://zief0002.github.io/epsy-8252/codebooks/graduation.html)) to explore predictors of college graduation. 


```{r message=FALSE}
# Load libraries
library(broom)
library(corrr)
library(MuMIn)
library(tidyverse)

# Read in data
grad = read_csv(file = "~/Documents/github/epsy-8252/data/graduation.csv")

# View data
head(grad)
```

In the last set of notes, we saw that using the linear probability model leads to direct violations of the linear model's assumptions. If that isn't problematic enough, it is possible to run into severe issues when we make predictions. For example, given the constant effect of $X$ in these models it is possible to have an $X$ value that results in a predicted proportion that is either greater than 1 or less than 0. This is a problem since proportions are constrained to the range of $\left[0,~1\right]$.

\newpage

Before we consider any alternative models, let's actually examine the empirical proportions of students who graduate at different ACT scores.

```{r out.width='3in', message=FALSE, fig.cap='The loess smoother suggests that the proportion of students who graduate is a non-linear function of ACT scores.'}
graduates = grad %>% 
  group_by(act, degree) %>% 
  summarize( N = n() ) %>% 
  mutate( Prop = N / sum (N) ) %>%
  filter(degree == 1) %>%
  ungroup() #Makes the resulting tibble regular

# View data
head(graduates, 10)

# Plot proportions
ggplot(data = graduates, aes(x = act, y = Prop)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Proportion of graduates")
```



# Alternative Models to the Linear Probability Model

Many of the non-linear models that are typically used to model dichotomous outcome data are "S"-shaped models. Below is a plot of one-such "S"-shaped model.

```{r echo=FALSE, out.width='3in'}
x = seq(from = -4, to = 4, by = .01)
y = 1 / (1 + exp(-(0 + 4*x)))

plot(x, y, type = "l", xlab = "X", ylab = "P(X = 1)")
```

The non-linear "S"-shaped model has many attractive features. First, the predicted $Y$ values are bounded between 0 and 1. Furthermore, as $X$ gets smaller, the proportion of $Y=1$, approaches 0 at a slower rate. Similarly, as $X$ gets larger, the proportion of $Y=1$, approaches 1 at a slower rate. Lastly, this model curve is monotonic; smaller values of $X$ are associated with smaller proportions of $Y=1$ (or if the "S" were backwards, larger values of $X$ would be associated with smaller proportions of $Y=1$). The key is that there are no bends in the curve; it is always growing or always decreasing. 

In our graduation example, the empirical data maps well to this curve. Higher ACT scores are associated with a higher proportion of students who graduate (monotonic). The effect of ACT, however, is not constant, and seems to diminish at higher ACT scores. Lastly, we want to bound the proportion at every ACT score to lie between 0 and 1.

How do we fit such an "S"-shaped curve? We apply a transformation function, call it $\Lambda$, to the predicted values. Mathematically,

$$
\Lambda(\pi_i) = \Lambda\bigg[\beta_0 + \beta_1(X)\bigg]
$$

The specific transformation function used is any mathematical function that can fit the criteria we had before (monotonic, nonlinear, maps to $[0,~1]$ space). There are several mathematical functions that do this. One common function that meets these specifications is the *logistic function*. Mathematically, the logistic function is

$$
\Lambda(w) = \frac{1}{1 + e^{-w}}
$$

where $w$ is the value fed into the logistic function. For example, to logistically transform $w=3$, we use

$$
\begin{split}
\Lambda(3) &= \frac{1}{1 + e^{-3}} \\
&= 0.953
\end{split}
$$


Below we show how to transform many such values using R.

```{r fig.width=6, fig.height=6, out.width='3in', fig.cap='Plot of the logistically transformed values for a sequence of values from -4 to 4.', fig.pos='H'}
example = data.frame(
  w = seq(from = -4, to = 4, by = 0.01)  # Set up values
  ) %>%
  mutate(
    Lambda = 1 / (1 + exp(-w))  # Transform using logistic function
  )

# View data
head(example)

# Plot the results
ggplot(data = example, aes(x = w, y = Lambda)) +
  geom_line() +
  theme_bw()
```

You can see that by using this transformation we get a monotonic "S"-shaped curve. Now try substituting a really large value of $w$ into the function. This gives an asymptote at 1. Also substitute a really "large"" negative value in for $w$. This gives an asymptote at 0. So this function also bounds the output between 0 and 1.


How does this work in a regression? There, we are transforming the *predicted values*, the $\pi_i$ values, which we express as a function of the predictors:

$$
\begin{split}
\Lambda(\hat\pi_i) &= \Lambda\bigg[\beta_0 + \beta_1(X_i)\bigg] \\
& = \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}}
\end{split}
$$

Since we took a linear model ($\beta_0 + \beta_1(X_i)$) and applied a logistic transformation, the resulting model is the *linear logistic model* or more simply, the *logistic model*. 

## Re-Expressing a Logistic Transformation

The logistic model expresses the proportion of 1s ($\pi_i$) as a function of the predictor $X$. It can be mathematically expressed as

$$
\pi_i = \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}}
$$

\newpage

We can re-express this using algebra and rules of logarithms.

$$
\begin{split}
\pi_i &= \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}} \\
\pi_i \times (1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]} ) &= 1 \\
\pi_i + \pi_i(e^{-\big[\beta_0 + \beta_1(X_i)\big]}) &= 1 \\
\pi_i(e^{-\big[\beta_0 + \beta_1(X_i)\big]}) &= 1 - \pi_i \\
e^{-\big[\beta_0 + \beta_1(X_i)\big]} &= \frac{1 - \pi_i}{\pi_i} \\
e^{\big[\beta_0 + \beta_1(X_i)\big]} &= \frac{\pi_i}{1 - \pi_i} \\
\ln \bigg(e^{\big[\beta_0 + \beta_1(X_i)\big]}\bigg) &= \ln \bigg( \frac{\pi_i}{1 - \pi_i} \bigg) \\
\beta_0 + \beta_1(X_i) &= \ln \bigg( \frac{\pi_i}{1 - \pi_i}\bigg)
\end{split}
$$

Or,

$$
\ln \bigg( \frac{\pi_i}{1 - \pi_i}\bigg) = \beta_0 + \beta_1(X_i)
$$

The logistic model expresses the natural logarithm of $\frac{\pi_i}{1 - \pi_i}$ as a linear function of $X$. Note that there is no error term on this model. This is because the model is for the mean structure only (the proportions), we are not modeling the actual $Y_i$ values with the logistic regression model.

## Log-Odds or Logits

The ratio that we are taking the logarithm of, $\frac{\pi_i}{1 - \pi_i}$, is referred to as *odds*. Odds are the ratio of two probabilities. Namely the chance an event occurs ($\pi_i$) versus the chance that same event does not occur ($1 - \pi_i$). As such, it gives the *relative chance* of that event occurring. To understand this better, we will look at a couple examples.

Let's assume that the probability of getting an "A" in a course is 0.7. Then we know the probability of NOT getting an "A" in that course is 0.3. The odds of getting an "A" are then

$$
\mathrm{Odds} = \frac{0.7}{0.3} = 2.33
$$

That the probability of getting an "A" in the class is 2.33 times as likely as NOT getting an "A" in the class. This is the relative probability of getting an "A". 

As another example, Fivethirtyeight.com computed the [probability that a Canadian hockey team would win the Stanley Cup in 2018 as 0.17](https://fivethirtyeight.com/features/will-canada-end-its-stanley-cup-drought-well-its-not-impossible/). The odds of a Canadian team winning the Stanley Cup is then

$$
\mathrm{Odds} = \frac{0.17}{0.83} = 0.21
$$

The probability that a Canadian team wins the Stanley Cup is 0.21 times as likely as a Canadian team NOT winning the Stanley Cup. (Odds less than 1 indicate that is is more likely for an event NOT to occur than to occur. Invert the fraction to compute how much more like the event is not to occur.)

In the logistic model, we are predicting the log-odds (also referred to as the *logit*. When we get these values, we typically transform the logits to odds by inverting the log-transformation (take $e$ to that power.) 

## Binomially Distributed Errors

The logistic transformation fixed two problems: (1) the non-linearity in the conditional mean function, and (2) bounding any predicted values between 0 and 1. However, just fitting this transformation does not fix the problem of non-normality. Remember from the previous notes we learned that at each $X_i$ there were only two potential values for $Y_i$; 0 or 1. Rather than use a normal (or Gaussian) distribution to model the conditional distribution of $Y_i$, we will use the *binomial distribution*. The binomial distribution is a discrete probability distribution that gives the probability of obtaining exactly $n$ successes out of $N$ Bernoulli trials (where the result of each Bernoulli trial is true with probability $\pi$ and false with probability $1-\pi$). This is appropriate since at each value of $X$ we can posit $N$ Bernoulli trials, $n$ of which are 1 (successes). 


# Fitting the Binomial Logistic Model in R

The syntax to fit the logistic model using `glm()` is

$$
\mathtt{glm(} \mathrm{y} \sim \mathrm{1~+~x,~}\mathtt{data=}~\mathrm{dataframe,~}\mathtt{family~=~binomial(link~=~"logit")}
$$

The formula depicting the model and the `data=` arguments are specified in the same manner as in the `lm()` function. Since the model is a generalized model, we need to specify the distribution for the conditional $Y_i$ values (binomial) and the link function (logit) via the `family=` argument. 

For our example,

```{r}
glm.1 = glm(degree ~ 1 + act, data = grad, family = binomial(link = "logit"))
```

The coefficient-level output of the model can be printed using `tidy()`.

```{r}
tidy(glm.1)
```

The fitted equation for the model is

$$
\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg) = -1.61 + 0.11(\mathrm{ACT~Score}_i)
$$

We interpret the coefficients in the same manner as we interpret coefficients from a linear model, with the caveat that the outcome is now in log-odds (or logits):

- For students with an ACT score of 0, their predicted log-odd of graduating are $-1.61$.
- Each one-point difference in ACT score is associated with a difference of 0.11in the predicted log-odds of graduating, on average.

For better interpretations, we can back-transform log-odds to odds. This is typically a better metric for interpretation of the coefficients. To back-transform to odds, we exponentiate both sides of the fitted equation and use the rules of exponents to simplify:

$$
\begin{split}
e^{\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg)} &= e^{-1.61 + 0.11(\mathrm{ACT~Score}_i)} \\
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(\mathrm{ACT~Score}_i)}
\end{split}
$$

When ACT score = 0, the predicted odds are


$$
\begin{split}
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(0)} \\
&= e^{-1.61} \times 1 \\
&= e^{-1.61} \\
&= 0.2
\end{split}
$$

For students with an ACT score of 0, their odds of graduating is 0.2. That is, for these students, the probability of graduating is 0.2 times that of not graduating. (It is far more likey these students will not graduate.)

To interpret the effect of ACT on the odds of graduating, we will compare the odds of graduating for students that have ACT score that differ by one point. Say ACT = 0 and ACT = 1.

We already know the predicted odds for students with ACT = 0, namely $e^{-1.61}$. For students with an ACT of 1, their predicted odds of graduating are

$$
\begin{split}
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(1)} \\
&= e^{-1.61} \times e^{0.11} \\
\end{split}
$$

These students odds of graduating are $e^{0.11}$ times greater than students with an ACT score of 0. Moreover, this increase in the odds, on average, is the case for every one-point difference in ACT score. In general,

- The predicted odds for $X=0$ are $e^{\hat\beta_0}$.
- Each one-unit difference in $X$ is associated with a $e^{\hat\beta_1}$ times increase (decrease) in the odds.

\newpage

We can obtain these values in R by using the `coef()` function to obtain the fitted model's coefficients and then exponentiating them using the `exp()` function. 

```{r}
exp(coef(glm.1))
```

From these values, we interpret the coefficients in the odds metric as

- The predicted odds of graduating for students with an ACT score of 0 are 0.20.
- Each one-unit difference in ACT score is associated with 1.11 times greater odds of graduating.

## Plotting the Results from the Fitted Model

To even further understand and interpret the fitted model, we can plot the fitted values for a range of ACT scores. Typically the proportion of students graduating ($\hat\pi_i$) is what should be plotted, as that is what we were initially modeling. This process is exactly the same as the process of plotting fitted model results for any of the other models we have worked with. The one, challenge is that we need to write the equation as:

$$
\hat{\pi_i} = \mathrm{something}
$$

The general fitted equation for the logistic regression model is written as:

$$
\ln\bigg[\frac{\hat\pi_i}{1 - \hat\pi_i}\bigg] = \hat\beta_0 + \hat\beta_1(x_i)
$$

We need to isolate the $\hat\pi_i$ term on the left-hand side of the equation. To do this, we will use algebra. 

$$
\begin{split}
\ln\bigg[\frac{\hat\pi_i}{1 - \hat\pi_i}\bigg] &= \hat\beta_0 + \hat\beta_1(x_i) \\[1em]
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{\hat\beta_0 + \hat\beta_1(x_i)} \\[1em]
\hat\pi_i &= e^{\hat\beta_0 + \hat\beta_1(x_i)} (1 - \hat\pi_i) \\[1em]
\hat\pi_i &= e^{\hat\beta_0 + \hat\beta_1(x_i)} - e^{\hat\beta_0 + \hat\beta_1(x_i)}(\hat\pi_i) \\[1em]
\hat\pi_i + e^{\hat\beta_0 + \hat\beta_1(x_i)}(\hat\pi_i) &= e^{\hat\beta_0 + \hat\beta_1(x_i)} \\[1em]
\hat\pi_i(1 + e^{\hat\beta_0 + \hat\beta_1(x_i)}) &= e^{\hat\beta_0 + \hat\beta_1(x_i)} \\[1em]
\hat\pi_i &= \frac{e^{\hat\beta_0 + \hat\beta_1(x_i)}}{1 + e^{\hat\beta_0 + \hat\beta_1(x_i)}}
\end{split}
$$


We include the right-side of this in the argument `fun=` of the `stat_function()` layer, substituting in the values for $\hat\beta_0$ and $\hat\beta_1$. Below we plot the results from our fitted logistic model.

```{r out.width='3in', fig.cap='Predicted probability of graduating college as a function of ACT score.', fig.pos='H'}
# Plot the fitted equation
ggplot(data = grad, aes(x = act, y = degree)) +
  geom_point(alpha = 0) +
  stat_function(
    fun = function(x) {exp(-1.611 + 0.108*x) / (1 + exp(-1.611 + 0.108*x))}
    ) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Predicted probability of graduating") +
  ylim(0, 1)
```

The monotonic increase in the curve indicates the positive effect of ACT score on the probability of graduating. (Note that we typically interpret $\pi_i$ as probability rather than proportion when interpreting logistic models.) The magnitude of this effect depends on ACT score; the transformed curve is non-linear. For lower ACT scores there is a larger effect of ACT score on the probability of graduating than for higher ACT scores.


# Model-Level Summaries

The `glance()` output for the GLM model also included model-level information. For the model we fitted, the model-level output was:

```{r}
# Model-level output
glance(glm.1)
```


The metric of measuring residual fit is the deviance (remember the deviance was $-2 \times$ log-likelihood). The value in the `null.deviance` column is the residual deviance from fitting the intercept-only model. It acts as a baseline to compare other models.

```{r}
# Fit intercept-only model
glm.0 = glm(degree ~ 1, data = grad, family = binomial(link = "logit"))

# Compute deviance
-2 * logLik(glm.0)[[1]]
```


The value in the `deviance` column is the residual deviance from fitting whichever model was fitted, in our case the model that used ACT score as a predictor.

```{r}
-2 * logLik(glm.1)[[1]]
```

Recall that deviance is akin to the sum of squared residuals (SSE) in conventional linear regression; smaller values indicate less error. In our case, the model that includes ACT score as a predictor has less error than the intercept only model; its deviance is 90 less than the intercept-only model. 

There are two ways to determine whether this decrease in deviance is statistically significant. The first is to examine the $p$-value associated with the ACT predictor in the fitted model. Since that is the only predictor included above-and-beyond the intercept, the $p$-value associated with it indicates whether the ACT predictor is statistically relevant.

The second method to test the improvement in deviance is a test of nested models. Since the intercept-only model is nested in the model that includes ACT as a predictor, we can use a *Likelihood Ratio Test* to examine this. To do so, we use the `anova()` function with the added argument `test="LRT"`.  

```{r}
anova(glm.0, glm.1, test = "LRT")
```

The null hypothesis of this test is that there is NO improvement in the deviance. The results of this test, $\chi^2(1)=89.3$, $p<.001$, indicate that the observed difference of 89.3 is more than we would expect if the null hypothesis was true. In practice, this implies that the more complex model has significantly less error than the intercept-only model and should be adopted.

We could have also used model evidence for making decisions about effects.

```{r}
model.sel(
  list(glm.0, glm.1)
)
```

Here, given the data and the candidate set of models, there is overwhelming evidence to support the model that includes ACT score.



# Including Covariates

Including covariates in the logistic model is done the same way as for `lm()` models. For example, say we wanted to examine the effect of ACT score on probability of graduating, after controlling for whether or not a student was first generation college student. We fit that model as

```{r}
# Fit model
glm.2 = glm(degree ~ 1 + act + firstgen, data = grad, family = binomial(link = "logit"))

# Coefficient-level output
tidy(glm.2)
```

The fitted equation is

$$
\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg) = -1.48 + 0.09(\mathrm{ACT~Score}_i) + 0.52(\mathrm{First~Generation}_i)
$$

Using the logit/log-odds metric, we interpret the coefficients as:

- For students who are not first generation college students with an ACT score of 0, the predicted log-odds of graduating are $-1.48$, on average.
- Each one-point difference in ACT score is associated with a difference of 0.09 in the predicted log-odds of graduating, on average, after controlling for whether o not the students are first generation college students.
- First generation college students, on average, have a predicted log-odds of graduating that is 0.52 higher than students who are not first generation students, after controlling for differences in ACT scores.

\newpage

If we back-transform to odds,

```{r}
exp(coef(glm.2))
```

The interpretations are:

- For students with an ACT score of 0 who are not first generation college students, the predicted odds of graduating are $0.23$, on average.
- Each one-point difference in ACT score is associated with improving the odds of graduating 1.09 times, on average, after controlling for whether o not the students are first generation college students.
- First generation college students, on average, predicted odds of graduating are 1.67 times that of students who are not first generation students, after controlling for differences in ACT scores.

We can also plot the predicted probability of graduating to aid interpretation. Algebraically manipulating the fitted equation,

$$
\hat\pi_i = \frac{e^{-1.48 + 0.088(\mathrm{ACT}_i) + 0.515(\mathrm{First~Generation}_i)}}{1 + e^{-1.48 + 0.088(\mathrm{ACT}_i) + 0.515(\mathrm{First~Generation}_i)}}
$$

We can then produce the fitted equations for non-first generation and first generation students by substituting either 0 or 1, respectively, into the `firstgen` variable. These equations are:

**Non-First Generation Students**

$$
\begin{split}
\hat\pi_i &= \frac{e^{-1.48 + 0.088(\mathrm{ACT}_i) + 0.515(0)}}{1 + e^{-1.48 + 0.088(\mathrm{ACT}_i) + 0.515(0)}} \\[1em]
&= \frac{e^{-1.48 + 0.088(\mathrm{ACT}_i)}}{1 + e^{-1.48 + 0.088(\mathrm{ACT}_i)}} 
\end{split}
$$

**First Generation Students**

$$
\begin{split}
\hat\pi_i &= \frac{e^{-1.48 + 0.088(\mathrm{ACT}_i) + 0.515(1)}}{1 + e^{-1.48 + 0.088(\mathrm{ACT}_i) + 0.515(1)}} \\[1em]
&= \frac{e^{-0.965 + 0.088(\mathrm{ACT}_i)}}{1 + e^{-0.965 + 0.088(\mathrm{ACT}_i)}} 
\end{split}
$$

We can include each of these in a `stat_function()` layer in our plot.

\newpage

```{r fig.width=8, fig.height=6, out.width='4in', fig.cap='Predicted probability of graduating college as a function of ACT score for first generation (solid, red line) and non-first generation (dashed, blue line) students.', fig.pos='H'}
# Plot the fitted equations
ggplot(data = grad, aes(x = act, y = degree)) +
  geom_point(alpha = 0) +
  # Non-first generation students
  stat_function(
    fun = function(x) {exp(-1.48 + 0.0888*x) / (1 + exp(-1.481 + 0.088*x))},
    linetype = "dashed",
    color = "blue"
    ) +
  # First generation students
  stat_function(
    fun = function(x) {exp(-0.965 + 0.0888*x) / (1 + exp(-0.965 + 0.088*x))},
    linetype = "solid",
    color = "red"
    ) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Predicted probability of graduating") +
  ylim(0, 1)
```




