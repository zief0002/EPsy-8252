[
["index.html", "EPsy 8252 Notes Introduction to the Course Prerequisites Books and Other Reading Materials Assignments, Data Sets, Codebooks, and Scripts", " EPsy 8252 Notes Andrew Zieffler 2018-12-12 Introduction to the Course Welcome to EPsy 8252: Methods in Data Analysis for Educational Research II. This is the second course of a two-semester sequence for students in education. The course content for EPsy 8252 includes: (1) likelihood estimation and inference, (2) information criteria for model selection, (3) mixed-effects/multi-level models for analysis of cross-sectional data, (4) mixed-effects/multi-level models for analysis of longitudinal data, and (5) logistic models for analyzing dichotomous outcomes. Time permitting, miscellaneous topics (e.g., design weights, empirical Bayes estimation, semi-parametric models) will also be introduced. Prerequisites Prerequisites include EPsy 8251: Methods in Data Analysis for Educational Research I, or a sound conceptual understanding of the topics of: Foundational topics in data analysis; Design (e.g., random assignment and random sampling) Descriptive statistics and plots One- and two-sample tests Correlation; Simple regression analysis; Model-level and coefficient-level interpretation Ordinary least squares estimation Model-level and coefficient-level inference Assumption checking/residual analysis Multiple linear regression Model-level and coefficient-level interpretation and inference Assumption checking/residual analysis Working with categorical predictors (including adjusting p-values for multiple tests) Interaction effects For the topics listed, students would be expected to be able to carry out an appropriate data analysis and properly interpret the results. It is also assumed that everyone enrolled in the course has some familiarity with using R. Students not meeting these prerequisites are strongly advised to not enroll in EPsy 8252. Books and Other Reading Materials This website will serve as the primary set of course notes. In addition the following textbook is required: Fox, J. (2009). A mathematical primer for social statistics. Thousand Oaks, CA: Sage. There are also two optional textbooks: Anderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. New York: Springer. Dunteman, G. H., &amp; Ho, M.-H. R. (2006). An introduction to generalized linear models. Thousand Oaks, CA: Sage. Assignments, Data Sets, Codebooks, and Scripts The assignments, datasets, and R script files for the course can be found on our accompanying github site: https://github.com/zief0002/epsy-8252 Directions for downloading these materials are found on that site. The data codebooks can be found on this website: Data Codebooks "],
["simple-descript.html", "Unit 1 Simple Linear Regression—Description 1.1 Preparation 1.2 Data Exploration 1.3 Relationship Between Variables 1.4 Statistical Model 1.5 Details about the Regression Model 1.6 Estimating the Regression Coefficients Using R 1.7 Using the Regression Equation 1.8 Conditional Averages 1.9 Slope (Re-visited) 1.10 Revisiting the Statistical Model 1.11 One More Example", " Unit 1 Simple Linear Regression—Description In this set of notes, you will begin your foray into regression analysis. To do so, we will use the riverview.csv data (see the data codebook here) to examine whether education level is related to income for the sample of \\(n=32\\) employees working for the city of Riverview. 1.1 Preparation # Load libraries library(corrr) library(dplyr) library(ggplot2) library(readr) library(sm) # Read in data city = read_csv(file = &quot;~/Documents/github/epsy-8251/data/riverview.csv&quot;) head(city) # A tibble: 6 x 6 education income seniority gender male party &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; 1 8 37.4 7 male 1 Democrat 2 8 26.4 9 female 0 Independent 3 10 47.0 14 male 1 Democrat 4 10 34.2 16 female 0 Independent 5 10 25.5 1 female 0 Republican 6 12 46.5 11 female 0 Democrat 1.2 Data Exploration Any analysis should start with an initial exploration of the data. During this exploration, you should examine each of the variables that you will be including in the regression analysis. This will help you understand results you get in later analyses, and will also help foreshadow potential problems with the analysis. For additional detail, this blog post describes initial ideas of data exploration reasonably well. You could also refer to almost any introductory statistics text. It is typical to begin by exploring the distribution of each variable separately. These distributions are referred to as marginal distributions. After that, it is appropriate to explore the relationships between the variables. 1.2.1 Income To begin this exploration, we will examine the marginal distribution of employees’ incomes. We can plot a marginal distribution using the sm.density() function from the sm package. sm.density(city$income, xlab = &quot;Income&quot;) Figure 1.1: Density plot of employee incomes. This plot suggests that employees’ incomes are unimodal with most incomes between roughly $50,000 and $70,000. The rug at the bottom of the plot (the small vertical line segments) show the 32 incomes from our sample. The smallest income in the sample is about $25,000 and the largest income is over $80,000. (We could find the exact values using the summary() function.) This suggests there is a fair amount of variation in the data. To further summarize the distribution, it is typical to compute and report summary statistics such as the mean and standard deviation. One way to compute these values is to use functions from the dplyr library. city %&gt;% summarize( M = mean(income), SD = sd(income) ) # A tibble: 1 x 2 M SD &lt;dbl&gt; &lt;dbl&gt; 1 53.7 14.6 Describing this variable we might write, The marginal distribution of income is unimodal with a mean of 53.74 thousand dollars. There is variation in employees’ salaries (SD = 14.55 thousand dollars). 1.2.2 Education Level We will also examine the distribution of the education level variable. # Plot sm.density(city$education, xlab = &quot;Education Level&quot;) Figure 1.2: Density plot of employee education levels. Computing the mean and standard deviation: # Summary statistics city %&gt;% summarize( M = mean(education), SD = sd(education) ) # A tibble: 1 x 2 M SD &lt;dbl&gt; &lt;dbl&gt; 1 16 4.36 Again, we might write, The marginal distribution of education is unimodal with a mean of 16 years. There is variation in employees’ level of education (SD = 4.4). 1.3 Relationship Between Variables Although examining the marginal distributions is an important first step in the analysis, those descriptions do not help us directly answer our research question. To better understand the relationship between income and education level we need to explore the distribution of income (\\(Y\\)) as a function of education (\\(X\\)). To do this, we will create a scatterplot of incomes versus education. 1.3.1 Scatterplot Below, we use ggplot() to create a scatterplot. ggplot(data = city, aes(x = education, y = income)) + geom_point() + theme_bw() + xlab(&quot;Education (in years)&quot;) + ylab(&quot;Income (in thousands of U.S. dollars)&quot;) Figure 1.3: Scatterplot displaying the relationship between employee education levels and incomes. The plot suggests a relationship (at least for these employees) between level of education and income. When describing the relationship we want to touch on four characteristics of the relationship: Functional form (structure) of the relationship Direction Strength Observations that do not fit the trend (outliers) 1.3.2 Correlation To numerically summarize relationships between variables, we typically compute correlation coefficients. The correlation coefficient is a quantification of the direction and strength of the relationship. (It is important to note that the correlation coefficient is only an appropriate summarization of the relationship if the functional form of the relationship is linear.) To compute the correlation coefficient, we use the correlate() function from the corrr package. We can use the dplyr-type syntax to select the variables we want correlations between, and then pipe that into the correlate() function. Typically the response (or outcome) variable is the first variable provided in the select() function, followed by the predictor. # Load corrr package library(corrr) # Compute correlation between income and education level city %&gt;% select(income, education) %&gt;% correlate() # A tibble: 2 x 3 rowname income education &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 income NA 0.795 2 education 0.795 NA When reporting the correlation coefficient is is conventional to use a lower-case \\(r\\) and report the value to two decimal places. Subscripts are also generally used to indicate the variables. For example, \\[ r_{\\mathrm{education,~income}} = 0.79 \\] Combining the information culled from the scatterplot with that of the correlation analysis, we could summarize the relationship between education level and income as, There is a strong, positive, linear relationship between education level and income (\\(r = .79\\)). This suggests that city employees with lower education levels tend to have lower incomes, on average, than employees with higher education levels. 1.4 Statistical Model The analytic goal is to now describe the relationship using a statistical model. Since the structure of the relationship between education level and income seems reasonably linear, we will use a linear model to describe the data. We can express this model mathematically as, \\[ Y_i = \\beta_0 + \\beta_1(X_i) + \\epsilon_i. \\] In this equation, \\(Y_i\\) is the outcome/response value; it has an \\(i\\) subscript because it can vary across cases/individuals. \\(\\beta_0\\) is the intercept of the line that best fits the data; it does not vary across individuals. \\(\\beta_1\\) is the slope of the line that best fits the data; it does not vary across individuals. \\(X_i\\) is the predictor value; it has an \\(i\\) subscript because it can vary across cases/individuals. \\(\\epsilon_i\\) is the error term; it has an \\(i\\) subscript because it can vary across cases/individuals. The regression model can be seperated into two components: a systematic (or fixed) component and a random (or stochastic) component. \\[ Y_i = \\underbrace{\\beta_0 + \\beta_1(X_i)}_{\\substack{\\text{Systematic} \\\\ \\text{(Fixed)}}} + \\underbrace{\\epsilon_i}_{\\substack{\\text{Random} \\\\ \\text{(Stochastic)}}} \\] 1.4.1 Fitted Regression Equation The systematic (fixed) part of the equation gives the predicted \\(Y\\) given a particular \\(X\\)-value. The notation for the predicted \\(Y\\) is \\(\\hat{Y}\\). We express this mathematically as, \\[ \\hat{Y}_i = \\beta_0 + \\beta_1(X_i). \\] This is sometimes referred to as the fitted regression equation or the fitted equation. The terms \\(\\beta_0\\) and \\(\\beta_1\\) are referred to as the regression parameters. One of the primary goals of a regression analysis is to estimate the values of the regression parameters (i.e., the intercept and slope terms). (Note that the fitted equation does not include any error terms.) 1.4.2 Errors Now we can re-write the statistical model, substituting \\(\\hat{Y}_i\\) in for the fitted part of the model. \\[ \\begin{split} Y_i &amp;= \\beta_0 + \\beta_1(X_i) + \\epsilon_i \\\\ Y_i &amp;= \\hat{Y}_i + \\epsilon_i \\end{split} \\] This equation implies that each observed \\(Y\\)-value is the sum of the predicted value of the \\(Y\\) (which is based on the \\(X\\)-value) and some error term. Re-arranging the terms, we can mathematically express the error term as, \\[ \\epsilon_i = Y_i - \\hat{Y}_i \\] To compute an observation’s error, we compute the difference between the observation’s observed value (\\(Y_i\\)) and its predicted value (\\(\\hat{Y}_i\\)) based on the fitted equation. When the observed value of \\(Y\\) is larger than the predicted value of \\(Y\\) the error term will be positive (underprediction). If the observed value of \\(Y_i\\) is smaller than the predicted value of \\(Y_i\\) the error term will be negative (overprediction). 1.5 Details about the Regression Model One question you may have is, why is there an error term in the statistical model? We use a single line to describe the relationship between education and income. This line is the same for all of the observations in the sample. For example, look at the figure below which shows the relationship between education and income, but this time also includes the regression line. Figure 1.4: Scatterplot displaying the relationship between employee education levels and incomes. The OLS fitted regression line is also displayed. Consider all the employees that have an education level of 10 years. For all three of them we would predict an income of approximately $37,800. This is denoted by the blue point on the line. The error term allows for discrepancy between the predicted \\(Y\\) and the observed \\(Y\\), which allows us to recover our observed value of the response variable from the model. Graphically, the residual is represented by the vertical distance between the line and a given point on the scatterplot. Some of those points are above the line (they have a positive residual) and some are below the line (they have a negative residual). Also note that for some observations the error term is smaller than for others. 1.5.1 Notation The regression model and the fitted regression equation describe the linear relationship in the population. Greek letters indicate a parameter (a summary of the population). That is why we use the Greek letters \\(\\beta\\) and \\(\\epsilon\\) when we notate the regression model. In most statistical analyses, you will use a sample of data (not the entire population). When we summarize a sample, it is referred to as a statistic, and we use either Roman letters or a Greek letter with a hat. This indicates that the summary measure is an estimate of the parameter. For example, the values produces for the intercept and slope from a regression analysis are estimates. Since they are estimates, we use the hat-notation on the greek letters. \\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1(X_i) \\] The parameter estimates are also referred to as regression coefficients. Synonymously, a hat means estimated/predicted value. Some people use Roman letters when referring to sample estimates. \\[ \\hat{Y}_i = B_0 + B_1(X_i) \\] Similarly, in the population the error terms are expressed as the Greek letter epsilon (\\(\\epsilon_i\\)), while in the sample we use the Roman letter e (\\(e_i\\)). The sample errors are also referred to as residuals. 1.6 Estimating the Regression Coefficients Using R To fit the regression model to data using R, we will use the lm() function. The syntax for this function looks like this: lm(outcome ~ 1 + predictor, data = dataframe) where outcome is the name of the outcome/response variable, predictor is the name of the predictor variable, and dataframe is the name of the data frame. (The one on the right side of the tilde tells R to include the intercept in its computation.) When we fit a regression model in R, we will also assign the output to a new object in R. Below, we fit the model using education level to predict income. lm.1 = lm(income ~ 1 + education, data = city) Here the output is assigned to an object called lm.1. We can print the regression parameter estimates by typing the lm() object name and hitting enter. lm.1 Call: lm(formula = income ~ 1 + education, data = city) Coefficients: (Intercept) education 11.32 2.65 Here the parameter estimates (or regression coefficients) are: \\(\\hat{\\beta}_0 = 11.32\\) \\(\\hat{\\beta}_1 = 2.65\\) The fitted regression equation is \\[ \\hat{\\mathrm{Income}}_i = 11.32 + 2.65(\\mathrm{Education~Level}_i). \\] 1.6.1 Intercept Interpretation The estimate for the intercept was 11.32. Graphically, this value indicates the \\(y\\)-value where the line passes through the \\(y\\)-axis (i.e., \\(y\\)-intercept). As such, it gives the predicted value of \\(Y\\) when \\(X = 0\\). Algebraically we get the same thing if we substitute 0 in for \\(X_i\\) in the estimated regression equation. \\[ \\begin{split} \\hat{Y}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1(0) \\\\ \\hat{Y}_i &amp;= \\hat{\\beta}_0 \\end{split} \\] In our example, \\[ \\begin{split} \\hat{Y}_i &amp;= 11.32 + 2.65(0) \\\\ \\hat{Y}_i &amp;= 11.32 \\end{split} \\] To interpret this value, we use that same idea. Namely The predicted income for all employees that have an education level of 0 years is 11.32 thousand dollars. 1.6.2 Slope Interpretation Recall from algebra that the slope of a line describes the change in \\(Y\\) versus the change in \\(X\\). In regression, the slope describes the predicted change in \\(\\hat{Y}\\) for a one-unit difference in \\(X\\). \\[ \\hat{\\beta}_1 = \\frac{\\Delta\\hat{Y}}{\\Delta X} = \\frac{2.65}{1} \\] In our example, Each one-year difference in education level is associated with a 2.65 thousand dollar predicted difference in income. To better understand this, consider three city employees. The first employee has an education level of 10 years. The second has an education level of 11 years, and the third has an education level of 12 years. Now let’s compute each employee’s predicted income. \\[ \\begin{split} \\mathbf{Employee~1:~}\\hat{\\mathrm{Income}} &amp;= 11.32 + 2.65(10) \\\\ &amp;= 37.82 \\end{split} \\] \\[ \\begin{split} \\mathbf{Employee~2:~}\\hat{\\mathrm{Income}} &amp;= 11.32 + 2.65(11) \\\\ &amp;= 40.47 \\end{split} \\] \\[ \\begin{split} \\mathbf{Employee~3:~}\\hat{\\mathrm{Income}} &amp;= 11.32 + 2.65(12) \\\\ &amp;= 43.12 \\end{split} \\] Each of the employee’s education levels differ by one year (10 to 11 to 12). The difference in predicted incomes for these employees differs by 2.65 thousand dollars. 1.7 Using the Regression Equation Consider the first case in the data frame. city %&gt;% filter(row_number() == 1) # A tibble: 1 x 6 education income seniority gender male party &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; 1 8 37.4 7 male 1 Democrat This employee has an education level of eight years (\\(X_{1}=8\\)). His income is 37.45 thousand dollars (\\(Y_{1}=37.45\\)). Using the fitted equation, we can compute that employee’s predicted income as, 11.32 + 2.65 * 8 [1] 32.52 \\(\\hat{Y}_{12} = 32.52\\). We can also compute that employee’s residual. 37.45 - 32.52 [1] 4.93 \\(e_{1} = 4.93\\). The positive residual suggests that this employee earns 4.93 thousand dollars more than would be expected for a city employee with eight years of formal education. We can also represent these values graphically. Figure 1.5: Plot displaying the OLS fitted regression line (blue) between employee education levels and incomes. The first employee’s observed data (black dot) is plotted, and a visual representation of the employee’s residual (red line) is also displayed. 1.8 Conditional Averages To this point, we have used the fitted regression equation to predict for individual employees. Another way to think about the predicted value is it describes the mean value of \\(Y\\) for all cases with a particular \\(X\\) value. For example, using the values from the previous example we found that \\(\\hat{Y}_{1} = 37.45\\) when \\(X_{1}=8\\). Using means, we could interpret this as, The estimated average income for city employees having 8 years of education is 37.45 thousand dollars. A statistician may refer to the predicted value of \\(Y\\) as a conditional mean (it is conditioned on a particular value of \\(X\\)). To help better understand the idea of conditional means, consider the following plot: Figure 1.6: Plot displaying conditional distribution of \\(Y\\) at several \\(X\\) values. The OLS fitted regression line (dotted) is also shown. The red points show the mean value of \\(Y\\) for each conditional distribution. Examining this plot, we see at each value of \\(X\\) there is a distribution of \\(Y\\). For example, there would be a distribution of incomes for the employees with an education level of 8 years (in the population). There would be another distribution of incomes for the employees with an education level of 9 years (in the population). And so on. The regression equation describes the pattern of conditional means. As such, we can write the fitted equation using means rather than \\(\\hat{Y}\\), \\[ \\mu_{Y|X_i} = \\beta_0 + \\beta_1(X_i) \\] The first part is read as, “the mean of \\(Y\\) given \\(X_i\\)”, or “the mean of \\(Y\\) conditioned on \\(X_i\\)”. Sometimes the mean of a population is denoted as \\(E(Y)\\), or the expected value of \\(Y\\). Then you might see the regression equation written as, \\[ E(Y|X_i) = \\beta_0 + \\beta_1(X_i) \\] When we assume a linear functional form for the model, we are saying that the mean value of \\(Y\\) differs by a constant amount for each one-unit difference in \\(X\\). In other words, the difference between the mean income for those employees who have ten years of education and those that have 11 years of education is the same as the difference between the mean income for those employees who have 17 years of education and those that have 18 years of education. Using this idea, the statistical model can be written as, \\[ \\begin{split} Y_i &amp;= \\beta_0 + \\beta_1(X_i) + \\epsilon_i \\\\ Y_i &amp;= \\mu_{Y|X_i} + \\epsilon_i \\end{split} \\] That is, each observations observed value (\\(Y_i\\)) is a function of the mean \\(Y\\)-value (given \\(X_i\\)) and error. The error represents how far the observed value is from that mean: \\[ \\epsilon_i = Y_i - \\mu_{Y|X_i} \\] In statistical parlance, this is referred to as a mean deviation. 1.8.1 Intercept (Re-visited) Using the idea of conditional means, we can re-visit the interpretation of the intercept, which we had said was the predicted \\(Y\\) for a person with an \\(X\\)-value of zero. Now we can say that the intercept is the predicted mean income for all employees with zero years of formal education. Figure 1.7: Plot displaying conditional distribution of \\(Y\\) at \\(X=0\\). The OLS fitted regression line (dotted) is also shown. The red points show the mean value of \\(Y\\) for this conditional distribution—which corresponfds to the intercept value of the regression line. 1.9 Slope (Re-visited) Using the idea of conditional means, we can also re-visit the interpretation of the slope, which we had said was the predicted difference in \\(Y\\) for employees with a one-year difference in education. Now we can say that the slope is the predicted difference in mean incomes between employees with education levels that differ by one year. Figure 1.8: Plot displaying conditional distribution of \\(Y\\) at \\(X=0\\) and \\(X=1\\). The OLS fitted regression line (dotted) is also shown. The red points show the mean value of \\(Y\\) for these conditional distributions—the relative change which corresponfds to the slope value of the regression line. In general, when interpreting the slope and intercept, you should use the conditional mean interpretations. 1.10 Revisiting the Statistical Model Recall that the statistical model underlying the simple regression is, \\[ Y_i = \\beta_0 + \\beta_1(X_i) + \\epsilon_i \\] and that we can estimate the coefficients (\\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)) and the errors (\\(e_i\\)) using the data (\\(X_i\\) and \\(Y_i\\)). This model can be used to express each of the 32 observations. \\[ \\begin{split} 37.4 &amp;= 11.32 &amp;+ 2.65(8) &amp;+ 4.93 \\\\ 26.4 &amp;= 11.32 &amp;+ 2.65(8) &amp;- 6.09 \\\\ 47.0 &amp;= 11.32 &amp;+ 2.65(10) &amp;+ 9.21 \\\\ 34.2 &amp;= 11.32 &amp;+ 2.65(10) &amp;- 3.64 \\\\ \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 56.3 &amp;= 11.32 &amp;+ 2.65(22) &amp;- 13.30 \\\\ 70.0 &amp;= 11.32 &amp;+ 2.65(22) &amp;+ 0.42 \\\\ 79.2 &amp;= 11.32 &amp;+ 2.65(24) &amp;+ 4.31 \\\\ \\end{split} \\] Looking at these equations, it can be seen that the coefficients (\\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)) are the same in all 32 equations. Because of this we say that the regression coefficients are fixed. The other components, \\(Y_i\\), \\(X_i\\), and \\(e_i\\) vary (can be different) across the 32 equations. Because of this we say that the data (\\(X_i\\) and \\(Y_i\\)) and the errors (\\(e_i\\)) are random. In writing the model, all the random components get an \\(i\\)-subscript; fixed components do not. 1.11 One More Example In this example, we will use the ed-school-2018.csv data (see data codebook for more information) to examine whether doctoral acceptance rate (doc_accept) is related to expert ratings of education schools (expert_score) for the 129 programs ranked in the 2018 Best Graduate Schools. ed_schools = read_csv(file = &quot;~/Documents/github/epsy-8251/data/ed-schools-2018.csv&quot;) head(ed_schools) # A tibble: 6 x 13 rank school score peer expert_score gre_verbal gre_quant doc_accept &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 Harva… 100 4.4 4.6 163 159 4.5 2 2 Stanf… 99 4.6 4.8 162 160 6.1 3 3 Unive… 96 4.2 4.3 156 152 29.1 4 3 Unive… 96 4.1 4.5 163 157 5 5 3 Unive… 96 4.3 4.5 155 153 26.1 6 6 Johns… 95 4.1 4.1 164 162 27.4 # ... with 5 more variables: phd_student_faculty_ratio &lt;dbl&gt;, # phd_granted_per_faculty &lt;chr&gt;, funded_research &lt;dbl&gt;, # funded_research_per_faculty &lt;dbl&gt;, enroll &lt;int&gt; To begin we examine the scatterplot of the relationship between the two variables. The plot below also shows the marginal distributions for the two variables. This plot was created with the ggMarginal() function from the ggExtra package. To learn more, see the ggExtra documentation at https://cran.r-project.org/web/packages/ggExtra/README.html. Figure 1.9: Scatterplot showing the relationship between doctoral acceptance rate and expert rating for 129 graduate programs of education. Plots of the marginal distributions for these distributions are also displayed. We also compute summary measure of each distribution and the correlation between them. # Summary measures of the predictor ed_schools %&gt;% summarize( M = mean(doc_accept), SD = sd(doc_accept) ) # A tibble: 1 x 2 M SD &lt;dbl&gt; &lt;dbl&gt; 1 41.4 21.2 # Summary measures of the outcome ed_schools %&gt;% summarize( M = mean(expert_score), SD = sd(expert_score) ) # A tibble: 1 x 2 M SD &lt;dbl&gt; &lt;dbl&gt; 1 3.64 0.475 # Correlation ed_schools %&gt;% select(expert_score, doc_accept) %&gt;% correlate() # A tibble: 2 x 3 rowname expert_score doc_accept &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 expert_score NA -0.501 2 doc_accept -0.501 NA The scatterplot suggests a moderate linear negative relationship between doctoral acceptance rate and expert rating of education graduate programs (\\(r = -0.50\\)). This suggests that graduate programs that have higher expert ratings tend to have lower acceptance rates of doctoral students. The scatterplot also shows that one program (New Mexico State University) has an expert rating that is substantially lower than the other program’s ratings. # Fit regression model lm.2 = lm(expert_score ~ 1 + doc_accept, data = ed_schools) # Obtain coefficients lm.2 Call: lm(formula = expert_score ~ 1 + doc_accept, data = ed_schools) Coefficients: (Intercept) doc_accept 4.1020 -0.0112 The fitted equation is: \\[ \\hat{\\mathrm{Expert~Rating}_i} = 4.10 - 0.01(\\mathrm{Doctoral~Acceptance~Rate}_i) \\] Interpreting the coefficients, Education graduate programs that accept 0% of doctoral students have a predicted average expert rating of 4.10 (extrapolation). Each one-percent difference in acceptance rate is associated with a \\(-0.01\\)-point difference in ratings, on average. "],
["literature.html", "Unit 2 Literature", " Unit 2 Literature Here is a review of existing methods. "],
["methods.html", "Unit 3 Methods", " Unit 3 Methods We describe our methods in this chapter. "],
["applications.html", "Unit 4 Applications 4.1 Example one 4.2 Example two", " Unit 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["final-words.html", "Unit 5 Final Words", " Unit 5 Final Words We have finished a nice book. "],
["data-codebook.html", "Unit 6 Data Codebooks 6.1 ed-schools.csv 6.2 riverview.csv", " Unit 6 Data Codebooks The data codebooks provide information about the attributes and source of each of the datasets used in the notes. 6.1 ed-schools.csv The data in ed-schools.csv come from U.S. News and World Report (2018) and contain 13 attributes collected from the \\(n=129\\) graduate schools of education ranked in the 2018 Best Graduate Schools. The attributes include: rank: Rank school: Graduate program of Education score: Years of seniority peer: Peer assessment score (5.0 = highest) expert_score: Administrator/expert assessment score (5.0 = highest) gre_verbal: Mean GRE verbal score in 2016 gre_quant: Mean GRE quantitative score in 2016 doc_accept: Acceptance rate for doctoral students in 2016 student_faculty_ratio: Ratio of doctoral students to faculty member in 2016 phd_granted_per_faculty: Doctorates granted per faculty member in 2015–16 funded_research: Funded research (in millions of dollars) funded_research_per_faculty: Funded research per faculty member (in thousands of dollars) enroll: Total graduate education enrollment in 2016 6.2 riverview.csv The data in riverview.csv come from C. Lewis-Beck &amp; Lewis-Beck (2016) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hyopothetical midwestern city. The attributes include: education: Years of formal education income: Annual income (in thousands of U.S. dollars) seniority: Years of seniority gender: Employee’s gender male: Dummy coded gender variable (0 = Female, 1 = Male) party: Political party affiliation References "],
["references.html", "References", " References "]
]
