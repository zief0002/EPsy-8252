[
["index.html", "EPsy 8252 Notes Introduction to the Course Course Content Books and Other Reading Materials Prerequisites", " EPsy 8252 Notes Andrew Zieffler 2018-12-28 Introduction to the Course Welcome to EPsy 8252: Methods in Data Analysis for Educational Research II. This is the second course of a two-semester sequence for students in education. Materials for the course can be found: Syllabus Assignments, Data, and Script Files Data Codebooks Course Content The statistical content for EPsy 8252 includes: (1) likelihood estimation and inference, (2) information criteria for model selection, (3) mixed-effects/multi-level models for analysis of cross-sectional data, (4) mixed-effects/multi-level models for analysis of longitudinal data, and (5) logistic models for analyzing dichotomous outcomes. Time permitting, miscellaneous topics (e.g., design weights, empirical Bayes estimation, semi-parametric models) will also be introduced. Books and Other Reading Materials This website will serve as the primary set of course notes. In addition, the following textbook is required: Fox, J. (2009). A mathematical primer for social statistics. Thousand Oaks, CA: Sage. There are also two optional textbooks: Anderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. New York: Springer. Dunteman, G. H., &amp; Ho, M.-H. R. (2006). An introduction to generalized linear models. Thousand Oaks, CA: Sage. Prerequisites Prerequisites include EPsy 8251: Methods in Data Analysis for Educational Research I, or a sound conceptual understanding of the topics of: Foundational topics in data analysis; Design (e.g., random assignment and random sampling) Descriptive statistics and plots One- and two-sample tests Correlation; Simple regression analysis; Model-level and coefficient-level interpretation Ordinary least squares estimation Model-level and coefficient-level inference Assumption checking/residual analysis Multiple linear regression Model-level and coefficient-level interpretation and inference Assumption checking/residual analysis Working with categorical predictors (including adjusting p-values for multiple tests) Interaction effects For the topics listed, students would be expected to be able to carry out an appropriate data analysis and properly interpret the results. It is also assumed that everyone enrolled in the course has some familiarity with using R. If you need a refresher on any of these topics, see the EPsy 8251 course notes. "],
["rmarkdown.html", "Unit 1: R Markdown Notes Other Resources", " Unit 1: R Markdown In this set of notes, you will learn how to integrate R syntax directly into your word-processed documents to create more reproducible reports. Preparation Before class you will need to do the following: Download the sample BibTeX file Download the CSL style file for the American Psychological Association 6th edition (single-spaced bibliography) from Zotero’s repository. Install the R package tinytex Notes The notes and files you will need can be found at: Unit 01: R Markdown [Class Notes] knitr::include_url(&quot;http://www.datadreaming.org/files/01-introduction-to-r.html#1&quot;) Other Resources In addition to the notes and what we cover in class, there many other resources for learning about R Markdown. Here are some resources that may be helpful in that endeavor: R Markdown documentation: Official R Markdown documentation from RStudio R Markdown cheatsheet: What it sounds like; a cheatsheet for R Markdown knitr: Document and code chunk options for R Markdown R Markdown Gallery: - Gallery of some R Markdown outputs Pimp my Rmd: Blog post providing a few tips to improve the appearance of output documents. For typesetting equations using R Markdown, check out: Using LaTeX to write mathematical content For integrating references into R Markdown, here are a few resources: Zotero CSL style repository Export a BibTeX file from Mendeley Export a BibTeX file from Zotero Finally, here are some references for using reveal.js and remark.js to create sweet-looking presentations: Reveal.js presentations Customizing Reveal.js presentations xaringan "],
["nonlinearity-log-transforming-the-predictor.html", "Unit 2: Nonlinearity: Log-Transforming the Predictor 2.1 Dataset and Research Question 2.2 Log-Transformation of a Variable 2.3 Fitting the Regression Model 2.4 Alternative Method of Fitting the Model 2.5 Plotting the Fitted Model 2.6 Different Base Values in the Logarithm 2.7 Base-\\(e\\) Logarithm: The Natural Logarithm 2.8 Including Covariates 2.9 Polynomial Effects vs. Log-Transformations Other Resources", " Unit 2: Nonlinearity: Log-Transforming the Predictor In this set of notes, you will learn about log-transforming the predictor in a regression model to account for nonlinearity. Preparation Before class you will need to do the following: Refresh your knowledge about logarithms by going though the Khan Academy Intro to Logarithms tutorial. 2.1 Dataset and Research Question The data we will use in this set of notes, mn-schools.csv (see the data codebook here), contains 2011 institutional data for \\(n=33\\) Minnesota colleges and universities. # Load libraries library(broom) library(dplyr) library(ggplot2) library(readr) library(sm) library(tidyr) # Import data mn = read_csv(file = &quot;~/Documents/github/epsy-8252/data/mn-schools.csv&quot;) head(mn) # A tibble: 6 x 6 id name grad public sat tuition &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Augsburg College 65.2 0 10.3 39.3 2 3 Bethany Lutheran College 52.6 0 10.6 30.5 3 4 Bethel University, Saint Paul, MN 73.3 0 11.4 39.4 4 5 Carleton College 92.6 0 14 54.3 5 6 College of Saint Benedict 81.1 0 11.8 43.2 6 7 Concordia College at Moorhead 69.4 0 11.4 36.6 Using these data, we will examine if (and how) academic “quality” of the student-body (measured by median composite SAT score) is related to institutional graduation rates. 2.2 Log-Transformation of a Variable Recall that the scatterplot of SAT scores and graduation rates suggested that the relationship between these variables was curvilinear. Figure 2.1: Scatterplot of the relationship between median SAT score and six-year graduation rate. The loess smoother is also displayed. One way to model this nonlinearity was to fit a model that included a polynomial effect (quadratic). Another method of modeling nonlinearity is to transform the predictor (or outcome) using a nonlinear transformation. One commonly used nonlinear transformation is the logarithm. Below is a comparison of the quadratic function to the logarithmic function. Figure 2.2: Quadratic and logarithmic functions. The quadratic function shows continuous and diminishing growth followed by contiuous and increasing loss (parabola; the function changes direction), while the logarithmic function models continuous, albeit diminishing, growth (the function does not change direction). 2.2.1 Quick Refresher on Logarithms The logarithm is an inverse function of an exponent. Consider this example, \\[ \\log_2 (32) \\] The logarithm of 32 is the exponent to which the base, 2 in our example, must be raised to produce that number. In other words, \\[ \\log_2 (32) \\longrightarrow 2^{x} = 32 \\longrightarrow x=5 \\] Thus, \\[ \\log_2 (32) = 5 \\] To compute a logarithm using R, we use the log() function. We also specify the argument base=, since logarithms are unique to a particular base. For example, to compute the mathematical expression \\(\\log_2 (32)\\), we use log(32, base = 2) [1] 5 There is also a shortcut function to use base-2. log2(32) [1] 5 2.2.2 Log-Transforming Variables For our purposes, we need to log-transform each value in a particular variable. Here, we will log-transform the SAT variable (using base-2). mn = mn %&gt;% mutate( L2sat = log(sat, base = 2) ) head(mn) # A tibble: 6 x 7 id name grad public sat tuition L2sat &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Augsburg College 65.2 0 10.3 39.3 3.36 2 3 Bethany Lutheran College 52.6 0 10.6 30.5 3.41 3 4 Bethel University, Saint Paul, MN 73.3 0 11.4 39.4 3.52 4 5 Carleton College 92.6 0 14 54.3 3.81 5 6 College of Saint Benedict 81.1 0 11.8 43.2 3.57 6 7 Concordia College at Moorhead 69.4 0 11.4 36.6 3.52 How does this log-transformed variable compare to the original SAT predictor. We can examine the density plot of both the original and log-transformed variables to answer this. Comparing the shapes of the two variables, we see that the original variable was right-skewed. The log-transformed variable is also right-skewed, although it is LESS right-skewed than the original. The scale is quite different between the two variables (one is, after all, log-transformed). This has greatly affected the variation. After log-transforming, the variation is much smaller. What happens when we use the log-transformed variable in a scatterplot with graduation rates? ggplot(data = mn, aes(x = L2sat, y = grad)) + geom_point() + geom_smooth(se = FALSE) + theme_bw() + xlab(&quot;Log-transformed SAT score&quot;) + ylab(&quot;Six-year graduation rate&quot;) Figure 2.3: Scatterplot of the relationship between log-transformed median SAT score (base-2) and six-year graduation rate. The loess smoother is also displayed. The relationship between graduation rate and the log-transformed SAT scores is MORE linear than the relationship between graduation rates and the untransformed SAT scores. By transforming the variable using a nonlinear transformation (log) we have “linearized” the relationship with graduation rates. As such, we can fit a linear model to predict graduation rates using the Log-transformed SAT scores as a predictor. 2.3 Fitting the Regression Model To fit the model, we use the lm() function and input the log-transformed SAT scores as the predictor. lm.1 = lm(grad ~ 1 + L2sat, data = mn) 2.3.1 Examine the Assumption of Linearity Before examining the coefficients, we can scrutinize the residuals to see whether the log-transformation helped us meet the assumption of linearity. # Obtain residuals out = augment(lm.1) # Check linearity assumptions ggplot(data = out, aes(x = .fitted, y = .std.resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth() + theme_bw() The assumption looks reasonably met as the horizontal line of \\(y=0\\) is encompassed in the confidence envelope of the loess smoother. 2.3.2 Interpret the Regression Results We can now look at the regression output and interpret the results. # Model-level output glance(lm.1) Examining the model-level output, we see that differences in \\(\\log_2(\\mathrm{SAT})\\) explain 81.13% of the variation in graduation rates. This is statistically significant, \\(F(1,~31)=133.3\\), \\(p&lt;.001\\). Since differences in \\(\\log_2(\\mathrm{SAT})\\) imply that there are differences in the raw SAT scores, we would typically just say that “differences in SAT scores explain 81.13% of the variation in graduation rates.” Moving to the coefficient-level output, # Coefficient-level output tidy(lm.1) We can write the fitted equation as, \\[ \\hat{\\mathrm{Graduation~Rate}} = -306.7 + 106.4\\bigg[\\log_2(\\mathrm{SAT})\\bigg] \\] We can interpret the coefficients as we always do, recognizing that these interpretation are based on the log-transformed predictor. The intercept value of \\(-306.7\\) is the predicted average graduation rate for all colleges/universities with a \\(\\log_2(\\mathrm{SAT})\\) value of 0. The slope value of 106.4 indicates that each one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) is associated with a 106.4-unit difference in graduation rate, on average. 2.3.3 Better Interpretations: Back-transforming While these interpretations are technically correct, it is more helpful to your readers (and more conventional) to interpret any regression results in the metric of SAT scores rather than log-transformed SAT scores. This means we have to back-transform the interpretations. To back-transform a logarithm, we use its inverse function; exponentiation. We interpreted the intercept as, “the predicted average graduation rate for all colleges/universities with a \\(\\log_2(\\mathrm{SAT})\\) value of 0”. To interpret this using the metric of our SAT attribute, we have to understand what \\(\\log_2(\\mathrm{SAT}) = 0\\) is. \\[ \\log_2 (\\mathrm{SAT}) = 0 \\longrightarrow 2^{0} = \\mathrm{SAT} \\] In this computation, \\(\\mathrm{SAT}=1\\). Thus, rather than using the log-transformed interpretation, we can, instead, interpret the intercept as, The predicted average graduation rate for all colleges/universities with a SAT measurement of 1 (median SAT = 100) is \\(-306.7\\). Since there are no colleges/universities in our data that have a SAT value of 1, this is extrapolation. What about the slope? Our interpretation was that “each one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) is associated with a 106.4-unit difference in graduation rate, on average.” Working with the same ideas of back-transformation, we need to understand what a one-unit difference in \\(\\log_2(\\mathrm{SAT})\\) means. Consider four values of \\(\\log_2(\\mathrm{SAT})\\) that are each one-unit apart: \\[ \\log_2(\\mathrm{SAT}) = 1\\\\ \\log_2(\\mathrm{SAT}) = 2\\\\ \\log_2(\\mathrm{SAT}) = 3\\\\ \\log_2(\\mathrm{SAT}) = 4 \\] If we back-transform each of these, then we can see how the four values of the raw SAT variable would differ. \\[ \\begin{split} \\mathrm{SAT} &amp;= 2^1 = 2\\\\ \\mathrm{SAT} &amp;= 2^2 = 4\\\\ \\mathrm{SAT} &amp;= 2^3 = 8\\\\ \\mathrm{SAT} &amp;= 2^4 = 16 \\end{split} \\] When \\(\\log_2(\\mathrm{SAT})\\) is increased by one-unit, the raw SAT value is doubled. We can use this in our interpretation of slope: A doubling of the SAT value is associated with a 106.4-unit difference in graduation rate, on average. The technical language for doubling is a “two-fold difference”. So we would conventionally interpret this as: Each two-fold difference in SAT value is associated with a 106.4-unit difference in graduation rate, on average. To understand this further, consider a specific school, say Augsburg. Their measurement on the SAT variable is 10.3, and their log-transformed SAT score is 3.36. Using the fitted regression equation (which employs the log-transformed SAT), -306.7 + 106.4 * 3.36 [1] 50.8 Augsburg’s predicted graduation rate would be 50.8. If we increase the L2sat score by 1 to 4.36 (which is equivalent to a raw SAT measurment of 20.6; double 10.3), their predicted graduation rate is, -306.7 + 106.4 * 4.36 [1] 157.2 This is an increase of 106.4. 2.4 Alternative Method of Fitting the Model Rather that create the log-transformed SAT score in the data, we can use the log() function on SAT directly in the lm() computation. lm.1 = lm(grad ~ 1 + log(sat, base = 2), data = mn) # Model-level output glance(lm.1) # Coefficient-level output tidy(lm.1) Using this method of fitting the model will be useful as we plot the fitted model. 2.5 Plotting the Fitted Model To aid interpretation of the effect of SAT on graduation rate, we can plot the fitted model. If we used the method of fitting in which we used log() directly in the lm() function, we only need to set up a sequence of SAT values, predict graduation rates using the fitted model, and finally connect these values using a line. # Set up data plot_data = crossing( sat = seq(from = 8.9, to = 14.0, by = 0.1) ) %&gt;% mutate( # Predict yhat = predict(lm.1, newdata = .) ) # Examine data head(plot_data) # Plot ggplot(data = plot_data, aes(x = sat, y = yhat)) + geom_line() + theme_bw() + xlab(&quot;Median SAT score (in hundreds)&quot;) + ylab(&quot;Predicted graduation rate&quot;) Figure 2.4: Plot of the predicted graduation rates as a function of median SAT score (in hundreds). The non-linearity in the plot indicates that there is a diminishing positive effect of SAT on graduation rates. 2.6 Different Base Values in the Logarithm The base value we used in the log() function in the previous example was base-2. Using a base value of 2 was an arbitrary choice. We can use any base value we want. For example, what happens if we use base-10. mn = mn %&gt;% mutate( L10sat = log(mn$sat, base = 10) ) # Examine data head(mn) # A tibble: 6 x 8 id name grad public sat tuition L2sat L10sat &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Augsburg College 65.2 0 10.3 39.3 3.36 1.01 2 3 Bethany Lutheran College 52.6 0 10.6 30.5 3.41 1.03 3 4 Bethel University, Saint P… 73.3 0 11.4 39.4 3.52 1.06 4 5 Carleton College 92.6 0 14 54.3 3.81 1.15 5 6 College of Saint Benedict 81.1 0 11.8 43.2 3.57 1.07 6 7 Concordia College at Moorh… 69.4 0 11.4 36.6 3.52 1.06 Comparing the logarithms of the SAT attribute using base-10 to those using base-2 we see that the base-10 logarithms are smaller. This is because now we are using the base of 10 in our exponent (rather than 2). For example, for Augsburg, \\[ 10^{1.013} = 10.3 \\] If we fit a model using the base-10 lagarithm, lm.2 = lm(grad ~ 1 + log(sat, base = 10), data = mn) # Model-level output glance(lm.2) Examining the model-level output, we see that differences in \\(\\log_{10}(\\mathrm{SAT})\\) explain 81.13% of the variation in graduation rates. Or simply, that differences in SAT scores explain 81.13% of the variation in graduation rates. This is statistically significant, \\(F(1,~31)=133.3\\), \\(p&lt;.001\\). These model-level results are the same as when we used the base-2 logarithm. # Coefficient-level output tidy(lm.2) The fitted equation is, \\[ \\hat{\\mathrm{Graduation~Rate}} = -306.7 + 353.6\\bigg[\\log_{10}(\\mathrm{SAT})\\bigg] \\] We can interpret the coefficients using the base-10 logarithm of SAT scores as: The intercept value of \\(-306.7\\) is the predicted average graduation rate for all colleges/universities with a \\(\\log_{10}(\\mathrm{SAT})\\) value of 0. The slope value of 353.6 indicates that each one-unit difference in \\(\\log_{10}(\\mathrm{SAT})\\) is associated with a 353.6-unit difference in graduation rate, on average. Better yet, we can back-transform the interpretations so that we are using SAT scores rather than \\(\\log_{10}(\\mathrm{SAT})\\) scores. The predicted average graduation rate for all colleges/universities with a SAT value of 1 (median SAT score = 100) is \\(-306.7\\). Each ten-fold difference in SAT is associated with a 353.6-unit difference in graduation rate, on average. To further think about the effect of SAT, if Augsburg improved its median SAT score ten-fold (i.e., going from a SAT value of 10.3 to a value of 103) we would predict its graduation rate to go up by 353.6. 2.6.1 Comparing the Output from the Two Bases The model-level information is all the same. Furthermore, the intercepts (and SE and \\(p\\)-value) was the same across both models. The slope coefficients and SEs were different in the two models, but the \\(t\\)-value and \\(p\\)-value for the effect of SAT was identical for both base-2 and base-10. The only real difference in using base-10 vs. base-2 in the logarithm is in the interpretation of the SAT effect. What if we look at the residual fit? Figure 2.5: Standardized residuals versus the fitted values for the models fitted with the log-2 predictor (left) and the log-10 predictor (right). The residuals fit EXACTLY the same. Why is this? Let’s again use Augsburg as an example. Using the fitted model that employed the base-2 logarithm, we found that Augsburg’s predicted graduation rate was, \\[ \\begin{split} \\hat{\\mathrm{Graduation~Rate}} &amp;= -306.7 + 106.4\\bigg[\\log_2(10.3)\\bigg] \\\\ &amp;= -306.7 + 106.4\\bigg[3.36\\bigg] \\\\ &amp;= 50.8 \\end{split} \\] Using the model that employed the base-10 logarithm, Augsburg’s predicted graduation rate would be \\[ \\begin{split} \\hat{\\mathrm{Graduation~Rate}} &amp;= -306.7 + 353.6\\bigg[\\log_{10}(10.3)\\bigg] \\\\ &amp;= -306.7 + 353.6\\bigg[1.01\\bigg] \\\\ &amp;= 50.8 \\end{split} \\] Augsburg’s predicted graduation rate is exactly the same in the two models. This implies that Augsburg’s residual would also be the same in the two models. This is true for every college. Because of this, increasing (or decreasing) the base used in the logarithm does not help improve the fit of the model. The fit is exactly the same no matter which base you choose. The only thing that changes when you choose a different base is the interpretation of the slope. You should choose the base to facilitate interpretation. For example, does it make more sense to talk about a two-fold difference in the predictor? A five-fold difference in the predictor? A ten-fold difference in the predictor? 2.7 Base-\\(e\\) Logarithm: The Natural Logarithm In our example, neither of the bases we examined is satisfactory in terms of talking about the effect of SAT. Two-fold differences in SAT are very unlikely, to say anything of ten-fold differences. One base that is commonly used for log-transformations is base-\\(e\\). \\(e\\) is a mathematical constant (Euler’s number) that is approximately equal to 2.71828. We can obtain this by using the exp() function in R. This function takes \\(e\\) to some exponanent that is given as the argument. So to obtain the approximation of \\(e\\) we use exp(1) [1] 2.718 The logarithm (base-\\(e\\)) for a number, referred to as the natural logarithm, can be obtained using the log() function with the argument base=exp(1). However, this base is so commonly used that it is the default value for the base= argument. So, if we use the log() function without defining the base= argument, it will automatically use base-\\(e\\). For example, the natural logarithm of Augsburg’s SAT score of 1030 can be computed as log(10.3) [1] 2.332 If we took \\(e^{2.332}\\) we would obtain 10.3. The natural logarithm even has its own mathematical notation; \\(\\ln\\). For example, we would mathematically express the natural logarithm of 10.3 as \\[ \\ln (10.3) = 2.332. \\] 2.7.1 Using the Natural Logarithm in a Regression Model Below we regress graduation rates on the log-transformed SAT scores, using the natural logarithm. # Fit model lm.3 = lm(grad ~ 1 + log(sat), data = mn) # Model-level output glance(lm.3) As with any base, using base-\\(e\\) results in the same model-level information (\\(R^2=.811\\), \\(F(1,~31)=133.3\\), \\(p&lt;.001\\)). # Coefficient-level output tidy(lm.3) The intercept has the same coefficient (\\(\\hat\\beta_0=-306.7\\)), SE, \\(t\\)-value, and \\(p\\)-value as the intercept from the models using base-2 and base-10 log-transformations of SAT. (This is, again, because \\(2^0=10^0=e^0=1\\).) And, although the coefficient and SE for the effect of SAT is again different (a one-unit change in the three different log-scales does not correspond to the same amount of change in raw SAT for the three models), the \\(t\\)-value and level of statistical significance (\\(t(31)=11.55\\), \\(p&lt;.001\\)) for this effect, are the same as when we used base-2 and base-10. So how can we interpret the model’s coefficients? The intercept can be interpreted exactly the same as in the previous models in which we used base-2 or base-10; namely that the predicted average graduation rate for colleges/univeristies with a SAT value of one is \\(-306.7\\). Interpreting the slope, we could say that an \\(e\\)-fold difference in SAT value is associated with a 153.6-unit difference in graduation rates, on average. 2.7.1.1 Interpretation Using Percentage Change Consider three schools, each having a SAT score that differs by 1%; say these schools have SAT values of 10, 10.1, 10.2. Using the fitted equation, we can compute the predicted graduation rate for each of these hypothetical schools: \\[ \\hat{\\mathrm{Graduation~Rate}} = -306.7 + 153.6 \\bigg[\\ln (\\mathrm{SAT})\\bigg] \\] The SAT values and predicted graduation rates for these schools are given below: Table 2.1: SAT values and Graduation Rates for Three Hypothetical Schools that have SAT Values that Differ by One Percent. SAT Predicted Graduation Rate 10.0 46.88 10.1 48.41 10.2 49.93 The difference between each subsequent predicted graduation rate is 1.53. 48.4058 - 46.8778 [1] 1.528 49.9338 - 48.4058 [1] 1.528 In other words, schools that have a SAT value that differ by 1%, have predicted graduation rates that differ by 1.53, on average. 2.7.1.2 Mathematical Explanation To understand how we can directly compute this difference, consider the predicted values for two \\(x\\)-values that differ by one-percent, if we use symbolic notation: \\[ \\begin{split} \\hat{y}_1 &amp;= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(x)\\right] \\\\ \\hat{y}_2 &amp;= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] \\end{split} \\] The difference in their predicted values is: \\[ \\begin{split} \\hat{y}_2 - \\hat{y}_1 &amp;= \\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\left(\\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(x)\\right]\\right) \\\\ &amp;=\\hat\\beta_0 + \\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\hat\\beta_0 - \\hat\\beta_1\\left[\\ln(x)\\right] \\\\ &amp;=\\hat\\beta_1\\left[\\ln(1.01x)\\right] - \\hat\\beta_1\\left[\\ln(x)\\right] \\\\ &amp;=\\hat\\beta_1\\left[\\ln(1.01x) - \\ln(x)\\right]\\\\ &amp;=\\hat\\beta_1\\left[\\ln(\\frac{1.01x}{1x})\\right] \\end{split} \\] If we substitute in any value for \\(x\\), we can now directly compute this constant difference. Note that a convenient value for \\(x\\) is 1. Then this reduces to: \\[ \\hat\\beta_1\\left[\\ln(1.01)\\right] \\] So now, we can interpret this as: a one-percent difference in \\(x\\) is associated with a \\(\\hat\\beta_1\\left[\\ln(1.01)\\right]\\)-unit difference in \\(Y\\), on average. In our model, we can compute this difference using the fitted coefficient \\(\\hat\\beta_1=153.6\\) as \\[ 153.6\\left[\\ln(1.01)\\right] = 1.528371 \\] The same computation using R is 153.6 * log(1.01) [1] 1.528 This gives you the constant difference exactly. So you can interpret the effect of SAT as, each 1% difference in SAT score is associated with a difference in graduation rates of 1.53, on average. 2.7.1.3 Approximate Interpretaton We can get an approximate estimate for the size of the effect by using the mathematical shortcut of \\[ \\mathrm{Effect} \\approx \\frac{\\hat\\beta_1}{100} \\] Using our fitted results, we could approximate the size of the effect as, \\[ \\frac{153.6}{100} = 1.536 \\] We could then interpret the effect of SAT by saying a 1% difference in median SAT score is associated with a 1.53-unit difference in predicted graduation rate, on average. 2.8 Including Covariates We can also include covariates in the model. Below we examine the nonlinear effect of SAT on graduation controlling for differences in sector. # Fit model lm.4 = lm(grad ~ 1 + public + log(sat), data = mn) # Model-level output glance(lm.4) The model explains 86.5% of the variation in graduation rates, \\(F(2,~30)=96.58\\), \\(p&lt;.001\\). # Coefficient-level output tidy(lm.4) Interpreting each of the coefficients using the raw SAT scores: The intercept value of \\(-286.1\\) is the predicted average graduation rate for all public colleges/universities with a SAT value of 1 (extrapolation). There is a statistically significant effect of sector after controlling for differences in SAT score (\\(p=.002\\)). Public schools have a predicted graduation rate that is 8.5-units lower, on average, than private schools controlling for differences in median SAT scores. There is a statistically significant effect of SAT on graduation rates, controlling for differences in sector (\\(p&lt;.001\\)). A 1% difference in median SAT value is associated with a 1.46-unit difference in predicted graduation rate, on average, after controlling for differences in sector. 2.8.1 Plot of the Model Results To further help interpret these effects, we can plot the fitted model. # Set up data plot_data = crossing( sat = seq(from = 8.9, to = 14.0, by = .1), public = c(0, 1) ) %&gt;% mutate( yhat = predict(lm.4, newdata = .), public = factor(public, levels = c(0, 1), labels = c(&quot;Private&quot;, &quot;Public&quot;)) ) #Examine data head(plot_data) # Plot ggplot(data = plot_data, aes(x = sat, y = yhat, color = public, linetype = public)) + geom_line() + theme_bw() + xlab(&quot;Median SAT score (in hundreds)&quot;) + ylab(&quot;Predicted graduation rate&quot;) + ggsci::scale_color_d3(name = &quot;Sector&quot;) + scale_linetype_manual(name = &quot;Sector&quot;, values = c(&quot;solid&quot;, &quot;dashed&quot;)) Figure 2.6: Predicted graduation rate as a function of median SAT score (in hundreds) and sector. The effect of SAT is log-linear. The plot shows the nonlinear, diminishing positive effect of SAT on graduation rate for both public and private schools. For schools with lower median SAT scores, there is a larger effect on graduation rates than for schools with higher median SAT scores (for both private and public schools). The plot also shows the controlled effect of sector. For schools with the same median SAT score, private schools have a higher predicted graduation rate than public schools, on average. 2.9 Polynomial Effects vs. Log-Transformations The inclusion of polynomial effects and the use of a log-transformation was to model the nonlinearity observed in the relationship between SAT scores and graduation rates. Both methods were successful in this endeavor. While either method could be used in practice to model nonlinearity, there are some considerations when making the choice of which may be more appropriate for a given modeling situation. The first consideration is one of theory. The plot below shows the mathematical function for a log-transformed \\(X\\)-value (solid, black line) and for a quadratic polynomial of \\(X\\) (dashed, red line). Figure 2.7: Comparison of quadratic (blue, dashed) and logarithmic (black, solid) functions of X. Both functions are nonlinear, however the polynomial function changes direction. For low values of \\(X\\), the function has a large positive effect. This effect diminishes as \\(X\\) gets bigger, and around \\(X=9\\) the effect is zero. For larger values of \\(X\\), the effect is actually negative. For the logarithmic function, the effect is always positive, but it diminishes as \\(X\\) gets larger. (Functions that constantly increase, or constantly decrease, are referred to as monotonic functions.) Theoretically, these are very different ideas, and if substantive literature suggests one or the other, you should probably acknowledge that in the underlying statistical model that is fitted. Empirically, the two functions are very similar especially within certain ranges of \\(X\\). For example, although the predictions from these models would be quite different for really high values of \\(X\\), if we only had data from the range of 2 to 8 (\\(2\\leq X \\leq 8\\)) both functions would produce similar residuals. In this case, the residuals would likely not suggest better fit for either of the two models. In this case, it might be prudent to think about Occam’s Razor—if two competing models produce similar predictions, adopt the simpler model. Between these two functions, the log-transformed model is simpler; it has one predictor compared to the two predictors in the quadratic model. The mathematical models make this clear: \\[ \\begin{split} \\mathbf{Polynomial:~}Y_i &amp;= \\beta_0 + \\beta_1(X_i) + \\beta_2(X_i^2) +\\epsilon_i \\\\ \\mathbf{Log\\mbox{-}Transform:~}Y_i &amp;= \\beta_0 + \\beta_1\\bigg[\\ln(X_i)\\bigg] + \\epsilon_i \\end{split} \\] The quadratic polynomial model has two effects: a linear effect of \\(X\\) and a quadratic effect of \\(X\\) (remember it is an interaction model), while the model using the log-transformed predictor only has a single effect. If there is no theory to guide your model’s functional form, and the residuals from the polynomial and log-transformed models seem to fit equally well, then the log-transformed model saves you a degree of freedom, and probably should be adopted. Other Resources In addition to the notes and what we cover in class, there are many other resources for learning about log-transformations. Here are some resources that may be helpful in that endeavor: Interpreting Coefficients in Regression with Log-Transformed Variables Interpret Regression Coefficient Estimates "],
["nonlinearity-log-transforming-the-outcome.html", "Unit 3: Nonlinearity: Log-Transforming the Outcome 3.1 Dataset and Research Question 3.2 Examine Relationship between Age and Budget 3.3 Transform the Outcome Using the Natural Logarithm (Base-e) 3.4 Re-analyze using the Log-Transformed Budget 3.5 Interpreting the Regression Output 3.6 Plotting the Fitted Model 3.7 Relationship between MPAA Rating and Budget 3.8 Multiple Regression: Main Effects Model 3.9 Multiple Regression: Interaction Model", " Unit 3: Nonlinearity: Log-Transforming the Outcome In this set of notes, you will learn about log-transforming the outcome variable in a regression model to account for nonlinearity and heterogeneity of variance. Preparation Before class you will need to do the following: Refresh your knowledge about logarithms by going though the Khan Academy Intro to Logarithms tutorial. 3.1 Dataset and Research Question The data we will use in this set of notes, movies.csv (see the data codebook here), includes attributes for \\(n=1,806\\) movies. # Load libraries library(broom) library(dplyr) library(ggplot2) library(readr) library(sm) library(tidyr) # Import data movies = read_csv(file = &quot;~/Documents/github/epsy-8252/data/movies.csv&quot;) head(movies) # A tibble: 6 x 4 title budget age mpaa &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 &#39;Til There Was You 23 21 PG-13 2 10 Things I Hate About You 16 19 PG-13 3 100 Mile Rule 1.1 16 R 4 13 Going On 30 37 14 PG-13 5 13th Warrior, The 85 19 R 6 15 Minutes 42 17 R Using these data, we will examine the relationship between age of a movie and budget. 3.2 Examine Relationship between Age and Budget To being the analysis, we will examine the scatterplot between age and budget of our sample data. ggplot(data = movies, aes(x = age, y = budget)) + geom_point() + geom_smooth(se = FALSE) + theme_bw() + xlab(&quot;Movie age&quot;) + ylab(&quot;Movie Budget (in millions of dollars)&quot;) Figure 3.1: Scatterplot between age and budget. The loess smoother is also displayed. The scatterplot suggests two potential problems with fitting a linear model to the data: The relationship is slightly curvilinear. The variation in budget for more recnt movies is much greater than the variation in budget for older movies (heteroskedasticity). We can see this much more clearly in the scatterplot of residuals versus fitted values from a fitted linear model. # Fit model lm.1 = lm(budget ~ 1 + age, data = movies) # Obtain residuals and fitted values out_1 = augment(lm.1) # Density plot of the residuals sm.density(out_1$.std.resid, model = &quot;normal&quot;, xlab = &quot;Standardized residuals&quot;) # Residuals versus fitted values ggplot(data = out_1, aes(x = .fitted, y = .std.resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth() + theme_bw() + xlab(&quot;Fitted values&quot;) + ylab(&quot;Standardized residuals&quot;) Figure 3.2: Residual plots from regressing budget on age. These plots suggest violations of the normality assumption (the marginal distribution of the residuals is right-skewed) and of the assumption of homoskedasticity. Because of the large sample size, violation the linearity assumption is more difficult to see in this plot. 3.3 Transform the Outcome Using the Natural Logarithm (Base-e) To alleviate problems of non-normality when the conditional distributions are right-skewed (or have high-end outliers) OR to alleviate heteroskadisticy, we can mathematically transform the outcome using a logarithm. Any base can be used for the logarithm, but we will transform the outcome using the natural logarithm because of the interpretive value. First, we will create the log-transformed variable as a new column in the data, and then we will use the log-transformed budget (rather than raw budget) in any analyses. # Create log-transformed budget movies = movies %&gt;% mutate( Lbudget = log(budget) ) # Examine data head(movies) Recall that the logarithm is the inverse function of an exponent. As an example, consider the budget and log-transformed budget for ’Til There Was You. \\[ \\begin{split} \\ln(\\textrm{Budget}) &amp;= 3.135 \\\\ \\ln(23.0) &amp;= 3.135 \\\\ \\end{split} \\] Or, \\[ e^{3.135} = 23.0 \\] Remember, the logarithm answers the mathematical question: \\(e\\) to what power is equal to 23.0? 3.4 Re-analyze using the Log-Transformed Budget Now we will re-examine the scatterplot using the log-transformed outcome to see how this transformation affects the relationship. # Scatterplot ggplot(data = movies, aes(x = age, y = Lbudget)) + geom_point() + geom_smooth(se = FALSE) + theme_bw() + xlab(&quot;Movie age&quot;) + ylab(&quot;ln(Movie Budget)&quot;) Figure 3.3: Scatterplot between age and budget. The loess smoother is also displayed. Log-transforming the outcome has drastically affected the scale for the outcome. Has this helped us better meet the assumptions? Again, we should examine the residual plots. # Fit model lm.2 = lm(Lbudget ~ 1 + age, data = movies) # Obtain residuals and fitted values out_2 = augment(lm.2) # Density plot of the residuals sm.density(out_2$.std.resid, model = &quot;normal&quot;, xlab = &quot;Standardized residuals&quot;) # Residuals versus fitted values ggplot(data = out_2, aes(x = .fitted, y = .std.resid)) + geom_point() + geom_hline(yintercept = 0) + geom_smooth() + theme_bw() + xlab(&quot;Fitted values&quot;) + ylab(&quot;Standardized residuals&quot;) Figure 3.4: Residual plots from regressing the natural logarithm of budget on age. These plots still suggest violations of the normality assumption (the marginal distribution of the residuals is now left-skewed). The assumption of homoskedasticity also seems still violated, although much less. Most importantly, however, is the assumption of linearity now seems satisfied. 3.5 Interpreting the Regression Output Let’s examine the output from the model in which we regressed the log-transformed budget on age. # Model-level output glance(lm.2) The model-level summary information suggests that differences in movies’ ages explains 2.1% of the variation in budget. (Remember, explaining variation in log-budget is the same as explaining variation in budget). Although this is a small amount of variation, it is statistically significant, \\(F(1,1804)=39.27\\), \\(p&lt;0.001\\). # Coefficient-level output tidy(lm.2) From the coefficient-level output the fitted equation is: \\[ \\ln\\left(\\hat{\\mathrm{Budget}_i}\\right) = 3.28 - 0.04(\\mathrm{Age}_i) \\] With log-transformations, there are two possible interpretations we can offer. The first is to interpret the coefficients using the log-transformed values. These we interpret in the exact same way we do any other regression coefficients (except we use log-outcome instead of outcome): The intercept, \\(\\hat{\\beta_0} = 3.28\\), is the average predicted log-budget for movies made in 2019 (Age = 0). The slope, \\(\\hat{\\beta_1} = -0.04\\), indicates that each one-year difference in age is associated with a log-budget that differ by \\(-0.04\\), on average. 3.5.1 Back-Transforming: A More Useful Interpretation A second, probably more useful, interpretation is to back-transform log-budget to budget. To think about how to do this, we first consider a more general expression of the fitted linear model: \\[ \\ln\\left(\\hat{Y}_i\\right) = \\hat\\beta_0 + \\hat\\beta_1(X_{i}) \\] The left-hand side of the equation is in the log-transformed metric, which drives our interpreatations. If we want to instead, interpret using the raw metric of \\(Y\\), we need to back-transform from \\(\\ln(Y)\\) to \\(Y\\). To back-transform, we use the inverse function, which is to exponentiate using the base of the logarithm, in our case, base-\\(e\\). \\[ e^{\\ln(Y_i)} = Y_i \\] If we exponentiate the left-hand side of the equation, to maintain the equality, we also need to exponentiate the right-hand side of the equation. \\[ e^{\\ln(Y_i)} = e^{\\hat\\beta_0 + \\hat\\beta_1(X_{i})} \\] Then we use rules of exponents to simplify this. \\[ Y_i = e^{\\hat\\beta_0} \\times e^{\\hat\\beta_1(X_{i})} \\] For our example, when we exponentiate both sides of the fitted equation: \\[ \\hat{\\mathrm{Budget}_i} = e^{3.28} \\times e^{-0.04(\\mathrm{Age}_i)} \\] 3.5.2 Substituting in Values for Age to Interpret Effects To interpret the effects (which are now interpreted using budget—not log-budget) we can substitute in the different values for age and solve. For example when Age = 0: \\[ \\begin{split} \\hat{\\mathrm{Budget}_i} &amp;= e^{3.28} \\times e^{-0.04(0)}\\\\ &amp;= 26.58 \\times 1 \\\\ &amp;= 26.58 \\end{split} \\] The predicted budget for a movie made in 2019 is 26.58 million dollars. How about a movie that was made in 2018 (a one-year difference)? \\[ \\begin{split} \\hat{\\mathrm{Budget}_i} &amp;= e^{3.28} \\times e^{-0.04(1)}\\\\ &amp;= 26.58 \\times 0.96 \\\\ &amp;= 25.54 \\end{split} \\] The predicted budget for a movie made in 2019 is 25.54 million dollars. This is 0.96 TIMES the budget of a movie made in 2018. Rather than using the language of TIMES difference you could also use the language of fold difference. In this case the slope coefficient would be interpreted as, Each one-year difference in age is associated with a 0.95-fold difference in budget, on average. Simply put, when we back-transform from interpretations of log-\\(Y\\) to \\(Y\\) the interpretations are multiplicatively related to the intercept rather than additive. We can obtain these multiplicative values (and the back-transformed intercept) by using the exp() function to exponentiate the coefficients from the fitted model, which we obtain using the coef() function. exp(coef(lm.2)) (Intercept) age 26.5261 0.9569 3.5.3 Approximate Interpretation of the Slope Remember that by using the natural logarithm we can interpret the effects as percent change. Rather than saying that a movie made in 2018 is predicted to have a budget that is 0.96 TIMES that of a movie made in 2019, we can directly interpret the slope as the percent change. Thus \\(\\hat{\\beta_1}=-0.04\\) can be interpreted as: Each one-year difference in age is associated with a four percent decrease in budget, on average. If you want the specific mathematical change in budget, find \\(1 - e^{\\hat{\\beta_1}}\\). 1 - exp(-0.04) [1] 0.03921 If you use the language of percent decrease/increase, be very careful. Percent change and percentage change are sometimes interpreted differently! It is generally more clear to use the X-fold difference language. 3.6 Plotting the Fitted Model As always, we can plot the fitted model to aid in interpretation. To do this we will create a sequence of ages, predict the log-budget using the fitted model, and then back-transform the log-budgets to raw budget. # Set up data plot_data = crossing( age = seq(from = 13, to = 80, by = 1) ) %&gt;% mutate( # Predict yhat = predict(lm.2, newdata = .) ) # Examine data head(plot_data) # Back-transform the log-budgets plot_data = plot_data %&gt;% mutate( budget = exp(yhat) ) # Examine data head(plot_data) # Plot ggplot(data = plot_data, aes(x = age, y = budget)) + geom_line() + theme_bw() + xlab(&quot;Age&quot;) + ylab(&quot;Predicted budget (in millions of U.S. dollars)&quot;) Figure 3.5: Plot of the predicted movie budget as a function of its age. The non-linearity in the plot indicates that there is a diminishing negative effect of age on budget. Based on this plot, we see the non-linear, negative effect of age on budget. In other words, older movies tend to have a smaller budget, on average, but this decrease is not constant. This pattern of non-linear decline is referred to as exponential decay. Although this function has a different look than the function we saw in the previous unit (it is negative rather than positive), it is also a monotonic function (no change in direction). 3.7 Relationship between MPAA Rating and Budget We also may want to control for differences in MPAA rating. Before we fit the multiple regression model, however, we will first explore whether MPAA rating is a useful covariate by seeing whether there are differences in budget between PG, PG-13m and R rated movies. Since we log-transformed budget (the outcome) in the previous analysis we will need to use the log-transformed outcome in this exploration as well. # Plot the observed data ggplot(data = movies, aes(x = mpaa, y = Lbudget)) + geom_jitter(alpha = 0.2) + stat_summary(fun.y = &#39;mean&#39;, geom = &quot;point&quot;, size = 4, color = &quot;darkred&quot;) + theme_bw() + xlab(&quot;MPAA rating&quot;) + ylab(&quot;ln(Movie Budget)&quot;) Figure 3.6: Jittered scatterplot of log-budget versus MPAA rating. # Compute summary statistics movies %&gt;% group_by(mpaa) %&gt;% summarize( M = mean(Lbudget), SD = sd(Lbudget) ) The scatterplot and summary statistics indicate there are sample differences in the mean log-budgets for the three MPAA ratings. The variation in log-budgets seems roughly the same for the three ratings. 3.7.1 Regression Model Let’s regress log-transformed budget on MPAA rating and examine the output from the model. To do so, we will need to first create three dummy variables for the different ratings. # Create dummy variables movies = movies %&gt;% mutate( pg = if_else(mpaa == &quot;PG&quot;, 1, 0), pg13 = if_else(mpaa == &quot;PG-13&quot;, 1, 0), r = if_else(mpaa == &quot;R&quot;, 1, 0) ) # Fit the model (pg is reference group) lm.3 = lm(Lbudget ~ 1 + pg13 + r, data = movies) # Model-level output glance(lm.3) The model-level summary information suggests that differences in MPAA rating explains 8.9% of the variation in budget. (Remember, explaining variation in log-budget is the same as explaining variation in budget). Although this is a small amount of variation, it is statistically significant, \\(F(2,1803)=88.28\\), \\(p&lt;0.001\\). # Coefficient-level output tidy(lm.3) From the coefficient-level output we see that the fitted equation is: \\[ \\ln\\left(\\hat{\\mathrm{Budget}_i}\\right) = 2.88 + 0.26(\\mathrm{PG\\mbox{-}13}_i) - 0.87(\\mathrm{R}_i) \\] With log-transformations, there are two possible interpretations we can offer. The first is to interpret the coefficients using the log-transformed values. These we interpret in the exact same way we do any other regression coefficients (except we use log-outcome instead of outcome): The intercept, \\(\\hat{\\beta_0} = 2.88\\), is the average predicted log-budget for PG rated movies. The slope associated with PG-13, \\(\\hat{\\beta_1} = 0.26\\), indicates that PG-13 rated movies have a log-budget that is 0.26 higher than PG rated movies, on average. The slope associated with R, \\(\\hat{\\beta_2} = -0.87\\), indicates that R rated movies have a log-budget that is 0.87 lower than PG rated movies, on average. We can also interpet these by back-transforming to raw budget. To do that we exponentiate the coefficients. exp(coef(lm.3)) (Intercept) pg13 r 17.8134 1.2998 0.4201 PG rated movies have a budget of 17.81 million dollars, on average. PG-13 rated movies have a budget that is 1.30 TIMES the estimated budget for PG rated movies, on average. R rated movies have a budget that is 0.42 TIMES the estimated budget for PG rated movies, on average. 3.7.2 Mathematical Explanation Remember, if we want to interpret using the raw metric of \\(Y\\), we need to back-transform from \\(\\ln(Y)\\) to \\(Y\\). To back-transform, we use the inverse function, which is to exponentiate using the base of the logarithm, in our case, base-\\(e\\). For our example, when we exponentiate both sides of the fitted equation: \\[ \\begin{split} e^{\\ln\\left(\\hat{\\mathrm{Budget}_i}\\right)} &amp;= e^{2.88 + 0.26(\\mathrm{PG\\mbox{-}13}_i) - 0.87(\\mathrm{R}_i)} \\\\ \\hat{\\mathrm{Budget}_i} &amp;= e^{2.88} \\times e^{0.26(\\mathrm{PG\\mbox{-}13}_i)} \\times e^{-0.87(\\mathrm{R}_i)} \\end{split} \\] To interpret the effects (which are now interpreted using budget—not log-budget) we can substitute in the different dummy variable patterns and solve. \\[ \\begin{split} \\textbf{PG Movie:}~~ \\hat{\\mathrm{Budget}_i} &amp;= e^{2.88} \\times e^{0.26(0)} \\times e^{-0.87(0)}\\\\ &amp;= 17.81 \\times 1 \\times 1 \\\\ &amp;= 17.81 \\end{split} \\] \\[ \\begin{split} \\textbf{PG-13 Movie:}~~ \\hat{\\mathrm{Budget}_i} &amp;= e^{2.88} \\times e^{0.26(1)} \\times e^{-0.87(0)}\\\\ &amp;= 17.81 \\times 1.30 \\times 1 \\\\ &amp;= 23.15 \\end{split} \\] \\[ \\begin{split} \\textbf{R Movie:}~~ \\hat{\\mathrm{Budget}_i} &amp;= e^{2.88} \\times e^{0.26(0)} \\times e^{-0.87(1)}\\\\ &amp;= 17.81 \\times 1 \\times 0.42 \\\\ &amp;= 7.48 \\end{split} \\] 3.7.3 Approximate Interpretations Unfortunately, the approximate interpretations of the slopes by directly interpreting the coefficients using the language of percent change are not completely trustworthy. If we did interpret them, the interpretations for the two slopes would be: PG-13 rated movies have budget that is 26.2 percent higher than PG rated movies, on average. R rated movies have budget that is 87 percent lower than PG rated movies, on average. This interpretation is roughly true for the PG-13 effect, but not for the R effect. This approximate interpretation starts to become untrustworthy when the slope value is higher than about 0.20 or so. 3.8 Multiple Regression: Main Effects Model Now we can fit a model that includes our focal predictor of age and our covariate of MPAA rating. # Fit model (PG is reference group) lm.4 = lm(Lbudget ~ 1 + age + pg13 + r, data = movies) # Model-level output glance(lm.4) The model-level summary information suggests that differences in age and MPAA rating of a movie explains 11.0% of the variation in budget. (Remember, explaining variation in log-budget is the same as explaining variation in budget); \\(F(3,1802)=73.84\\), \\(p&lt;0.001\\). # Coefficient-level output tidy(lm.4) The coefficient-level output suggest that there is still a statistically significant effect of age on budget, after controlling for differences in MPAA rating; \\(t(1802)=-6.41\\), \\(p&lt;.001\\). To determine if there is an effect of MPAA rating, after accounting for differences in age, at least one of the effects of MPAA rating need to be statistically significant. Here we see that the coefficient associated with rated R movies is statistically significant. Remember that when we have more than two categories (more than one dummy variable) there can be many ways for the effect to play out, and not all of these are represented in the model we fitted. One way we can simultaneously examine ALL the ways this effect can play out is to use a nested \\(F\\)-test. 3.8.1 Nested F-Test If we want to examine if there is a controlled effect of MPAA rating (controlling for age), we want to see whether by including MPAA rating in a model THAT ALREDY INCLUDES age we explain additional variation in the outcome. To do this we can compare a model that only includes the effect of age to a model that includes both the effects of age and MPAA rating. If the latter model explains a statistically significant amount of additional variation we can say that there is an effect of MPAA rating after controlling for differences in age. In statistical hypothesis testing we are examining the following null hypothesis: \\[ H_0: \\rho^2_{\\mathrm{Age},\\mathrm{MPAA~rating}} - \\rho^2_{\\mathrm{Age}} = 0 \\] If we fail to reject this hypothesis, then the two models explain the SAME amount of variation and we should adopt the simpler model; MPAA rating does not explain additional variation in budget. If we reject this hypothesis, MPAA rating does explain additional variation in budget, above and beyond age; and we should adopt the model that includes both effects. To test this hypothesis we fit both models and then give both models to the anova() function. # Fit models lm.2 = lm(Lbudget ~ 1 + age, data = movies) lm.4 = lm(Lbudget ~ 1 + age + pg13 + r, data = movies) # Nested F-test anova(lm.2, lm.4) The test suggests that there is a statistically significant effect of MPAA rating even after accounting for differences in age; \\(F(2, 1802)=89.21\\), \\(p&lt;.001\\). 3.8.2 Coefficient-Level Interpretation To interpret the coefficients, we will again exponentiate the fitted coefficients so we can interpret them using the raw-metric of budget. exp(coef(lm.4)) (Intercept) age pg13 r 41.7120 0.9578 1.2318 0.4051 The model estimated budget for a PG movie (reference group) that was made in 2019 (age = 0) is 41.71 million dollars. Each one-year difference in age is associated with a 0.96-fold difference (4.3% decrease) in budget, on average, controlling for differences in MPAA rating. PG-13 rated movies have a budget that is 1.23 times that for PG movies, on average, controlling for differences in age. R rated movies have a budget that is 0.41 times that for PG movies, on average, controlling for differences in age. 3.8.3 Plot of the Fitted Model To plot the fitted model that includes a categorical predictor with more than two levels, it is best to re-fit the lm() using the categorical variable. # Re-fit the model lm.5 = lm(Lbudget ~ 1 + age + mpaa, data = movies) # Model-level output glance(lm.5) # Coefficient-level output tidy(lm.5) Note this is the exact same model we fitted using the dummy variables, but R will choose the reference group for us (alphabetically). We can now set up our plotting data, predict, back-transform the outcome, and plot. # Set up data plot_data = crossing( age = seq(from = 13, to = 80, by = 1), mpaa = c(&quot;PG&quot;, &quot;PG-13&quot;, &quot;R&quot;) ) %&gt;% mutate( yhat = predict(lm.5, newdata = .), budget = exp(yhat) ) # Examine data head(plot_data) # Plot ggplot(data = plot_data, aes(x = age, y = budget, color = mpaa, linetype = mpaa)) + geom_line() + theme_bw() + xlab(&quot;Age&quot;) + ylab(&quot;Predicted budget (in millions of U.S. dollars)&quot;) + ggsci::scale_color_d3(name = &quot;MPAA rating&quot;) + scale_linetype_manual(name = &quot;MPAA rating&quot;, values = 1:3) Figure 3.7: Plot of the predicted movie budget as a function of its age and MPAA rating. The non-linearity in the plot indicates that there is a diminishing negative effect of age on budget. The plot displays the negative, nonlinear effect of age on budget for all three types of movies (main effect of age). It also shows that PG-13 rated movies have a higher predicted budget than PG and R rated movies, and that PG rated movies have a higher predicted budget than R rated movies at EVERY age. This is the main effect of MPAA rating. Notice that in the plot, the three lines are not parallel. This is a mathematical artifact of back-transforming log-budget to raw budget. It does not indicate that an interaction model was fitted. How non-parallel the lines are depends on the size of the coefficients associated with the MPAA effects (in this example). This is why, especially with transformed data, it is essential to plot the model to make sure you are understanding the interpretations from your coefficients. 3.9 Multiple Regression: Interaction Model To study whether there is an interaction effect between MPAA rating and age, we will fit the interaction model and compare it to the main-effects model using the nested \\(F\\)-test. # Fit the models lm.5 = lm(Lbudget ~ 1 + age + mpaa, data = movies) lm.6 = lm(Lbudget ~ 1 + age + mpaa + age:mpaa, data = movies) # Nested F-test anova(lm.5, lm.6) The test suggests that we should adopt the main-effects model. The interaction-effect was not statistically significant; \\(F(2,1800)=1.41\\), \\(p=.244\\). If the model that included the interaction effect was adopted, it would suggest that: (1) the effect of age on budget depends on MPAA rating, or (2) the effect of MPAA rating on budget depnds on age of the movie. To further interpret these effects, you should plot the results of the fitted interaction model. "],
["log-transformations-some-final-thoughts.html", "Log Transformations: Some Final Thoughts Power Transformations", " Log Transformations: Some Final Thoughts There are four general curvilinear, monotonic functions (shown below). Figure 3.8: Four general monotonic, curvilinear shapes. In the previous two units you learned how to transform the data to “linearize” two of the four monotonic functions: For positive, decelerating functions we log-transformed X; and For positive, accelerating functions we log-transformed Y. To better understand how we can use transformations to straighten-out relationships, we will examine a set of transformations known as power transformations. Power Transformations Consider a set of powers, \\(p\\) that can be used in the exponent of a variable \\(X\\) (the variable is irrelevant; it could also be \\(Y\\)) so that a transfortmation of the variable \\(X\\) is: \\[ \\mathrm{Transformed~Variable} = X^p \\] Consider the following values for \\(p\\): \\[ p = \\{-3,-2,-1,0,1,2,3\\} \\] These all represent particular transformations of \\(X\\). Note that when \\(p=1\\), the variable \\(X\\) is left untransformed (\\(X^1 = X\\)). Consider the following transformations: \\[ \\begin{split} &amp;X^3 \\\\ &amp;X^2 \\\\ &amp;X^1 \\qquad \\mathrm{Untransformed} \\end{split} \\] These are shown in the figure below. Power transformations that are bigger than 1 (\\(p&gt;1\\)) show positive acceleration. Powers larger than one are referred to as upward transformations as they increase the power (move it up) from one. Now consider these transformations: \\[ \\begin{split} &amp;X^{-1} \\\\ &amp;X^{-2} \\\\ &amp;X^{-3} \\end{split} \\] These are shown in the figure below. For power transformations that are smaller than 1 (\\(p&lt;1\\)), the function shows positive deceleration. Powers that are smaller than one are referred to as downward transformations as they decrease the power (move it down) from one. Ladder of Transformations If we order the different values of \\(p\\), they form what statisticians call a “ladder of re-expression” or “ladder of transformations”. \\[ \\begin{split} &amp; ~~~~~\\vdots \\\\ &amp;Y^3,X^3 &amp;\\qquad \\mathrm{Upward~Transformation}\\\\ &amp;Y^2,X^2 &amp;\\qquad \\mathrm{Upward~Transformation}\\\\ &amp;Y^1,X^1 &amp;\\qquad \\mathrm{Untransformed} \\\\ &amp;Y^{\\frac{1}{2}},X^{\\frac{1}{2}} &amp;\\qquad \\mathrm{Downward~Transformation}\\\\ &amp;Y^0,X^0 \\equiv \\ln(X) &amp;\\qquad \\mathrm{Downward~Transformation}\\\\ &amp;Y^{-1},X^{-1} &amp;\\qquad \\mathrm{Downward~Transformation}\\\\ &amp;Y^{-2},X^{-2} &amp;\\qquad \\mathrm{Downward~Transformation}\\\\ &amp;Y^{-3},X^{-3} &amp;\\qquad \\mathrm{Downward~Transformation} \\\\ &amp; ~~~~~\\vdots \\end{split} \\] Rule of the Bulge To determine how we need to transform data, we can rely on Mosteller and Tukey’s ‘Rule of the Bulge’. This rule, depicted visually below, has us “move on the ladder in the direction in which the bulge points”. Figure 3.9: Four general monotonic, curvilinear shapes. The Rule of the Bulge helps us identify how to transform the data to linearize any of these four shapes. For example, below we re-vist the mn-schools.csv data and again look at the relationship between median SAT score and six-year graduation rate. Figure 3.10: Scatterplot of the relationship between median SAT score and six-year graduation rate. The loess smoother is also displayed. This positive, decelerating relationship is similar to the one in the upper-lefthand quadrant of the ‘Rule of the Bulge’. To linearize this we can either: Transform \\(X\\) using a DOWNWARD transformation; or Transform \\(Y\\) using an UPWARD transformation. In the Unit 2 notes, we linearized this by taking the natural logarithm of SAT; a downward transformation of \\(X\\). What about the relationship between movie age and budget we looked at in Unit 3? Figure 3.11: Scatterplot between age and budget. The loess smoother is also displayed. This negative, decelerating relationship is similar to the one in the lower-lefthand quadrant of the ‘Rule of the Bulge’. To linearize this we can either: Transform \\(X\\) using a DOWNWARD transformation; or Transform \\(Y\\) using an DOWNWARD transformation. In the Unit 3 notes, we linearized this by taking the natural logarithm of budget; a downward transformation of \\(Y\\). By transforming \\(Y\\) instead of \\(X\\), we also fixed a problem of heterogeneity of variance; a problem in the residuals (which is related to \\(Y\\)). "],
["probability-distributions.html", "Unit 4: Probability Distributions 4.1 Dataset and Research Question 4.2 Normal Distribution 4.3 Student’s \\(t\\)-Distribution 4.4 Using the \\(t\\)-Distribution in Regression 4.5 Model-Level Inference: The \\(F\\)-Distribution 4.6 Mean Squares are Variance Estimates", " Unit 4: Probability Distributions In this set of notes, you will learn about common probability distributions. Preparation Before class you will need to do the following: Read Section 3.1.1: Probability Basics in Fox [Required Textbook] Read Sections 3.3.1–3.3.4 (Continuous Distributions) in Fox [Required Textbook] Refresh your knowledge about probability distributions by going though the Kahn Academy: Random Variables and Probability Distributions tutorial. 4.1 Dataset and Research Question In this set of notes, we will not be using a specific dataset. # Load libraries library(broom) library(dplyr) library(ggplot2) library(readr) library(sm) library(tidyr) 4.2 Normal Distribution The probability distribution of a normal distribution is mathematically defined as: \\[ p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] \\] for \\(-\\infty \\leq x \\leq \\infty\\). Consider a normal distribution with a mean (\\(\\mu\\)) of 50, and a standard deviation (\\(\\sigma\\)) of 10. We can compute the probability density (\\(p(x)\\)) for a particular \\(x\\) value by using this equation. For example, the probability density for \\(x=65\\) can be found using, \\[ p(65) = \\frac{1}{10\\sqrt{2\\pi}}\\exp\\left[-\\frac{(65-50)^2}{2\\times10^2}\\right] = 0.01295176 \\] Using R, we can carry out the computation, (1 / (10 * sqrt(2 * pi))) * exp(-(225) / 200) [1] 0.01295 There is also a more direct way to compute this using the dnorm() function. This function computes the density of x from a normal distribution with a specified mean and sd. dnorm(x = 65, mean = 50, sd = 10) [1] 0.01295 If we compute the density for several \\(x\\) values and plot them, we get the familiar normal shape; the graphical instantiation of the mathematical equation. # Create dataset fig_01 = data.frame( X = seq(from = 10, to = 90, by = 0.01) ) %&gt;% rowwise() %&gt;% mutate( Y = dnorm(x = X, mean = 50, sd = 10) ) # Create plot ggplot(data = fig_01, aes(x = X, y = Y)) + geom_line() + theme_bw() + geom_point(x = 65, y = 0.01295176, size = 3) Figure 4.1: Plot of the probability density function (PDF) for a Normal distribution with mean of 50 and standard deviation of 10. The density value for \\(x=65\\), \\(p(65)= 0.01295176\\), is also displayed on the PDF. 4.2.1 Other Useful R Functions for Working with Probability Distributions There are four primary functions for working with the normal probability distribution: dnorm() : To compute the probability density (point on the curve) pnorm() : To compute the probability (area under the PDF) qnorm() : To compute the \\(x\\) value given a particular probability rnorm() : To draw a random observation from the distribution Each of these requires the arguments mean= and sd=. Let’s look at some of them in use. 4.2.2 Finding Cumulative Probability The function pnorm() gives the probability \\(x\\) is less than or equal to some quantile value in the distribution; the cumulative probability. For example, to find the probability that \\(x \\leq 65\\) we would use, pnorm(q = 65, mean = 50, sd = 10) [1] 0.9332 This is akin to finding the proportion of the area under the normal PDF that is to the left of 65. Figure 4.2: Plot of the PDF for a normal distribution (M=50, SD=10) with the cumulative probability for X less than or equal to 65 shaded. For the mathematically inclined, the grey-shaded area is expressed as an integral \\[ \\int_{-\\infty}^{65} p(x) dx \\] where \\(p(x)\\) is the PDF for the normal distribution. 4.2.3 Cumulative Density and \\(p\\)-Value This type of computation is used most commonly to find a \\(p\\)-value. The \\(p\\)-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. Consider a hypothesis test of whether a population parameter is equal to 0. Also consider that we observed a statistic (that has been standardized) of \\(z=2.5\\). Then, the \\(p\\)-value can be graphically displayed in the standard normal distribution as follows: Figure 4.3: Plot of the probability density function (PDF) for the standard normal distribution (M=0, SD=1). The cumulative density representing the p-value for a two-tailed test evaluating whether mu=0 using an observed z-value of 2.5 is also displayed. In most hypthesis tests, we test whether the parameter IS EQUAL to 0. Thus the values in the standard normal distribution more extreme than 2.5 encompass evidence against the hypothesis; those values greater than 2.5 and also those values less than \\(-2.5\\). (This is akin to testing a fair coin when both 8 heads OR 8 tails would provide evidence against fairness we have to consider evidence in both directions). To compute this we use pnorm(). Remember, it computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the are to the left of \\(-2.5\\) and then double it to produce the actual \\(p\\)-value. 2 * pnorm(q = -2.5, mean = 0, sd = 1) [1] 0.01242 4.2.4 Finding Quantiles The qnorm() function is essentially the inverse of the pnorm() function. The p functions find the cumulative probability GIVEN a particular quantile. The q functions find the quantile GIVEN a cumulative probability. For example, in the normal distribution we defined earlier, half of the area is below the quantile value of 50 (the mean). qnorm(p = 0.5, mean = 50, sd = 10) [1] 50 4.3 Student’s \\(t\\)-Distribution Student’s \\(t\\)-distribution looks like a standard normal distribution. In the figure below, Student’s \\(t\\)-distribution is depicted with a solid, black line and the standard normal distribution (\\(M=0\\), \\(SD=1\\)) is depicted with a dotted, red line. Figure 4.4: Plot of the probability density function (PDF) for the standard normal distribution (dotted, red line) and Student’s t-distribution with 5 degrees of freedom (solid, black line). Both the standard normal distribution and Student’s \\(t\\)-distribution have a mean (expected value) of 0. The standard deviation for Student’s \\(t\\)-distribution is larger than the standard deviation for the standard normal distribution (\\(SD&gt;1\\)). You can see this in the distribution because the tails in Student’s \\(t\\)-distribution are fatter (more error) than the standard normal distribution. In practice, we often use Student’s \\(t\\)-distribution rather than the standard normal distribution when we are using sample data to estimate the population. This estimation increases the error and thus is typically modeled using Student’s \\(t\\)-distribution. Student’s \\(t\\)-distribution constitutes a family of distributions—not just a single distribution. The specific shape (and thus probability density) is defined by the degrees of freedom; df. The plot below shows the standard normal distribution (purple) and four \\(t\\)-distributions with varying df-values. Figure 4.5: Plot of several t-distributions with differing degrees of freedom. If we compare the means and SDs for these distributions, we find that the mean for all the \\(t\\)-distributions is 0, same as the standard normal distribution. All \\(t\\)-distributions are unimodal and symmetric around zero. The SD for every \\(t\\)-distribution is higher than the SD for the standrad normal distribution. Student \\(t\\)-distributions with higher df values have less variation. It turns out that the standard normal distribution is a \\(t\\)-distribution with \\(\\infty\\) df. For the formula for the SD in a \\(t\\)-distribution, see Fox (2009). There are four primary functions for working with Student’s \\(t\\)-distribution: dt() : To compute the probability density (point on the curve) pt() : To compute the probability (area under the PDF) qt() : To compute the \\(x\\) value given a particular probability rt() : To draw a random observation from the distribution Each of these requires the arguments df=. Let’s look at some of them in use. 4.3.1 Comparing Probability Densities How do the probability densities for a value of \\(X\\) compare across these distributions? Let’s examine the \\(X\\) value of 2. # Standard normal distribution pnorm(q = 2, mean = 0, sd = 1) [1] 0.9772 # t-distribution with 3 df pt(q = 2, df = 3) [1] 0.9303 # t-distribution with 5 df pt(q = 2, df = 5) [1] 0.949 # t-distribution with 10 df pt(q = 2, df = 10) [1] 0.9633 # t-distribution with 25 df pt(q = 2, df = 25) [1] 0.9718 We are essentially comparing the height of these distributions at \\(X=2\\). Figure 4.6: Plot of several t-distributions with differing *degrees of freedom. The probability density for t=2 is also displayed for each of the distributions. 4.3.2 Comparing Cumulative Densities What if we wanted to look at cumulative density? Consider out hypothesis test of whether a population parameter is equal to 0. ALso consider that we observed a statistic (that has been standardized) of 2.5 using a sample size of \\(n=15\\). If we can assume that the SAMPLING DISTRIBUTION is normally-distributed then we can use the cumulative density in a normal distribution to compute a \\(p\\)-value: 2 * pnorm(q = -2.5, mean = 0, sd = 1) [1] 0.01242 If, however, the SAMPLING DISTRIBUTION is \\(t\\)-distributed then we need to use the cumulative density for a \\(t\\)-distribution with the appropriate df to compute a \\(p\\)-value. For example if we use \\(df=n-1\\), the two-tailed \\(p\\)-value would be: 2 * pt(q = -2.5, df = 14) [1] 0.02547 The \\(p\\)-value using the \\(t\\)-distribution is larger than the \\(p\\)-value computed based on the standard normal distribution. This is again because of the increased error (uncertainty) we are introducing when we estimate from sample. This added uncertainty makes it harder for us to reject a hypothesis. 4.4 Using the \\(t\\)-Distribution in Regression To illustrate how probability distributions are used in practice, we will will use the riverside.csv (see the data codebook here) and fit a regression model that uses education level and seniority to predict variation in employee income. # Read in data city = read_csv(file = &quot;~/Documents/github/epsy-8252/data/riverside.csv&quot;) head(city) # Fit regression model lm.1 = lm(income ~ 1 + education + seniority, data = city) # Coefficient-level output tidy(lm.1) How do we obtain the \\(p\\)-value for each of the coefficients? Recall that the coefficients and SEs for the coefficients are computed directly from the raw data. Then we can compute a test-statistic by dividing the coefficient estimate by the SE. For example, to compute the test-statistic associated with education level: \\[ t = \\frac{2252}{335} = 6.72 \\] Since we are estimating the SE using sample data, our test statistic is likely \\(t\\)-distributed. Which value should we use for df? Well, for that, statistical theory tells us that we should use the error df value. In our data, \\[ \\begin{split} n &amp;= 32 \\\\ \\mathrm{Total~df} &amp;= 32-1 = 31\\\\ \\mathrm{Model~df} &amp;= 2~\\mathrm{(two~predictors)} \\\\ \\mathrm{Error~df} &amp;= 31-2 = 29 \\end{split} \\] Using the \\(t\\)-distribution with 29 df, 2 * pt(q = -6.72, df = 29) [1] 0.0000002257 For seniority (and the intercept), we would use the same \\(t\\)-distribution, but our test statistic would differ: \\[ \\begin{split} t_{\\mathrm{Intercept}} &amp;= \\frac{6769}{5373} = 1.26 \\\\ t_{\\mathrm{Seniority}} &amp;= \\frac{739}{210} = 3.52 \\\\ \\end{split} \\] The associated \\(p\\)-values are: # Intercept p-value 2 * pt(q = -1.26, df = 29) [1] 0.2177 # Seniority p-value 2 * pt(q = -3.52, df = 29) [1] 0.001446 4.5 Model-Level Inference: The \\(F\\)-Distribution The model-level inference for regression is based on an \\(F\\)-statistic, which is a standardized measure of \\(R^2\\). # Model-level output glance(lm.1) In this example, the sample \\(R^2\\) value is 0.742. Computation of the \\(F\\)-statistic relies on two df values—the model degrees of freedom (2) and the error degrees of freedom (29). To compute the \\(F\\)-statistics from \\(R^2\\) we use: \\[ F = \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\] In our example, we compute \\(F\\) as: \\[ \\begin{split} F &amp;= \\frac{0.742}{1-0.742} \\times \\frac{29}{2} \\\\ &amp;= 41.7 \\end{split} \\] We write this standardization of \\(R^2\\) as \\(F(2,29)=41.7\\). 4.5.1 Testing the Model-Level Null Hypothesis It is often worth testing whether the model explains a statistically significant amount of variation in the population. To do this we test the null hypothesis: \\[ H_0:\\rho^2 = 0 \\] Similar to the tests of the coefficients, we evaluate our test statistic (\\(F\\) in this case) in the appropriate test distribution, in this case an \\(F\\)-distribution with 2 and 29 degrees of freedom. (The shape of the \\(F\\)-distribution is based on two df values.) The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. Figure 4.7: Plot of several F-distributions with differing degrees of freedom. The F(2,29)-distribution is shown as a solid, black line. The \\(F\\)-distribution, like the \\(t\\)-distribution is a family of distributions. They are positively skewed and generally have a lower-limit of 0. Because of this, when we use the \\(F\\)-distribution to compute a \\(p\\)-value, we only compute the cumulative density GREATER THAN OR EQUAL TO the value of the standradized test statistic. 4.5.1.1 Computing F from the ANOVA Partitioning We can also compute the model-level \\(F\\)-statistic using the partititioning of variation from the ANOVA table. anova(lm.1) The \\(F\\)-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general computation \\[ \\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}} \\] The model includes both the education and seniority predictor, so we combine the SS and df. The MS model is: \\[ \\begin{split} \\mathrm{MS}_{\\mathrm{Model}} &amp;= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\ &amp;= \\frac{4147330492 + 722883649}{1 + 1} \\\\ &amp;= \\frac{4870214141}{2} \\\\ &amp;= 2435107070 \\end{split} \\] The MS error is: \\[ \\begin{split} \\mathrm{MS}_{\\mathrm{Error}} &amp;= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\ &amp;= \\frac{1695313285 }{29} \\\\ &amp;= 58459079 \\end{split} \\] Then, we compute the \\(F\\)-statistic by computing the ratio of these two mean squares. \\[ \\begin{split} F &amp;= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\ &amp;= \\frac{2435107070}{58459079} \\\\ &amp;= 41.7 \\end{split} \\] This is the observed \\(F\\)-statistic for the model. Note that this is an identical computation (although reframed) as the initial computation for \\(F\\). $$ \\[\\begin{split} F &amp;= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1em] &amp;= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1em] &amp;= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1em] &amp;= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1em] &amp;= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1em] &amp;= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\end{split}\\] $$ To test the null hypothesis, \\(H_0:\\rho^2=0\\), we evaluate this observed \\(F\\)-statistic in an \\(F\\)-distribution with the 2 and 29 degrees of freedom. Figure 4.8: Plot of the probability density function (PDF) for the F-distribution with 2 and 29 degrees of freedom. The cumulative density representing the p-value for a two-tailed test evaluating whether rho-squared=0 using an observed F-statistic of 41.7 is also displayed. The computation using the cumulative density function, pf(), to obtain the \\(p\\)-value is: 1 - pf(41.7, df1 = 2, df2 = 29) [1] 0.000000002942 4.6 Mean Squares are Variance Estimates Mean squares are estimates of the variance. Consider the computational formula for the sample variance, \\[ \\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1} \\] This is the total sum of squares divided by the total df. When we compute an \\(F\\)-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our \\(F\\)-statistic would be close to zero. When the model explains asignificant amount of variation, the numerator gets larger relative to the denominator and the \\(F\\)-value is larger. The mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression. Figure 4.9: Visual depiction of the distributional assumptions of the residuals underlying linear regression. Recall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58,459,079. While the variance is a mathematical convenience, the standard deviation is a better descriptor of the variation in these distributions. The standard deviation is 7646. sqrt(58459079) [1] 7646 We can also obtain this value from the model-level regression output. Here it is typically referred to as the Root Mean Squared Error (RMSE). In the glance() output this value is in the sigma column. glance(lm.1) Why is this value important? It gives the expected variation in the distribution. For example, since all of the conditional distributions of the residuals are normally distribued, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15292\\) and 15292. Observations with residuals that are more extreme may be regression outliers. References "],
["data-codebook.html", "Data Codebooks ed-schools.csv mn-schools.csv movies.csv riverview.csv", " Data Codebooks The data codebooks provide information about the attributes and source of each of the datasets used in the notes. ed-schools.csv The data in ed-schools.csv come from U.S. News and World Report (2018) and contain 13 attributes collected from the \\(n=129\\) graduate schools of education ranked in the 2018 Best Graduate Schools. The attributes include: rank: Rank school: Graduate program of Education score: Years of seniority peer: Peer assessment score (5.0 = highest) expert_score: Administrator/expert assessment score (5.0 = highest) gre_verbal: Mean GRE verbal score in 2016 gre_quant: Mean GRE quantitative score in 2016 doc_accept: Acceptance rate for doctoral students in 2016 student_faculty_ratio: Ratio of doctoral students to faculty member in 2016 phd_granted_per_faculty: Doctorates granted per faculty member in 2015–16 funded_research: Funded research (in millions of dollars) funded_research_per_faculty: Funded research per faculty member (in thousands of dollars) enroll: Total graduate education enrollment in 2016 mn-schools.csv The data in mnSchools.csv were collected from http://www.collegeresults.org and contain 2011 institutional data for \\(n=33\\) Minnesota colleges and universities. The attributes include: name: College/university name grad: Six-year graduation rate, as a percentage public: Sector (1 = public college/university, 0 = private college/university) sat: Estimated median composite SAT score (in hundreds) tuition: Amount of tuition and required fees covering a full academic year for a typical student, in thousands of U.S. dollars movies.csv The data in movies.csv includes attributes for \\(n=1,806\\) movies. These data are a subset of data from the movies data object included in the ggplot2movies package. The original data contains information on 24 variables collected from 28,819 movies. The attributes include: title: Movie’s title budget: Movie’s budget (in millions of U.S. dollars) age: Age of the movie; Computed by subtracting the movie’s release date from 2019 mpaa: MPAA rating (PG, PG-13, R) riverview.csv The data in riverview.csv come from C. Lewis-Beck &amp; Lewis-Beck (2016) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hyopothetical midwestern city. The attributes include: education: Years of formal education income: Annual income (in thousands of U.S. dollars) seniority: Years of seniority gender: Employee’s gender male: Dummy coded gender variable (0 = Female, 1 = Male) party: Political party affiliation References "],
["references.html", "References", " References "]
]
